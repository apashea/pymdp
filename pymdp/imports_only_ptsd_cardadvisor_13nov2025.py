# -*- coding: utf-8 -*-
"""Imports only PTSD CardAdvisor 13NOV2025

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HTAKiGIlIw4xIdwG7wklNUaZ7sis1a8U
"""

# Install pymdp

#! python --version
#! pip install inferactively-pymdp   # version 0.0.7.1 as of 2025-09-16
#!pip list | grep inferactively-pymdp

# # Alternative: directly pull from github (sophisticated inference and other updates)
# !git clone https://github.com/infer-actively/pymdp.git
# %cd pymdp
# !pip install .

# Alternative: directly pull from github (saved 0.0.7.1)
!git clone https://github.com/apashea/pymdp.git
import sys
sys.path.append('/content/pymdp')
import pymdp

# Library imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import pymdp
from pymdp import utils
from pymdp.maths import softmax
from pymdp.agent import Agent
from pymdp import inference, utils, control, maths
from pymdp import control
from pymdp import maths
import datetime
import copy
import math
import random
from pymdp.pymdp_external_custom import custom_test_print_function, update_posterior_policies_full_info, infer_policies_info, select_highest_equivalent, infer_states_info
from pymdp.pymdp_external_custom import update_A_MMP_distributional, update_gamma, update_C_MMP_distributional, update_E
from pymdp.pymdp_external_custom import compute_current_qs_bma, sample_action_timestep_dependent, _sample_action_marginal_external, _sample_policy_external
from pymdp.pymdp_external_custom import compute_spm_lfp, plot_spm_lfp, compute_spm_dopamine, plot_spm_dopamine, spm_conv, compute_xn_bma

# Set random seeds for reproducibility
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Agent configuration settings

# Lower level agent parameters
pA_advice_trust = 0.9     # A[0]: Probability that advice is correct if advisor is believed to be trustworthy (only in advice stage); P(obs_advice=color|trust,correct=blue,...,)
pA_feedback_belief_choice = 0.9        # A[1]: Probability that feedback is correct if agent chooses the card they believe is correct (only in decision stage; and expects null if chose null); P(feedback=correct|...,correct=blue,...,choice=blue,...)
pA_arousal_affect = 1.0  # A[2]: Probability that high/low arousal aligns with negative/positive (angry/calm) affect; P(high|..,angry,..,..,..)
# No parameter for A[3] (proprioceptive outcome aligns with choice they made)
pB_trust = 0.9    # B[0]: Probability that the advisor will remain (un)trustworthy if the agent (dis)trusts them
pB_correctcard = 0.9  # B[1]: Probability that the correct card color will remain the same for the given trial (fixity of agent's beliefs about correct card color), e.g., P(blue at t+1|blue at t); uncontrollable
pB_affect_trust = 0.6667   # B[2]: Probability that trust/distrust actions lead to positive/negative affect; P(calm at t+1|<any affect>,trust)
pB_choice = 0.95     # B[3]: Probability that the choice belief will be conditioned on the action, e.g., P(blue @ t+1|believe blue @ t,choose blue @ t)
pB_stage = 1.0       # B[4]: Probability that stages proceed deterministically (null->advice->decision); uncontrollable by agent
pC_correct = 0.5             # C[1]: Agent's preference for 'correct' feedback (where 'incorrect'= -cc-3 and 'null' hard-coded to zero, followed by softmax)
pC_arousal_low = 0.35  # C[2]: Agent's preference for low arousal (and conversely, high arousal preference = 1 - arousal_low_preference)
pD_trust = 0.5    # D[0]: prior belief that advisor is trustworthy
pD_affect = 0.5   # D[2]: prior belief that agent is in negative affect

# Higher level agent parameters
pA2_trust_safety = 0.667 # A2[0]: association between inferences about advisor's trustworthiness with higher-order beliefs about safety
pA2_affect_safety = 0.667 # A2[2]: association between inferences about negative/angry affect with higher-order safety beliefs
pB2_safety_self_transition = 1.0  # B2[0]: Agent's belief that self safety persists over time, P(self=safe at t+1|self=safe at t)
pB2_safety_world_transition = 1.0  # B2[1]: Agent's belief that world safety persists over time, P(self=safe at t+1|self=safe at t)
pB2_safety_other_transition = 1.0  # B2[2]: Agent's belief that others' safety persists over time, P(self=safe at t+1|self=safe at t)
pC2_trust = 0.5   # C2[0]: Agent's preference for inferring the advisor is trustworthy (as opposed to untrustworthy)
pC2_angry_affect = 1.0  # C2[2]: Agent's preference for inferring negative/angry affect (as opposed to positive/calm affect)
pD2_safety = 0.25  # D2[0],D2[1],D2[2]: Agent's initial beliefs about safety

# Define hidden state factors with respective hidden states
trustworthiness_states = ['trust','distrust'] # Hidden state factor denoting beliefs about advisor's trustworthiness, used in B[0] and D[0] and A matrix
correct_card_states = ['blue','green'] # Hidden state factor denoting beliefs about the correct color of the card, used in B[1] and D[1] and A matrix
affect_states = ['angry','calm']     # Hidden state factor denoting beliefs about affective state, used in B[2] and D[2] and A matrix
choice_states = ['blue','green','null','withdraw']  # Hidden state factor denoting belief about the action the agent took (if they chose the blue card, green card, or took no action), used in B[3] and D[3] and A matrix
stage_states = ['null','advice','decision']   # Hidden state factor denoting beliefs about the current stage of the trial, used in B[4] and D[4] and A matrix
num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)]
num_factors = len(num_states)

# Define observation modalities with respective discrete observations
advice_obs = ['blue','green','null']                # Advice modality, used in A[0] and C[0]. Agent receives 'blue' or 'green' advice from the advisor during the advice stage, or 'null' if in a different behavioral stage.
feedback_obs = ['correct','incorrect','null']       # Feedback modality, used in A[1] and C[1]. Agent receives 'correct' or 'incorrect' following the decision stage, or 'null' if in a prior behavioral task stage.
arousal_obs = ['high','low']                        # Arousal modality, used in A[2] and C[2]. Agent receives 'high' or 'low' arousal observations at each behavioral stage.
choice_obs = ['blue','green','null','withdraw']                # Choice modality, used in A[3] and C[3]. Agent receives 'blue' or 'green' based on the choice they made after the decision stage, or 'null' if in a different behavioral task change.
num_obs = [len(advice_obs), len(feedback_obs), len(arousal_obs), len(choice_obs)]
num_modalities = len(num_obs)

# Define control factors with respective actions
choice_trust_actions = ['trust','distrust']    # Control factor with actions to trust or distrust, used in B[0] and B[2], i.e. choosing to trust or distrust the advisor is assumed to influence trustworthiness and affect.
choice_card_actions = ['blue','green','null','withdraw']  # Control factor with actions for choosing the card color during the decision stage, or 'null' in timesteps where choosing is not applicable, used in B[3]
null_actions = ['NULL']                         # Control factor with null action for the uncontrollable hidden state factors correct card and stage, used B[1] and B[4], i.e. the agent does not assume it can act to influence the true correct card state nor behavioral task stage state.

# Hidden state factors at level 2
safety_self_states_2 = ['safe','danger']
safety_world_states_2 = ['safe','danger']
safety_other_states_2 = ['safe','danger']
num_states_2 = [len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2)]
num_factors_2 = len(num_states_2)
# The level 2 model does not take actions, thus we simply supply a singular NULL action to allow the computations to play out coherently.
null_action_2 = ['NULL']

def construct_agent_components(
    # Lower level agent parameters
    pA_advice_trust = 0.9,     # A[0]: Probability that advice is correct if trustworthy (only in advice stage); P(obs_advice=color|trust,correct=blue,...,)
    pA_feedback_belief_choice = 0.9,        # A[1]: Probability that feedback is correct if agent chooses the card they believe is correct (only in decision stage; and expects null if chose null); P(feedback=correct|...,correct=blue,...,choice=blue,...)
    pA_feedback_withdraw_trust = 0.5,       # A[1]: Probability that withdrawal at decision stage evidences advisor is trustworthy or not (default of 0.5: withdrawal leads to no evidence for trustworthy or untrustworthy)
    pA_arousal_affect = 1.0,  # A[2]: Probability that high/low arousal aligns with negative/positive (angry/calm) affect; P(high|..,angry,..,..,..)
    # No parameter for A[3] (proprioceptive outcome aligns with choice they made)
    pB_trust = 0.9,    # B[0]: Probability that the advisor will stay (un)trustworthy if the agent (dis)trusts them
    pB_correctcard = 0.9,  # B[1]: Probability that the correct card color will remain the same for the given trial (fixity of agent's beliefs about correct card), e.g., P(blue at t+1|blue at t); uncontrollable
    pB_affect_trust = 0.6667,   # B[2]: Probability that trust actions lead to positive (calm) affect, and vice-versa; P(calm at t+1|<any affect>,trust)
    pB_choice = 0.95,     # B[3]: Probability that the choice belief will be conditioned on the action, e.g., P(blue @ t+1|believe blue,choose blue)
    pB_stage = 1.0,       # B[4]: Probability that stages proceed deterministically (null->advice->decision); uncontrollable by agent
    pC_correct = 0.5,             # C[1]: Agent's preference for 'correct' feedback (where 'incorrect'= -cc-3 and 'null' hard-coded to zero, followed by softmax)
    pC_arousal_low = 0.35,  # C[2]: Agent's preference for low arousal (and conversely, high arousal preference = 1 - arousal_low_preference)
    pD_trust = 0.5,    # D[0]: prior belief that advisor is trustworthy
    pD_affect = 0.5,   # D[2]: prior belief that agent is in negative affect
    # Higher level agent parameters
    pA2_trust_safety = 0.667, # A2[0]: association between inferences about advisor's trustworthiness with higher-order beliefs about safety
    pA2_affect_safety = 0.667, # A2[2]: association between inferences about negative/angry affect with higher-order safety beliefs
    pB2_safety_self_transition = 1.0,  # B2[0]: Agent's belief that self safety persists over time, P(self=safe at t+1|self=safe at t)
    pB2_safety_world_transition = 1.0,  # B2[1]: Agent's belief that world safety persists over time, P(self=safe at t+1|self=safe at t)
    pB2_safety_other_transition = 1.0,  # B2[2]: Agent's belief that others' safety persists over time, P(self=safe at t+1|self=safe at t)
    pC2_trust = 0.5,   # C2[0]: Agent's preference for inferring the advisor is trustworthy (as opposed to untrustworthy)
    pC2_angry_affect = 1.0,  # C2[2]: Agent's preference for inferring negative/angry affect (as opposed to positive/calm affect)
    pD2_safety = 0.25, # D2[0],D2[1],D2[2]: Agent's initial beliefs about safety

    # Define hidden state factors with respective hidden states
    trustworthiness_states = ['trust','distrust'], # Hidden state factor denoting beliefs about advisor's trustworthiness, used in B[0] and D[0] and A matrix
    correct_card_states = ['blue','green'], # Hidden state factor denoting beliefs about the correct color of the card, used in B[1] and D[1] and A matrix
    affect_states = ['angry','calm'],     # Hidden state factor denoting beliefs about affective state, used in B[2] and D[2] and A matrix
    choice_states = ['blue','green','null','withdraw'],  # Hidden state factor denoting belief about the action the agent took (if they chose the blue card, green card, or took no action), used in B[3] and D[3] and A matrix
    stage_states = ['null','advice','decision'],   # Hidden state factor denoting beliefs about the current stage of the trial, used in B[4] and D[4] and A matrix
    # num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)],
    # num_factors = len(num_states),

    # Define observation modalities with respective discrete observations
    advice_obs = ['blue','green','null'],                # Advice modality, used in A[0] and C[0]. Agent receives 'blue' or 'green' advice from the advisor during the advice stage, or 'null' if in a different behavioral stage.
    feedback_obs = ['correct','incorrect','null'],       # Feedback modality, used in A[1] and C[1]. Agent receives 'correct' or 'incorrect' following the decision stage, or 'null' if in a prior behavioral task stage.
    arousal_obs = ['high','low'],                        # Arousal modality, used in A[2] and C[2]. Agent receives 'high' or 'low' arousal observations at each behavioral stage.
    choice_obs = ['blue','green','null','withdraw'],                # Choice modality, used in A[3] and C[3]. Agent receives 'blue' or 'green' based on the choice they made after the decision stage, or 'null' if in a different behavioral task change.
    # num_obs = [len(advice_obs), len(feedback_obs), len(arousal_obs), len(choice_obs)],
    # num_modalities = len(num_obs)

    # Define control factors with respective actions
    choice_trust_actions = ['trust','distrust'],    # Control factor with actions to trust or distrust, used in B[0] and B[2], i.e. choosing to trust or distrust the advisor is assumed to influence trustworthiness and affect.
    choice_card_actions = ['blue','green','null','withdraw'],  # Control factor with actions for choosing the card color during the decision stage, or 'null' in timesteps where choosing is not applicable, used in B[3]
    null_actions = ['NULL'],                         # Control factor with null action for the uncontrollable hidden state factors correct card and stage, used B[1] and B[4], i.e. the agent does not assume it can act to influence the true correct card state nor behavioral task stage state.

    # Hidden state factors at level 2
    safety_self_states_2 = ['safe','danger'],
    safety_world_states_2 = ['safe','danger'],
    safety_other_states_2 = ['safe','danger'],
    #num_states_2 = [len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2)]
    #num_factors_2 = len(num_states_2)
    # The level 2 model does not take actions, thus we simply supply a singular NULL action to allow the computations to play out coherently.
    null_action_2 = ['NULL'],
    monitoring=False
                                       ):
    num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)]
    num_factors = len(num_states)
    num_obs = [len(advice_obs), len(feedback_obs), len(arousal_obs), len(choice_obs)]
    num_modalities = len(num_obs)
    num_states_2 = [len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2)]
    num_factors_2 = len(num_states_2)
    # Level 1 of the hierarchical model: defining hidden states, observations, and actions

    # A matrix (likelihood / perception) containing conditional probability distributions of each observation modality with all hidden state factors.
    # Each submatrix A[i] is the conditional probability distribution P(o_i|s) where s denotes all hidden state factors.
    # During the process of inference, with model inversion via Bayes' theorem, the A matrix contributes to updating posterior beliefs Q(s|o).
    A = utils.obj_array(num_modalities)   # Initialize A matrix with number of observation modalities

    # A[0]: Advice modality which determinstically maps agent's trust belief, correct card belief, and other hidden state beliefs to advice received.
    # The `pA_advice_trust` parameter determines probabilities linking observed advice from advisor, agent's belief in advisor's trutworthiness, and agent's belief in the correct card.
    # If pA_advice_trust=0.9: If agent believes advisor is trustworthy then they believe advisor's advice will match agent's own belief about the true card color.
    pA_advice_trust = pA_advice_trust #  0.9
    A[0] = np.zeros(( len(advice_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
    # regardless of affect and choice beliefs, if advice matches correct card belief, then 'trust' (and if non-match, then 'distrust')
    for affect_state in range(len(affect_states)):  # cycles thru both affect states
      for choice_state in range(len(choice_states)):
        #print(f"{affect_state}, {choice_state}")
        A[0][0,0,0,affect_state,choice_state,1] = pA_advice_trust    # P(blue|trust,blue,<any affect>,<any choice>,) = pA_advice_trust
        A[0][1,0,1,affect_state,choice_state,1] = pA_advice_trust    # P(green|trust,green,<any affect>,<any choice>) = pA_advice_trust
        A[0][0,1,1,affect_state,choice_state,1] = pA_advice_trust    # P(blue|distrust,green,<any affect>,<any choice>) = pA_advice_trust (if agent believes 'green', and advice is 'blue', distrust)
        A[0][1,1,0,affect_state,choice_state,1] = pA_advice_trust    # P(green|distrust,blue,<any affect>,<any choice>) = pA_advice_trust (if agent believes 'blue', and advice is 'green', distrust)
        A[0][1,0,0,affect_state,choice_state,1] = 1-pA_advice_trust    # P(green|trust,blue,<any affect>,<any choice>,) = 1-pA_advice_trust
        A[0][0,0,1,affect_state,choice_state,1] = 1-pA_advice_trust    # P(blue|distrust,green,<any affect>,<any choice>) = 1-pA_advice_trust
        A[0][1,1,1,affect_state,choice_state,1] = 1-pA_advice_trust    # P(green|distrust,green,<any affect>,<any choice>) = 1-pA_advice_trust (if agent believes 'green', and advice is 'green', less distrust)
        A[0][0,1,0,affect_state,choice_state,1] = 1-pA_advice_trust    # P(blue|distrust,blue,<any affect>,<any choice>) = 1-pA_advice_trust (if agent believes 'blue', and advice is 'blue', less distrust)
        A[0][2,:,:,affect_state,choice_state,2] = 1           # P(null|<any trust>,<any correct>,<any affect>,<any choice>,decision> = 1 (expects no advice during final decision stage)
        A[0][2,:,:,affect_state,choice_state,0] = 1           # P(null|<any trust>,<any correct>,<any affect>,<any choice>,null) = 1 (expects no advice during initial null stage)
    A[0] = utils.norm_dist(A[0])
    if monitoring == True:
      print(f"is A[0] normalized? : {utils.is_normalized(A[0])}")

    # A[1]: Feedback modality maps from the hidden states of correct card (blue or green), choice made (blue or green), and other hidden state beliefs to feedback outcomes
    #     (correct, incorrect, or null if before feedback is received)) with 90% probability.
    # The `pA_feedback_belief_choice` parameter determines how precisely these beliefs map to feedback observations. pA_feedback_belief_choice of 1.0 will map these beliefs with full one-to-one precision.
    pA_feedback_belief_choice = pA_feedback_belief_choice # alpha = 0.9
    pA_feedback_withdraw_trust = pA_feedback_withdraw_trust
    # feedback_obs = ['correct','incorrect','null']
    A[1] = np.zeros( (len(feedback_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
    for trustworthiness_state in range(len(trustworthiness_states)):
      for affect_state in range(len(affect_states)):
        #for stage_state in range(len(stage_states)):
        #for non_null_choice in [0,1]:
          A[1][0,trustworthiness_state,0,affect_state,0,2] = pA_feedback_belief_choice     # P(correct|<any trust>,blue,<any affect>,blue,<any stage>) = pA_feedback_belief_choice (if believes 'blue' is correct and chose 'blue', then expects feedback is 'correct')
          A[1][0,trustworthiness_state,1,affect_state,1,2] = pA_feedback_belief_choice     # P(correct|<any trust>,green,<any affect>,green,<any stage>) = pA_feedback_belief_choice (if believes 'green' is correct and chose 'green', then expects feedback outcome is 'correct')
          A[1][0,trustworthiness_state,0,affect_state,1,2] = 1-pA_feedback_belief_choice   # P(correct|<any trust>,blue,<any affect>,green,<any stage>) = 1-pA_feedback_belief_choice (if believes 'blue' is correct and chose 'green', then expects feedback outcome is not 'correct')
          A[1][0,trustworthiness_state,1,affect_state,0,2] = 1-pA_feedback_belief_choice   # P(correct|<any trust>,green,<any affect>,blue,<any stage>) = 1-pA_feedback_belief_choice (if believes 'green' is correct and chose 'blue', then expects feedback outcome is not 'correct')
          A[1][1,trustworthiness_state,0,affect_state,1,2] = pA_feedback_belief_choice     # P(incorrect|<any trust>,blue,<any affect>,green,<any stage>) = pA_feedback_belief_choice (if believes 'blue' is correct and chose 'green', then expects feedback is 'incorrect')
          A[1][1,trustworthiness_state,1,affect_state,0,2] = pA_feedback_belief_choice     # P(incorrect|<any trust>,green,<any affect>,blue,<any stage>) = pA_feedback_belief_choice (if believes 'green' is correct and chose 'blue', then expects feedback is 'incorrect')
          A[1][1,trustworthiness_state,0,affect_state,0,2] = 1-pA_feedback_belief_choice   # P(incorrect|<any trust>,blue,<any affect>,blue,<any stage>) = 1-pA_feedback_belief_choice (if believes 'blue' is correct and chose 'blue', then expects feedback is not 'incorrect')
          A[1][1,trustworthiness_state,1,affect_state,1,2] = 1-pA_feedback_belief_choice   # P(incorrect|<any trust>,green,<any affect>,green,<any stage>) = 1-pA_feedback_belief_choice (if believes 'green' is correct and chose 'green', then expects feedback is not 'incorrect')
          A[1][2,trustworthiness_state,:,affect_state,2,2] = 1         # P(null|<any trust>,<any correct>,<any affect>,null,decision) = 1 (in decision stage, if no decision made, expects feedback outcome 'null')
          A[1][2,trustworthiness_state,:,affect_state,:,1] = 1         # P(null|<any trust>,<any correct>,<any affect>,<any choice>,advice) = 1 (in advice stage, expects feedback outcome 'null')
          A[1][2,trustworthiness_state,:,affect_state,:,0] = 1         # P(null|<any trust>,<any correct>,<any affect>,<any choice>,null>) = 1 (in null stage, expects feedback outcome 'null')

    # For withdraw: Withdraw withdraws from receiving feedback. pA_feedback_withdraw_trust=0.5 reflects the 'reality' that without feedback, the agent
    # cannot actually infer if the advisor was trustworthy or not.
    A[1][2,0,:,:,3,2] = pA_feedback_withdraw_trust   #P(null|trustworthy,<any color>,<any affect>,withdraw,decision) = 0.5 # p_A_feedback_withdraw_trust
    A[1][2,1,:,:,3,2] = 1 - pA_feedback_withdraw_trust
    A[1][0,:,:,:,3,2] = 0.0  # P(correct|<any trust>,<any color>,<any affect>,withdraw,null_stage)
    A[1][1,:,:,:,3,2] = 0.0  # P(incorrect|<any trust>,<any color>,<any affect>,withdraw,advice_stage)
    ##        A[1][2,trustworthiness_state,correct_card_state,affect_state,2,stage_states] = 1   # P(null|<any trust>,<any correct>,<any affect>,null,<any stage>)

    A[1] = utils.norm_dist(A[1])
    #print(A[1])
    if monitoring == True:
      print(f"is A[1] normalized? : {utils.is_normalized(A[1])}")

    # A[2]: Arousal modality maps deterministically from hidden affective state (angry or calm) to an interoceptive outcome of arousal (high or low), regardless of other hidden state beliefs.
    # In this simulation, the agent deterministically infers 'anger' from 'high' arousal and infers 'calm' from 'low' arousal.
    pA_arousal_affect = pA_arousal_affect # 1.0
    A[2] = np.zeros( (len(arousal_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
    for trustworthiness_state in range(len(trustworthiness_states)):
      for correct_card_state in range(len(correct_card_states)):
        for choice_state in range(len(choice_states)):
          for stage_state in range(len(stage_states)):
            #A[2][0,trustworthiness_state,correct_card_state,0,choice_state,stage_state] = 1.0   # P(high|<any trust>,<any correct>,angry,<any choice>) = 1.0
            #A[2][1,trustworthiness_state,correct_card_state,1,choice_state,stage_state] = 1.0   # P(low|<any trust>,<any correct>,calm,<any choice>) = 1.0
            A[2][0,trustworthiness_state,correct_card_state,0,choice_state,stage_state] = pA_arousal_affect   # P(high|<any trust>,<any correct>,angry,<any choice>) = 1.0
            A[2][1,trustworthiness_state,correct_card_state,1,choice_state,stage_state] = pA_arousal_affect   # P(low|<any trust>,<any correct>,calm,<any choice>) = 1.0
            A[2][0,trustworthiness_state,correct_card_state,1,choice_state,stage_state] = 1- pA_arousal_affect   # P(high|<any trust>,<any correct>,calm,<any choice>) = 0.0
            A[2][1,trustworthiness_state,correct_card_state,0,choice_state,stage_state] = 1- pA_arousal_affect   # P(low|<any trust>,<any correct>,calm,<any choice>) = 0.0

    A[2] = utils.norm_dist(A[2])
    if monitoring == True:
      print(f"is A[2] normalized? : {utils.is_normalized(A[2])}")

    # A[3]: Choice modality maps deterministically from the hidden state of decision (null, blue or green) to a proprioceptive outcome of choice made (null, blue or green).
    # In this simulation, the agent determinstically infers
    A[3] = np.zeros( (len(choice_obs), len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states) ))
    for trustworthiness_state in range(len(trustworthiness_states)):
      for correct_card_state in range(len(correct_card_states)):
        for affect_state in range(len(affect_states)):
          for choice in range(len(choice_states)):     # choice_states and choice_obs indeces are aligned therefore we can re-use the same indeces
            for stage_state in range(len(stage_states)):
              # Previous:
              A[3][choice,trustworthiness_state,correct_card_state,affect_state,choice,stage_state] = 1.0   # P(blue|<any trust>,<any correct>,<any affect>,blue,decision) = 1.0
              # Added for withdraw:
              if choice in [0,1,3]:
                A[3][choice,trustworthiness_state,correct_card_state,affect_state,choice,2] = 1.0  # P(<blue/green/withdraw>|,<any trust>,<any correct>,<any affect>,<blue/green/withdraw>,decision)=1.0
              else:
                if stage_state in [0,1]:
                  A[3][choice,trustworthiness_state,correct_card_state,affect_state,choice,stage_state] = 1.0  # P(null|,<any trust>,<any correct>,<any affect>,null,<null/advice>)=0.0

    #print(A[3])
    A[3] = utils.norm_dist(A[3])
    #print(f"A[3] = {A[3]}")
    if monitoring == True:
      print(f"is A[3] normalized? : {utils.is_normalized(A[3])}")

    # B matrix (transitions) for 5-action 2-step policies

    # B matrix (hidden state transitions) containing conditional probability distributions of each hidden state at the next discrete timestep, conditioned on the
    # hidden state at the current discrete time step and action at the current discrete timestep, P(s_{t+1}|s_{t},pi).
    # Each submatrix B[i] is the conditional probability distribution P(s_{i,t+1}|s_{i,t},pi) s_{i} denotes a particular hidden state factor and pi denotes all policies for controlling
    # that hidden state factor.
    B = utils.obj_array(num_factors)

    # B[0] : The state transition matrix for trustworthiness hidden state factor transitions, with discrete levels 'trust' and 'distrust'.
    # Trustworthiness hidden state factor transitions (trustworthiness_t+1|trustworthiness,action)
    # For the trust and distrust actions, regardless of trustworthiness belief in current timestep,
    # the trustworthiness for next timestep (t+1) has a high probability (parameterized by pB_trust) if agent chooses the corresponding
    # trustworthiness action (ex. even if agent distrusts in their belief now, if the agent chooses 'trust' action,
    # agent believes advisor is more trustworthy for next time step)
    B[0] = np.zeros((len(trustworthiness_states), len(trustworthiness_states), len(choice_trust_actions)))
    pB_trust= pB_trust # 0.9

    B[0][0,0,0] = pB_trust # P(trust_t+1|trust,trust) = 0.9
    B[0][0,1,0] = pB_trust # P(trust_t+1|distrust,trust) = 0.9
    B[0][1,0,0] = 1-pB_trust # P(distrust_t+1|trust,trust) = 0.1
    B[0][1,1,0] = 1-pB_trust # P(distrust_t+1|distrust,trust) = 0.1

    B[0][1,1,1] = pB_trust # P(distrust_t+1|distrust,distrust) = 0.9
    B[0][1,0,1] = pB_trust # P(distrust_t+1|trust,distrust) = 0.9
    B[0][0,0,1] = 1-pB_trust # P(trust_t+1|trust,distrust) = 0.1
    B[0][0,1,1] = 1-pB_trust # P(trust_t+1|distrust,distrust) = 0.1

    if monitoring == True:
      print(f"is B[0] normalized? : {utils.is_normalized(B[0])}")


    # B[1] : The state transition matrix for the correct card state. This hidden state factor is considered to be 'uncontrollable' (the agent can choose a card,
    # but there is no action that can 'change' which card is correct), thus this matrix is simpler with a single 'NULL' action.
    # correct card hidden state factor transitions -- highly deterministic and parameterized by pB_correctcard, though a probability of 0.9 (rather than 1.0)
    # introduces slight uncertainty into the agent's belief about the correct card over time.
    # Note that this differs slightly from the Adams et. al 2022 "Everything is connected: Inference and attractors in delusions" simulation, whose agent
    # believes the correct card remains the same with full certainty.
    pB_correctcard= pB_correctcard  # 0.9
    B[1] = np.ones(( len(correct_card_states), len(correct_card_states), len(null_actions) ))*(1-pB_correctcard)
    for correct_card_state in range(len(correct_card_states)):
      for null_action in range(len(null_actions)):
        B[1][correct_card_state,correct_card_state,null_action] = pB_correctcard    # P(blue/green at t+1|blue/green,null_action)
    #print(f"B[1] = {B[1]}")
    if monitoring == True:
      print(f"is B[1] normalized? : {utils.is_normalized(B[1])}")

    # B[2] : The state transition matrix for the agent's affective state, which is deemed 'controllable' based on choosing to trust or distrust.
    # Regardless of the agent's current emotional state, choosing trust has a 2/3 probability of transitioning to a calm affective state
    # and vice-versa for distrust transitioning to an angry affective state.
    # affect hidden state factor transitions --
    pB_affect_trust = pB_affect_trust  # 0.6667
    B[2] = np.zeros((len(affect_states), len(affect_states), len(choice_trust_actions)))
    B[2][0,0,0] = 1 - pB_affect_trust  # P(angry_t+1|angry_t,trust)
    B[2][0,1,0] = 1 - pB_affect_trust  # P(angry_t+1|calm_t,trust)
    B[2][1,0,0] = pB_affect_trust  # P(calm_t+1|angry_t,trust)
    B[2][1,1,0] = pB_affect_trust  # P(calm_t+1|calm_t,trust)

    B[2][0,0,1] = pB_affect_trust # P(angry_t+1|angry_t,distrust)
    B[2][0,1,1] = pB_affect_trust  # P(angry_t+1|calm_t,distrust)
    B[2][1,0,1] = 1 - pB_affect_trust  # P(calm_t+1|angry_t,distrust)
    B[2][1,1,1] = 1 - pB_affect_trust  # P(calm_t+1|angry_t,distrust)
    B[2] = utils.norm_dist(B[2])
    if monitoring == True:
      print(f"is B[2] normalized? : {utils.is_normalized(B[2])}")


    # B[3] : The state transition matrix for the agent's beliefs about the card choice (blue, green null, withdraw) hidden state factor,
    # i.e. the card they chose rather than what the correct card actually is.
    # The transitions are highly deterministic (parameterized by pB_choice with value of probability 0.95), making the
    # choice the agent takes and the belief it has to most likely be what is transitioned to.
    # For example, the probability that the agent chose blue is 0.95 when in the last timestep the agent believed they chose blue (hidden state)
    # as well as in fact chose blue (action taken).
    pB_choice= pB_choice #  0.95
    # B[3] = np.ones((len(choice_states), len(choice_states), len(choice_card_actions)))*(1-pB_choice)/2
    B[3] = np.zeros((len(choice_states), len(choice_states), len(choice_card_actions)))
    for choice in range(len(choice_states)):
      B[3][choice,:,choice]=pB_choice
      for other_action in range(len(choice_card_actions)):
        if other_action != choice:
          B[3][choice,:,other_action] = (1-pB_choice) / (len(choice_card_actions)-1)
    #print(f"B[3] = {B[3]}")
    if monitoring == True:
      print(f"is B[3] normalized? : {utils.is_normalized(B[3])}")

    # B[4] : The state transition matrix for the stage hidden state factor, which is considered 'uncontrollable' (the stages of each trial are
    # assumed to follow in the same order without uncertainty, thus there is only a single 'NULL' action).
    # This maintains a deterministic temporal succession, where if the agent believes the advice stage follows the
    # initial null stage, and the choice stage follows the advice stage.
    pB_stage = pB_stage #  1.0
    B[4] = np.ones((len(stage_states), len(stage_states), len(null_actions)))*(1-pB_stage)/2
    B[4][1,0,0] = pB_stage  # P(advice_t+1|null_t,null_action) = 1.0
    B[4][2,1,0] = pB_stage   # P(decision_t+1|advice_t,null_action) = 1.0
    B[4][0,2,0] = pB_stage   # P(null_t+1|decision_t,null_action) = 1.0
    if monitoring == True:
      print(f"is B[4] normalized? : {utils.is_normalized(B[4])}")
    print(f"B[4] = {B[4]}")

    # C matrix (priors over observations, i.e. "preferences")
    # The `pC_correct` parameter determines the agent's preference for receiving 'correct', and dyspreference for receiving 'incorrect', as the final feedback for their choice.
    pC_correct = pC_correct  # 0.5
    # The `pC_arousal_low` parameter determines the agent's preference for the 'low' arousal observation
    pC_arousal_low = 0.35
    C = utils.obj_array_zeros(num_obs)
    C[0] = utils.norm_dist(softmax(np.array([1,1,1])))  # C[0]: Advice modality: indifferent to if blue, green, or null advice
    C[1] = utils.norm_dist(softmax(np.array([pC_correct, -pC_correct-3,0]))) # C[1]: Feedback modality: strongly prefers 'correct' card feedback, dysprefers 'incorrect' card, indifferent to null
    C[2] = utils.norm_dist(np.array([1-pC_arousal_low, pC_arousal_low]))   # C[2]: Arousal modality: prefers low arousal
    C[3] = utils.norm_dist(softmax(np.array([1,1,1,1])))  # C[3]: Preference for choice modality: indifferent to choice (!!*if null=withdrawal, consider changing)

    for i in range(len(C)):
      if monitoring == True:
        print(f"Is C[{i}] normalized? : {utils.is_normalized(C[i])}")
      #print(f"C[{i}] = {C[i]}")


    # D matrix (prior beliefs about hidden states), i.e. an agent's starting beliefs.
    pD_trust = pD_trust  # 0.5
    pD_affect = pD_affect  # 0.5
    D = utils.obj_array(num_factors)
    D[0] = utils.norm_dist(np.array([pD_trust, 1 - pD_trust]))   # D[0]: priors over trustworthiness_states = ['trust','distrust']: agent is unsure of trust
    D[1] = utils.norm_dist(np.array([0.5, 0.5]))   # D[1]: priors over correct correct_card_states = ['blue','green']: agent is unsure of correct card
    D[2] = utils.norm_dist(np.array([pD_affect, 1- pD_affect]))   # D[2]: priors over affect_states = ['angry','calm']: agent is unsure of affect
    D[3] = utils.norm_dist(np.array([0,0,1,0]))      # D[3]: priors over their own choice from self-observation, choice_states = ['blue','green','null','withdraw']: agent is certain they have not made a choice
    D[4] = utils.norm_dist(np.array([1,0,0]))   # D[4]: priors over current stage of the trial: agent believes they are at the first 'null' stage

    for i in range(len(D)):
        if monitoring == True:
          print(f"Is D[{i}] normalized? : {utils.is_normalized(D[i])}")

    # Policies map to hidden state factors: [trustworthiness, correct_card, affect, choice, stage]
    # Policies map to particular actions for controlling hidden state factors: [trust/distrust, null, trust/distrust, blue/green/null, null]
    policies=[np.array([[0, 0, 0, 2, 0],[0, 0, 0, 1, 0]]), # trust,null,trust,null,null -> trust,null,trust,green,null              # The 1st policy involves trusting the advisor, and then choosing green
              np.array([[0, 0, 0, 2, 0],[0, 0, 0, 0, 0]]), # trust,null,trust,null,null -> trust,null,trust,blue,null               # The 2nd policy involves trusting the advisor, and then choosing blue
              np.array([[1, 0, 1, 2, 0],[1, 0, 1, 0, 0]]), # distrust,null,distrust,null,null -> distrust,null,distrust,blue,null   # The 3rd policy involves distrusting the advisor, and then choosing blue
              np.array([[1, 0, 1, 2, 0],[1, 0, 1, 1, 0]]), # distrust,null,distrust,null,null -> distrust,null,distrust,green,null  # The 4th policy involves distrusting the advisor, and then choosing green
              np.array([[0, 0, 0, 2, 0],[0, 0, 0, 3, 0]]), # trust,null,trust,null,null -> trust,null,trust,withdraw,null               # The 5th policy involves trusting the advisor, but withdrawing from the task
              np.array([[1, 0, 1, 2, 0],[1, 0, 1, 3, 0]])] # distrust,null,distrust,null,null -> distrust,null,distrust,withdraw,null   # The 6th policy involves distrusting the advisor, and then withdrawing from the task

    if monitoring == True:
      print(len(policies))
      for pol_idx, policy in enumerate(policies):
        print(f"policies[{pol_idx}].shape = {policy.shape}")
        for t in range(policy.shape[0]):
          print(f"policies[{pol_idx}][{t}] = {policy[t]}")

    # E matrix (priors over policies, i.e. habits), a vector (1D matrix) with prior probabilities indexed to each of the six available policies (defined below).
    # In this construction, habits are simplified into static (not learned) uniform probabilities, which will thus have no impact on final action selection but
    # nonetheless are included as necessary for the matrix operations and further allow for future experimentation.
    # As E figures in the EFE (expected free energy) computation during policy inference within a softmax function, it does not need to be normalized
    # upon initialization.

    E = utils.norm_dist(np.ones(6))
    if monitoring == True:
      print(f"E.dtype = {E.dtype}")
      print(utils.is_normalized(E))
      print(f"E = {E}")

    # Define matrix Dirichlet distributions as hyperparameters for lower level agent to allow learning A, B, and D matrices
    pA = utils.dirichlet_like(A, scale = 1.0)   # Prior matrix Dirichlet distribution for A matrix (likelihood model)
    pB = utils.dirichlet_like(B, scale = 1.0)   # Prior matrix Dirichlet distribution for B matrix (state transitions model)
    pD = utils.dirichlet_like(D, scale = 1.0)   # Prior matrix Dirichlet distribution for D matrix (priors about initial states)



    # Hierarchical layer level

    # A2 matrix (likelihood / perception model) for the higher level agent, containing conditional probability distributions of each observation modality with all hidden state factors.
    # Each submatrix A2[i] is the conditional probability distribution P(o_i|s) where s denotes all hidden state factors at the higher level.
    A2 = utils.obj_array(num_factors)

    # A2[0]: Inferred trustworthiness modality which determinstically maps the lower level agent's posterior trustworthiness (as an observation)
    # to the higher level agent's hidden state beliefs.
    # The `trust_safety_association` parameter determines the probability linking observed 'trust' to the 'safe' beliefs in the higher level's safety hidden state factors.
    # This is the case vice-versa for linking 'distrust' to the 'danger' beliefs.
    pA2_trust_safety = pA2_trust_safety # 0.667    # The agent moderately associates 'trust' with 'safe' as well as 'distrust' with 'danger'
    A2[0] = np.zeros(( len(trustworthiness_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
    A2[0][0,0,0,0] = pA2_trust_safety       # P(trust|safe,safe,safe) = 0.667
    A2[0][0,0,0,1] = 1-pA2_trust_safety     # P(trust|safe,safe,danger) = 0.333
    A2[0][0,0,1,0] = 1-pA2_trust_safety     # P(trust|safe,danger,safe) = 0.333
    A2[0][0,1,0,0] = 1-pA2_trust_safety     # P(trust|danger,safe,safe) = 0.333
    A2[0][0,0,1,1] = 1-pA2_trust_safety     # P(trust|safe,danger,danger) = 0.333
    A2[0][0,1,0,1] = 1-pA2_trust_safety     # P(trust|danger,safe,danger) = 0.333
    A2[0][0,1,1,0] = 1-pA2_trust_safety     # P(trust|danger,danger,safe) = 0.333
    A2[0][0,1,1,1] = 1-pA2_trust_safety     # P(trust|danger,danger,danger) = 0.333
    A2[0][1,0,0,0] = 1-pA2_trust_safety     # P(distrust|safe,safe,safe) = 0.333
    A2[0][1,0,0,1] = pA2_trust_safety       # P(distrust|safe,safe,danger) = 0.667
    A2[0][1,0,1,0] = pA2_trust_safety       # P(distrust|safe,danger,safe) = 0.667
    A2[0][1,1,0,0] = pA2_trust_safety       # P(distrust|danger,safe,safe) = 0.667
    A2[0][1,0,1,1] = pA2_trust_safety       # P(distrust|safe,danger,danger) = 0.667
    A2[0][1,1,0,1] = pA2_trust_safety       # P(distrust|danger,safe,danger) = 0.667
    A2[0][1,1,1,0] = pA2_trust_safety       # P(distrust|danger,danger,safe) = 0.667
    A2[0][1,1,1,1] = pA2_trust_safety       # P(distrust|danger,danger,danger) = 0.667
    A2[0] = utils.norm_dist(A2[0])
    # print(f"""P(trust|s_1^2,s_2^2,s_3^2)""")
    # for trustworthiness_state in range(len(trustworthiness_states)):
    #   for safety_self_state in range(len(safety_self_states_2)):
    #     for safety_world_state in range(len(safety_world_states_2)):
    #       for safety_other_state in range(len(safety_other_states_2)):
    #         print(f"P({trustworthiness_states[trustworthiness_state]}|{safety_self_states_2[safety_self_state]},{safety_world_states_2[safety_world_state]},{safety_other_states_2[safety_other_state]}) = {A2[0][trustworthiness_state,safety_self_state,safety_world_state,safety_other_state]}")


    # A[2] : Inferred correct card modality. There is no relationship between the lower level's belief about 'blue' or 'green' being the correct card with safety beliefs,
    # thus this is fully uniform.
    A2[1] = np.zeros(( len(correct_card_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
    for correct_card_state in range(len(correct_card_states)):
      for safety_self_state in range(len(safety_self_states_2)):
        for safety_world_state in range(len(safety_world_states_2)):
          for safety_other_state in range(len(safety_other_states_2)):
            A2[1][correct_card_state,safety_self_state,safety_world_state,safety_other_state] = 1
    A2[1] = utils.norm_dist(A2[1])
    #print(f"A2[1] = {A2[1]}")

    # A[2] : Inferred affect modality linking affect and safety beliefs, where 'calm' is linked more strongly to 'safe' beliefs with parameter
    # `affect_safety_association` (probability 0.667) and vice-versa for 'angry' and 'danger'.
    #A2(angry|self=danger OR world=danger OR other=danger)=affect_safety_association
    #A2(calm|self=safe AND world=safe AND other=safe)=affect_safety_association
    pA2_affect_safety = pA2_affect_safety  # 0.667
    A2[2] = np.zeros(( len(affect_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
    A2[2][0,0,0,0] = 1-pA2_affect_safety    # P(angry|safe,safe,safe) = 0.333
    A2[2][0,0,0,1] = pA2_affect_safety      # P(angry|safe,safe,danger) = 0.667
    A2[2][0,0,1,0] = pA2_affect_safety      # P(angry|safe,danger,safe) = 0.667
    A2[2][0,1,0,0] = pA2_affect_safety      # P(angry|danger,safe,safe) = 0.667
    A2[2][0,0,1,1] = pA2_affect_safety      # P(angry|safe,danger,danger) = 0.667
    A2[2][0,1,0,1] = pA2_affect_safety      # P(angry|danger,safe,danger) = 0.667
    A2[2][0,1,1,0] = pA2_affect_safety      # P(angry|danger,danger,safe) = 0.667
    A2[2][0,1,1,1] = pA2_affect_safety      # P(angry|danger,danger,danger) = 0.667
    A2[2][1,0,0,0] = pA2_affect_safety      # P(calm|safe,safe,safe) = 0.667
    A2[2][1,0,0,1] = 1-pA2_affect_safety    # P(calm|safe,safe,danger) = 0.333
    A2[2][1,0,1,0] = 1-pA2_affect_safety    # P(calm|safe,danger,safe) = 0.333
    A2[2][1,1,0,0] = 1-pA2_affect_safety    # P(calm|dnger,safe,safe) = 0.333
    A2[2][1,0,1,1] = 1-pA2_affect_safety    # P(calm|safe,danger,danger) = 0.333
    A2[2][1,1,0,1] = 1-pA2_affect_safety    # P(calm|danger,safe,danger) = 0.333
    A2[2][1,1,1,0] = 1-pA2_affect_safety    # P(calm|danger,danger,safe) = 0.333
    A2[2][1,1,1,1] = 1-pA2_affect_safety    # P(calm|danger,danger,danger) = 0.333
    A2[2] = utils.norm_dist(A2[2])
    # print("P(affect|s_1^2,s_2^2,s_3^2)")
    # for affect_state in range(len(affect_states)):
    #   for safety_self_state in range(len(safety_self_states_2)):
    #     for safety_world_state in range(len(safety_world_states_2)):
    #       for safety_other_state in range(len(safety_other_states_2)):
    #         print(f"P({affect_states[affect_state]}|{safety_self_states_2[safety_self_state]},{safety_world_states_2[safety_world_state]},{safety_other_states_2[safety_other_state]}) = {A2[2][affect_state,safety_self_state,safety_world_state,safety_other_state]}")

    # A2[3] : Inferred choice modality linking the agent's belief about the choice they made ('blue','green','null') with safety beliefs.
    # There is no relationship, thus the distribution is uniform.
    # choice_states (blue,green,null): no relationship with safety
    A2[3] = np.zeros(( len(choice_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
    for choice_state in range(len(choice_states)):
      for safety_self_state in range(len(safety_self_states_2)):
        for safety_world_state in range(len(safety_world_states_2)):
          for safety_other_state in range(len(safety_other_states_2)):
            A2[3][choice_state,safety_self_state,safety_world_state,safety_other_state] = 1
    A2[3] = utils.norm_dist(A2[3])

    # A2[4] : Inferred state modality linking the agent's belief about the current stage of the trial with safety beliefs.
    # There is no relationship, thus the distribution is uniform.
    # stage_states (null,advice,decision): no relationship with safety (???)
    A2[4] = np.zeros(( len(stage_states), len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2) ))
    for stage_state in range(len(stage_states)):
      for safety_self_state in range(len(safety_self_states_2)):
        for safety_world_state in range(len(safety_world_states_2)):
          for safety_other_state in range(len(safety_other_states_2)):
            A2[4][stage_state,safety_self_state,safety_world_state,safety_other_state] = 1
    A2[4] = utils.norm_dist(A2[4])
    if monitoring == True:
      for i in range(len(A2)):
            try:
              print(f"Is A2[{i}] normalized? : {utils.is_normalized(A2[i])}")
            except:
              print(f"normalization error: A2[{i}] -- ")

    # B2 : B matrix (hidden state transitions) containing conditional probability distributions of each hidden state at the next discrete timestep, conditioned on the
    # hidden state at the current discrete time step and action at the current discrete timestep, P(s_{t+1}|s_{t},pi) at the higher level.
    # Each submatrix 2B[i] is the conditional probability distribution P(s_{i,t+1}|s_{i,t},pi) s_{i} denotes a particular hidden state factor and pi denotes all policies for controlling
    # that hidden state factor.
    # The higher level does not have actions available and simply maps the safety belief from the current timestep to the next timestep.
    B2 = utils.obj_array(num_factors_2)

    # B2[0] : State transitions for safety beliefs about the self. The `B2_safety_self_tansition` (probability 1.0) parameter makes this mapping fully deterministic,
    # akin to hyper-precise priors where 'safe' necessarily leads to 'safe' and 'danger' necessarily leads to 'danger' without ambiguity.
    #B2(self_t=safe|self_t-1=safe)=B2_safety_self_transition
    #B2(self_t=danger|self_t-1=danger)=B2_safety_self_transition
    pB2_safety_self_transition = pB2_safety_self_transition #  1.0
    B2[0] = np.zeros((len(safety_self_states_2), len(safety_self_states_2), len(null_action_2)))
    B2[0][0,0,0] = pB2_safety_self_transition
    B2[0][1,1,0] = pB2_safety_self_transition
    B2[0][0,1,0] = 1-pB2_safety_self_transition
    B2[0][1,0,0] = 1-pB2_safety_self_transition
    B2[0] = utils.norm_dist(B2[0])
    if monitoring == True:
      print(f"Is B2[0] normalized? : {utils.is_normalized(B2[0])}")

    # B2[1] : State transitions for safety beliefs about the world. The `B2_safety_world_tansition` (probability 1.0) parameter makes this mapping fully deterministic,
    # akin to hyper-precise priors where 'safe' necessarily leads to 'safe' and 'danger' necessarily leads to 'danger' without ambiguity.
    #B2(world_t=safe|world_t-1=safe)=B2_safety_world_transition
    #B2(world_t=danger|world_t-1=danger)=B2_safety_world_transition
    pB2_safety_world_transition = pB2_safety_world_transition  # 1.0
    B2[1] = np.zeros((len(safety_world_states_2), len(safety_world_states_2), len(null_action_2)))
    B2[1][0,0,0] = pB2_safety_world_transition
    B2[1][1,1,0] = pB2_safety_world_transition
    B2[1][0,1,0] = 1-pB2_safety_world_transition
    B2[1][1,0,0] = 1-pB2_safety_world_transition
    B2[1] = utils.norm_dist(B2[1])
    if monitoring == True:
      print(f"Is B2[1] normalized? : {utils.is_normalized(B2[1])}")

    # B2[2] : State transitions for safety beliefs about other people. The `B2_safety_other_tansition` (probability 1.0) parameter makes this mapping fully deterministic,
    # akin to hyper-precise priors where 'safe' necessarily leads to 'safe' and 'danger' necessarily leads to 'danger' without ambiguity.
    #B2(other_t=safe|other_t-1=safe)=B2_safety_other_transition
    #B2(other_t=danger|other_t-1=danger)=B2_safety_other_transition
    pB2_safety_other_transition = pB2_safety_other_transition  # 1.0
    B2[2] = np.zeros((len(safety_world_states_2), len(safety_world_states_2), len(null_action_2)))
    B2[2][0,0,0] = pB2_safety_other_transition
    B2[2][1,1,0] = pB2_safety_other_transition
    B2[2][0,1,0] = 1-pB2_safety_other_transition
    B2[2][1,0,0] = 1-pB2_safety_other_transition
    B2[2] = utils.norm_dist(B2[2])
    if monitoring == True:
      print(f"Is B2[2] normalized? : {utils.is_normalized(B2[2])}")

    # C2: C matrix (preferences) for the higher level agent, which align with the hidden state factors at the lower level
    # as the lower level's posterior inferences are passed upwards to the higher level as the latter's observations.
    # This effectively makes C2 the higher level's preferences for hidden state posteriors at the lower level.
    pC2_trust = pC2_trust  # 0.5
    pC2_angry_affect = pC2_angry_affect  # 1.0
    C2 = utils.obj_array(num_factors)    # hidden state factors at lower level are observation modalities at higher level
    C2[0] = utils.norm_dist(np.array([pC2_trust, 1 - pC2_trust]))   # no preference over trust
    C2[1] = utils.norm_dist(np.array([0.5, 0.5]))   # no preference over correct card color
    C2[2] = utils.norm_dist(np.array([pC2_angry_affect, 1 - pC2_angry_affect]))   # no preference over affect (angry, calm)
    C2[3] = utils.norm_dist(np.array([1,1,1,1]))      # no preference over choice
    C2[4] = utils.norm_dist(np.array([1,1,1]))   # no preference over stage
    if monitoring == True:
      for i in range(len(C2)):
        print(f"Is C2[{i}] normalized? : {utils.is_normalized(C2[i])}")

    # D2: D matrix (priors over initial hidden states) for the higher level agent. All safety beliefs are biased towards danger via the `prior_on_danger` parameter
    # with probability 0.75.
    D2 = utils.obj_array(num_factors_2)
    pD2_safety = pD2_safety # 0.25
    D2[0] = utils.norm_dist(np.array([pD2_safety, 1 - pD2_safety]))   # priors over safety (self)
    D2[1] = utils.norm_dist(np.array([pD2_safety, 1 - pD2_safety]))   # priors over safety (world)
    D2[2] = utils.norm_dist(np.array([pD2_safety, 1 - pD2_safety]))   # priors over safety (others)
    if monitoring == True:
      for i in range(len(D2)):
        print(f"Is D2[{i}] normalized? : {utils.is_normalized(D2[i])}")

    # Matrix dirichlet distributions for learning A2, B2, and D2 for the higher level agent
    pA2 = utils.dirichlet_like(A2, scale = 1.0)
    pB2 = utils.dirichlet_like(B2, scale = 1.0)
    pD2 = utils.dirichlet_like(D2, scale = 1.0)

    return A, B, C, D, E, policies, pA, pB, pD, A2, B2, C2, D2, pA2, pB2, pD2

A, B, C, D, E, policies, pA, pB, pD, A2, B2, C2, D2, pA2, pB2, pD2 = construct_agent_components(
    # Lower level agent parameters

    pA_advice_trust = 0.9,     # A[0]: Probability that advice is correct if trustworthy (only in advice stage); P(obs_advice=color|trust,correct=blue,...,)
    pA_feedback_belief_choice = 0.9,        # A[1]: Probability that feedback is correct if agent chooses the card they believe is correct (only in decision stage; and expects null if chose null); P(feedback=correct|...,correct=blue,...,choice=blue,...)
    pA_arousal_affect = 1.0,  # A[2]: Probability that high/low arousal aligns with negative/positive (angry/calm) affect; P(high|..,angry,..,..,..)
    # No parameter for A[3] (proprioceptive outcome aligns with choice they made)
    pB_trust = 0.9,    # B[0]: Probability that the advisor will stay (un)trustworthy if the agent (dis)trusts them
    pB_correctcard = 0.9,  # B[1]: Probability that the correct card color will remain the same for the given trial (fixity of agent's beliefs about correct card), e.g., P(blue at t+1|blue at t); uncontrollable
    pB_affect_trust = 0.6667,   # B[2]: Probability that trust actions lead to positive (calm) affect, and vice-versa; P(calm at t+1|<any affect>,trust)
    pB_choice = 0.95,     # B[3]: Probability that the choice belief will be conditioned on the action, e.g., P(blue @ t+1|believe blue,choose blue)
    pB_stage = 1.0,       # B[4]: Probability that stages proceed deterministically (null->advice->decision); uncontrollable by agent
    pC_correct = 0.5,             # C[1]: Agent's preference for 'correct' feedback (where 'incorrect'= -cc-3 and 'null' hard-coded to zero, followed by softmax)
    pC_arousal_low = 0.35,  # C[2]: Agent's preference for low arousal (and conversely, high arousal preference = 1 - arousal_low_preference)
    pD_trust = 0.5,    # D[0]: prior belief that advisor is trustworthy
    pD_affect = 0.5,   # D[2]: prior belief that agent is in negative affect
    # Higher level agent parameters
    pA2_trust_safety = 0.667, # A2[0]: association between inferences about advisor's trustworthiness with higher-order beliefs about safety
    pA2_affect_safety = 0.667, # A2[2]: association between inferences about negative/angry affect with higher-order safety beliefs
    pB2_safety_self_transition = 1.0,  # B2[0]: Agent's belief that self safety persists over time, P(self=safe at t+1|self=safe at t)
    pB2_safety_world_transition = 1.0,  # B2[1]: Agent's belief that world safety persists over time, P(self=safe at t+1|self=safe at t)
    pB2_safety_other_transition = 1.0,  # B2[2]: Agent's belief that others' safety persists over time, P(self=safe at t+1|self=safe at t)
    pC2_trust = 0.5,   # C2[0]: Agent's preference for inferring the advisor is trustworthy (as opposed to untrustworthy)
    pC2_angry_affect = 1.0,  # C2[2]: Agent's preference for inferring negative/angry affect (as opposed to positive/calm affect)
    pD2_safety = 0.25, # D2[0],D2[1],D2[2]: Agent's initial beliefs about safety
)

print(utils.is_normalized(B))

##### PRINTOUT each A[i] WITH LABELS #####

# Define hidden state factors with respective hidden states
trustworthiness_states = ['trust','distrust'] # Hidden state factor denoting beliefs about advisor's trustworthiness, used in B[0] and D[0] and A matrix
correct_card_states = ['blue','green'] # Hidden state factor denoting beliefs about the correct color of the card, used in B[1] and D[1] and A matrix
affect_states = ['angry','calm']     # Hidden state factor denoting beliefs about affective state, used in B[2] and D[2] and A matrix
choice_states = ['blue','green','null','withdraw']  # Hidden state factor denoting belief about the action the agent took (if they chose the blue card, green card, or took no action), used in B[3] and D[3] and A matrix
stage_states = ['null','advice','decision']   # Hidden state factor denoting beliefs about the current stage of the trial, used in B[4] and D[4] and A matrix
# num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)],
# num_factors = len(num_states),

# Define observation modalities with respective discrete observations
advice_obs = ['blue','green','null']                # Advice modality, used in A[0] and C[0]. Agent receives 'blue' or 'green' advice from the advisor during the advice stage, or 'null' if in a different behavioral stage.
feedback_obs = ['correct','incorrect','null']       # Feedback modality, used in A[1] and C[1]. Agent receives 'correct' or 'incorrect' following the decision stage, or 'null' if in a prior behavioral task stage.
arousal_obs = ['high','low']                        # Arousal modality, used in A[2] and C[2]. Agent receives 'high' or 'low' arousal observations at each behavioral stage.
choice_obs = ['blue','green','null','withdraw']                # Choice modality, used in A[3] and C[3]. Agent receives 'blue' or 'green' based on the choice they made after the decision stage, or 'null' if in a different behavioral task change.
# num_obs = [len(advice_obs), len(feedback_obs), len(arousal_obs), len(choice_obs)],
# num_modalities = len(num_obs)

# Define control factors with respective actions
choice_trust_actions = ['trust','distrust']    # Control factor with actions to trust or distrust, used in B[0] and B[2], i.e. choosing to trust or distrust the advisor is assumed to influence trustworthiness and affect.
choice_card_actions = ['blue','green','null','withdraw']  # Control factor with actions for choosing the card color during the decision stage, or 'null' in timesteps where choosing is not applicable, used in B[3]
null_actions = ['NULL']                         # Control factor with null action for the uncontrollable hidden state factors correct card and stage, used B[1] and B[4], i.e. the agent does not assume it can act to influence the true correct card state nor behavioral task stage state.

# Hidden state factors at level 2
safety_self_states_2 = ['safe','danger']
safety_world_states_2 = ['safe','danger']
safety_other_states_2 = ['safe','danger']
#num_states_2 = [len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2)]
#num_factors_2 = len(num_states_2)
# The level 2 model does not take actions, thus we simply supply a singular NULL action to allow the computations to play out coherently.
null_action_2 = ['NULL']

num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)]
num_factors = len(num_states)
num_obs = [len(advice_obs), len(feedback_obs), len(arousal_obs), len(choice_obs)]
num_modalities = len(num_obs)
num_states_2 = [len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2)]
num_factors_2 = len(num_states_2)

# for affect_tplus1 in range(len(affect_states)):
#     for affect_t in range(len(affect_states)):
#         for trust_action in range(len(choice_trust_actions)):
#             transitions_string = f"{affect_states[affect_tplus1]},{affect_states[affect_t]},{choice_trust_actions[trust_action]}"
#             prob = B[2][affect_tplus1, affect_t, trust_action]
#             print(f"B[2][{transitions_string}] = {prob}")


# def make_transition_df(Bi, states_labels, actions_labels):
#     records = []
#     uncontrollable = len(actions_labels) == 1

#     for next_idx in range(len(states_labels)):
#         if uncontrollable:
#             # Add a row per current state; columns = all current states, 'state_next', 'action', 'sum_rule'
#             row = {'state_next': states_labels[next_idx], 'action': actions_labels[0]}
#             current_probs = {}
#             for current_idx, current in enumerate(states_labels):
#                 prob = Bi[next_idx, current_idx, 0]
#                 current_probs[current] = prob
#             row.update(current_probs)
#             prob_sum = sum(current_probs.values())
#             row['sum_rule'] = abs(prob_sum - 1.0) < 1e-8
#             records.append(row)
#         else:
#             for current_idx in range(len(states_labels)):
#                 row = {
#                     'state_next': states_labels[next_idx],
#                     'state_current': states_labels[current_idx]
#                 }
#                 action_probs = {}
#                 for action_idx, action in enumerate(actions_labels):
#                     prob = Bi[next_idx, current_idx, action_idx]
#                     action_probs[action] = prob
#                 row.update(action_probs)
#                 prob_sum = sum(action_probs.values())
#                 row['sum_rule'] = abs(prob_sum - 1.0) < 1e-8
#                 records.append(row)
#     df = pd.DataFrame.from_records(records)
#     return df


# B0_df = make_transition_df(B[0], trustworthiness_states, choice_trust_actions)
# print(B0_df['sum_rule'].value_counts(dropna=False))
# display(B0_df)


# # for correct_card_tplus1 in range(len(correct_card_states)):
# #     for correct_card_t in range(len(correct_card_states)):
# #         for null_action in range(len(null_actions)):
# #             transitions_string = f"{correct_card_states[correct_card_tplus1]},{correct_card_states[correct_card_t]},{null_actions[null_action]}"
# #             prob = B[1][correct_card_tplus1, correct_card_t, null_action]
# #             print(f"B[1][{transitions_string}] = {prob}")
# #print(f"B[1]: =================================")
# #print(B[1])

# B1_df = make_transition_df(B[1], correct_card_states, null_actions)
# print(B1_df['sum_rule'].value_counts(dropna=False))
# display(B1_df)
# print(f"Is B1 normalized? : {utils.is_normalized(B[1])}")

# print(f"B[2] ============================================")
# print(B[2])
# B2_df = make_transition_df(B[2], affect_states, choice_trust_actions)
# print(B2_df['sum_rule'].value_counts(dropna=False))
# display(B2_df)

# print(f"B[3] ===========================================")
# print(B[3])
# B3_df = make_transition_df(B[3], choice_states, choice_card_actions)
# print(B3_df['sum_rule'].value_counts(dropna=False))
# display(B3_df)
# print(f"Is B[3] normalized? : {utils.is_normalized(B[3])}")

# print(f"B[4] ==========================================")

# B4_df = make_transition_df(B[4], stage_states, null_actions)
# print(B4_df['sum_rule'].value_counts(dropna=False))
# display(B4_df)

# #advice_obs|trustworthiness_state,correct_card_state,affect_state,choice,stage_state
# def make_likelihood_df_state_string(Ai, advice_obs, trustworthiness_states, correct_card_states, affect_states, choice_states, stage_states):
#     records = []
#     for trust in range(len(trustworthiness_states)):
#         for card in range(len(correct_card_states)):
#             for affect in range(len(affect_states)):
#                 for choice in range(len(choice_states)):
#                     for stage in range(len(stage_states)):
#                         states_string = f"{trustworthiness_states[trust]},{correct_card_states[card]},{affect_states[affect]},{choice_states[choice]},{stage_states[stage]}"
#                         likelihoods = {}
#                         for advice_obs_idx, obs in enumerate(advice_obs):
#                             likelihoods[obs] = Ai[advice_obs_idx, trust, card, affect, choice, stage]
#                         prob_sum = sum(likelihoods.values())
#                         sum_rule = abs(prob_sum - 1.0) < 1e-80  # tolerance for float error
#                         record = {'states_string': states_string}
#                         record.update(likelihoods)
#                         record['sum_rule'] = sum_rule
#                         records.append(record)
#     df = pd.DataFrame.from_records(records)
#     return df

# import pandas as pd

# def make_likelihood_df(A0, advice_obs, trustworthiness_states, correct_card_states, affect_states, choice_states, stage_states):
#     records = []
#     for trust in range(len(trustworthiness_states)):
#         for card in range(len(correct_card_states)):
#             for affect in range(len(affect_states)):
#                 for choice in range(len(choice_states)):
#                     for stage in range(len(stage_states)):
#                         likelihoods = {}
#                         for advice_obs_idx, obs in enumerate(advice_obs):
#                             likelihoods[obs] = A0[advice_obs_idx, trust, card, affect, choice, stage]
#                         prob_sum = sum(likelihoods.values())
#                         sum_rule = abs(prob_sum - 1.0) < 1e-8  # tolerance for float error
#                         record = {
#                             'trustworthiness_states': trustworthiness_states[trust],
#                             'correct_card_states': correct_card_states[card],
#                             'affect_states': affect_states[affect],
#                             'choice_states': choice_states[choice],
#                             'stage_states': stage_states[stage]
#                         }
#                         record.update(likelihoods)
#                         record['sum_rule'] = sum_rule
#                         records.append(record)
#     df = pd.DataFrame.from_records(records)
#     return df

# # A0_df = make_likelihood_df(A[0], advice_obs, trustworthiness_states, correct_card_states, affect_states, choice_states, stage_states)
# # print(A0_df['sum_rule'].value_counts(dropna=False))
# # display(A0_df)

# A1_df = make_likelihood_df(A[1], feedback_obs, trustworthiness_states, correct_card_states, affect_states, choice_states, stage_states)
# print(A1_df['sum_rule'].value_counts(dropna=False))
# #display(A1_df[A1_df['sum_rule'] == False])
# display(A1_df[A1_df['choice_states'] == 'withdraw'].sort_values('stage_states'))

# # Policies (pi)
# # Construction:
# # Our 'deep inference' construction allows specifying specific policies the agent can choose for simplification and alignment with the reality of the
# # simulation trial flow, rather than including all possible policy trajectories.
# # As our agent only takes non-null actions at the end of timestep 1 and timestep 2 of the three-stage (three timestep) trial, policies are constructed
# # to encompass two timesteps (an array of two chronological subarrays). the only relevant actions at the agent can take in

# """

# ##SEE THIS!!! full code for translation
# #https://deepwiki.com/search/please-carefully-thoroughly-re_c3698155-204b-47ba-bb55-e2915794fdb4:
# % ---- original spm code
# V = zeros(2,4,5);
# V(:,:,1) = [ones(2,2) 2*ones(2,2)];
# V(:,:,2) = ones(2,4);
# V(:,:,3) = vertcat(3*ones(1,4),[1 2 1 2]);
# V(:,:,4) = [ones(2,2) 2*ones(2,2)];
# V(:,:,5) = ones(2,4);
# Dimensions: V is a 3D array. Its intended dimensions and use are:

# The first dimension indexes action/control alternatives at a given step (e.g., trust vs. distrust, blue vs. green).
# The second dimension indexes the set of allowable policies (policy number).
# The third dimension refers to the hidden state factors (such as trust/distrust, choice, affective states, etc.).
# These arrays are constructed for each hidden factor across a trials timesteps, and their numeric values (ones, 2*ones, etc.) encode which action option is taken in each policy.
# """



# import numpy as np

# # Recreate the SPM MATLAB V structure
# V = np.zeros((2, 4, 5))  # (timesteps, policies, factors)
# V[:, :, 0] = np.array([[1, 1, 2, 2], [1, 1, 2, 2]])  # ones(2,2) 2*ones(2,2)
# V[:, :, 1] = np.ones((2, 4))
# V[:, :, 2] = np.array([[3, 3, 3, 3], [1, 2, 1, 2]])  # vertcat(3*ones(1,4),[1 2 1 2])
# V[:, :, 3] = np.array([[1, 1, 2, 2], [1, 1, 2, 2]])  # ones(2,2) 2*ones(2,2)
# V[:, :, 4] = np.ones((2, 4))

# # Convert to pymdp-compatible format
# def spm_to_pymdp_policies(V):
#     """
#     Convert SPM MATLAB V array (timesteps, policies, factors)
#     to pymdp policy list format
#     """
#     num_timesteps, num_policies, num_factors = V.shape
#     policies = []

#     for p_idx in range(num_policies):
#         # Extract policy p_idx: transpose to get (timesteps, factors)
#         policy = V[:, p_idx, :].astype(int)
#         # Convert to 0-indexed (SPM uses 1-indexed)
#         policy = policy - 1
#         policies.append(policy)

#     return policies

# # Convert to pymdp format
# policies = spm_to_pymdp_policies(V)

# # Verify the conversion
# print(f"Number of policies: {len(policies)}")
# print(f"Policy shape: {policies[0].shape}")  # Should be (timesteps, factors)
# print(f"First policy:\n{policies[0]}")


# # Policies map to hidden state factors: [trustworthiness, correct_card, affect, choice, stage]
# # Policies map to particular actions for controlling hidden state factors: [trust/distrust, null, trust/distrust, blue/green/null, null]
# policies=[np.array([[0, 0, 0, 2, 0],[0, 0, 0, 1, 0]]), # trust,null,trust,null,null -> trust,null,trust,green,null              # The 1st policy involves trusting the advisor, and then choosing green
#           np.array([[0, 0, 0, 2, 0],[0, 0, 0, 0, 0]]), # trust,null,trust,null,null -> trust,null,trust,blue,null               # The 2nd policy involves trusting the advisor, and then choosing blue
#           np.array([[1, 0, 1, 2, 0],[1, 0, 1, 0, 0]]), # distrust,null,distrust,null,null -> distrust,null,distrust,blue,null   # The 3rd policy involves distrusting the advisor, and then choosing blue
#           np.array([[1, 0, 1, 2, 0],[1, 0, 1, 1, 0]]), # distrust,null,distrust,null,null -> distrust,null,distrust,green,null  # The 4th policy involves distrusting the advisor, and then choosing green
#           np.array([[0, 0, 0, 2, 0],[0, 0, 0, 2, 0]]), # trust,null,trust,null,null -> trust,null,trust,null,null               # The 5th policy involves trusting the advisor, but withdrawing from the task
#           np.array([[1, 0, 1, 2, 0],[1, 0, 1, 2, 0]])] # distrust,null,distrust,null,null -> distrust,null,distrust,null,null   # The 6th policy involves distrusting the advisor, and then withdrawing from the task

# print(len(policies))
# for pol_idx, policy in enumerate(policies):
#   print(f"policies[{pol_idx}].shape = {policy.shape}")
#   for t in range(policy.shape[0]):
#     print(f"policies[{pol_idx}][{t}] = {policy[t]}")

# # 6SEPT2025: on constructing policies in accord with Adams et. al: https://www.perplexity.ai/search/thoroughly-carefully-accuratel-AyHeYjE8RlGzSlTXcQYK6w
# # 7SEPT2025:
# #  - custom sample_action() ? : https://deepwiki.com/search/i-am-reviewing-a-matlab-script_efc18451-59ed-4855-9ffe-d05c515ee8d5
# #  - ^cross-referencing with MATLAB code (which claims `sample_action_timestep_dependent(agent, timestep=0):` below is accurate and robust as alternative to potential issues of relying on step_time() and curr_timestep internally in agent)
# #      ^- https://www.perplexity.ai/search/thoroughly-carefully-accuratel-AyHeYjE8RlGzSlTXcQYK6w

# # # Create agent with custom policies
# # agent = Agent(A=A_matrices, B=B_matrices, C=C_vectors,
# #               policies=policies, policy_len=2, inference_algo="MMP",
# #               use_BMA=False,      # still compatible, but involves averaging over all policies to compute latest_belief
# #               policy_sep_prior=True    # tracks distinct belief states FOR EACH POLICY instead of averaging; it can be interesting to view
# #               sampling_mode='full',    # this needs to be used: "full" mode: Samples a complete policy, then extracts the timestep-specific action (as opposed to 'marginal':  Marginalizes over all policies to create action probability distributions, then samples from those, which might not correspond to any of the actual hard-coded policies)
# #                                        # For the Adams spm code, Actions for each timestep are extracted directly from the chosen policy, which means that the agent's behavior is derived from one full policy per episode (i.e., it is not marginalizing over multiple policies).
# #          )     # note: inference_horizon can equal 1 to reduce overhead since we are handling timestep tracking externally with this function

# # # Run your behavioral task loop
# # number_of_timesteps_in_behavioral_task = 2
# # for t in range(number_of_timesteps_in_behavioral_task):
# #     qs = agent.infer_states(observation)
# #     q_pi, G = agent.infer_policies()
# #     sampled_action = sample_action_timestep_dependent(agent, timestep=t)
# #     print(f"Timestep {t}: Action {sampled_action}")

# # ## Access beliefs for policy p_idx at timestep t_idx for factor f_idx
# # belief = agent.qs[p_idx][t_idx][f_idx]

# # # Example: Get beliefs for all policies at current timestep (timestep 0)
# # for p_idx in range(len(agent.policies)):
# #     current_beliefs = agent.qs[p_idx][0]  # Current timestep beliefs
# #     print(f"Policy {p_idx} beliefs: {current_beliefs}")

# infer_states_info test infer_states_info() test

A, B, C, D, E, policies, pA, pB, pD, A2, B2, C2, D2, pA2, pB2, pD2 = construct_agent_components()

agent = Agent(A, B, C, D, E, pA=pA, pB=pB, pD=pD, gamma=1.0,
                             policies=policies, policy_len=2, inference_horizon=3, control_fac_idx=[0,2,3],
                             inference_algo="MMP", sampling_mode='full', policy_sep_prior=True,
                             save_belief_hist=True, use_BMA=False)
agent_info = Agent(A, B, C, D, E, pA=pA, pB=pB, pD=pD, gamma=1.0,
                             policies=policies, policy_len=2, inference_horizon=3, control_fac_idx=[0,2,3],
                             inference_algo="MMP", sampling_mode='full', policy_sep_prior=True,
                             save_belief_hist=True, use_BMA=False)


obs = [ [2,2,1,2],   #(null,null,low,null)
        [0,2,0,2],   #(blue,null,high,null)
        [0,2,0,3] ]  #(blue,null,high,withdraw)
#qs_seq, F, xn, vn = infer_states_info(agent_info, obs)
for i in range(2):
  print(f"agent iteration {i}------------------------------------")
  qs = agent.infer_states(obs[i])
  q_pi, G = agent.infer_policies()
  action = sample_action_timestep_dependent(agent, i)
  print(f"qs: {len(qs)}, {len(qs[0])}, {len(qs[0][0])}, {len(qs[0][0][0])}")
  print(f"q_pi: {q_pi}")
  print(f"G: {G}")
  print(action)
print("=====================================================================================================================")
for t in range(3):
  print(f"agent_info iteration {t}-------------------------------")
  qs, xn, vn = infer_states_info(agent_info, obs[t])
  q_pi, G, expected_utility, info_gain = infer_policies_info(agent_info)   #pA_info_gain, pB_info_gain
  if t == 2:
    gamma, beta, gamma_history, q_pi_history = update_gamma(agent_info, num_iterations=16, step_size=2)
  action = sample_action_timestep_dependent(agent_info, t)
  #update_C_MMP_distributional(agent_info, obs[t], lr_pC=1.0, initial_scale=1.0, distr_obs=False, modalities_to_learn=[1,2], monitoring=False)
  #update_E(agent_info, q_pi, monitoring=True)
  print(f"qs: {len(qs)}, {len(qs[0])}, {len(qs[0][0])}, {len(qs[0][0][0])}")
  print(f"xn: {len(xn)}, {len(xn[0])}, {len(xn[0][0])}, {len(xn[0][0][0])}, {len(xn[0][0][0][0])}")
  print(f"xn[0][0][0] = {xn[0][0][0]}")
  print(f"vn: {len(vn)} policies, {len(vn[0])} iterations, {len(vn[0][0])}, {len(vn[0][0][0])}, {len(vn[0][0][0][0])}")
  qs_bma = compute_current_qs_bma(agent_info, current_timestep_idx=t, save_history=True)
  if t == 0:
    qs_info_0 = copy.deepcopy(qs)
    qs_bma_0 = copy.deepcopy(qs_bma)
    xn_info_0 = copy.deepcopy(xn)
    vn_info_0 = copy.deepcopy(vn)
  if t == 1:
    qs_info_1 = copy.deepcopy(qs)
    qs_bma_1 = copy.deepcopy(qs_bma)
    xn_info_1 = copy.deepcopy(xn)
    vn_info_1 = copy.deepcopy(vn)
  if i==2:
    agent.step_time()

  print(f"q_pi: {q_pi}")
  print(f"G: {G}")
  print(expected_utility)
  print(info_gain)
  print(action)

    #   qs_seq_pi: ``numpy.ndarray`` of dtype object
    #     Posterior beliefs over hidden states for each policy. Nesting structure is policies, timepoints, factors,
    #     where e.g. ``qs_seq_pi[p][t][f]`` stores the marginal belief about factor ``f`` at timepoint ``t`` under policy ``p``.
    # F: 1D ``numpy.ndarray``
    #     Vector of variational free energies for each policy
    # xn_seq_pi: ``numpy.ndarray`` of dtype object
    #     Posterior beliefs over hidden states for each policy, for each iteration of marginal message passing.
    #     Nesting structure is policy, iteration, factor, so ``xn_seq_p[p][itr][f]`` stores the ``num_states x infer_len``
    #     array of beliefs about hidden states at different time points of inference horizon.
    # vn_seq_pi: `numpy.ndarray`` of dtype object
    #     Prediction errors over hidden states for each policy, for each iteration of marginal message passing.
    #     Nesting structure is policy, iteration, factor, so ``vn_seq_p[p][itr][f]`` stores the ``num_states x infer_len``
    #     array of beliefs about hidden states at different time points of inference horizon.



xn_bma_current = compute_xn_bma(None, xn, policy_len=2)
vn_bma_current = compute_xn_bma(None, vn, policy_len=2)

print(xn_bma_current[0][0])

# xn = output from your agent, shape is [iterations][factor], where each [factor] entry is a 1D probability vector
import matplotlib.pyplot as plt

for i in range(len(xn_bma_current[0])):
  plot_spm_lfp(xn_bma_current, factor_idx=i)

# gamma_history = [float gamma value per inference iteration], output from your agent
plot_spm_dopamine(gamma_history)

# # Comparing policy_len and inference_horizon settings --> qs structure

# # # policy_len=1, inference_horizon=1
# # qs_info_0_1_1 = copy.deepcopy(qs_info_0)
# # qs_info_1_1_1 = copy.deepcopy(qs_info_1)
# # qs_info_2_1_1 = copy.deepcopy(qs)
# # ^^ qs[0].shape = (3,) at t=0, (3,) at t=1, (3,) at t=2

# # # policy_len=2, inference_horizon=1
# # qs_info_0_2_1 = copy.deepcopy(qs_info_0)
# # qs_info_1_2_1 = copy.deepcopy(qs_info_1)
# # qs_info_2_2_1 = copy.deepcopy(qs)
# # ^^ qs[0].shape = (3,) at t=0, (3,) at t=1, (3,) at t=2

# # # policy_len=1, inference_horizon=3
# # qs_info_0_1_3 = copy.deepcopy(qs_info_0)
# # qs_info_1_1_3 = copy.deepcopy(qs_info_1)
# # qs_info_2_1_3 = copy.deepcopy(qs)
# # ^^ qs[0].shape = (3,) at t=0, (5,) at t=1, (5,) at t=2

# # # policy_len=2, inference_horizon=3 (main simulation)
# # qs_info_0_2_3 = copy.deepcopy(qs_info_0)
# # qs_info_1_2_3 = copy.deepcopy(qs_info_1)
# # qs_info_2_2_3 = copy.deepcopy(qs)
# # ^^ qs[0].shape = (3,) at t=0, (5,) at t=1, (5,) at t=2

# # # policy_len=2, inference_horizon=2
# # qs_info_0_2_2 = copy.deepcopy(qs_info_0)
# # qs_info_1_2_2 = copy.deepcopy(qs_info_1)
# # qs_info_2_2_2 = copy.deepcopy(qs)
# # ^^ qs[0].shape = (3,) at t=0, (4,) at t=1, (4,) at t=2

# # # policy_len=4, inference_horizon=4
# # qs_info_0_4_4 = copy.deepcopy(qs_info_0)
# # qs_info_1_4_4 = copy.deepcopy(qs_info_1)
# # qs_info_2_4_4 = copy.deepcopy(qs)

# list_of_qs = [qs_info_0_1_1, qs_info_0_2_1, qs_info_0_1_3, qs_info_0_2_3, qs_info_0_2_2,qs_info_0_4_4]
# list_of_qs1 = [qs_info_1_1_1, qs_info_1_2_1, qs_info_1_1_3, qs_info_1_2_3, qs_info_1_2_2,qs_info_1_4_4]
# list_of_qs2 = [qs_info_2_1_1, qs_info_2_2_1, qs_info_2_1_3, qs_info_2_2_3, qs_info_2_2_2,qs_info_2_4_4]

# # all same at timestep 0 -- timesteps=3
# for i in range(len(list_of_qs)):
#   print(f"{i}")
#   print(f"{list_of_qs[i][0][0][0]}")
#   print(f"{list_of_qs1[i][0][0][0]}")
#   print(f"{list_of_qs2[i][0][0][0]}")
#   print(f"{list_of_qs[i][0].shape}")
#   print(f"{list_of_qs1[i][0].shape}")
#   print(f"{list_of_qs2[i][0].shape}")

# ### FINDINGS:
# # - all configs have minimum (3,) timesteps (qs[0].shape)
# # - increasing policy_len does NOT increase total timesteps t
# # - increasing inference_horizon DOES increase total timesteps t
# # - thus, we only see total timesteps t increase in when inference_horizon>1, with linear proportionate increase,
# #   such that when inference_horizon=3 (regardless of policy_len), timesteps reach 5.
# # - future_len ends up being policies.shape[0], meaning that it is *independent of setting policy_len manually*--our future_len is going to be 2
# #   as we are consistently using the same policies
# # - future_len depends on the *actual length of the policies*--since we are manually passing in two-timestep policies, the policy_len argument
# #   is ignored in the Agent() constructor (thus we changing policy_len while retaining inference_horizon leads to no change in timestep count,
# #   the pre-constructed policies we're submitting are still the same length every time). This means it's as if policy_len is always 2 here.

# print(type(xn_info_0))
# print(xn_info_0.shape)
# print(xn_info_0[0][0][0])
# print(xn_info_0[0][0][0][0])

# print(type(xn_info_1))
# print(xn_info_1.shape)
# print(xn_info_1[0][0][0])

# print("")
# print(qs_bma_0)
# print("")
# print(qs_bma_1)

print(xn[0][0][0])
print(qs_bma)

# from scipy.signal import butter, filtfilt
# import numpy as np


# xn_bma_current = compute_xn_bma(None, xn, policy_len=2)
# vn_bma_current = compute_xn_bma(None, vn, policy_len=2)

# def stack_xn_over_iterations(xn, factor_idx):
#     # Converts every entry to np.array of float
#     return np.stack([np.asarray(xn_iter[factor_idx], dtype=np.float64) for xn_iter in xn])

# def stack_vn_over_iterations(vn, factor_idx):
#     return np.stack([np.asarray(vn_iter[factor_idx], dtype=np.float64) for vn_iter in vn])

# def bandpass_filter(data, lowcut=1, highcut=4, fs=16, order=2):
#     # Reasonable defaults for per-iteration band extraction; adjust fs as needed
#     nyq = 0.5 * fs
#     b, a = butter(order, [lowcut / nyq, highcut / nyq], btype='band')
#     return filtfilt(b, a, data, axis=0)

# def compute_lfp(xn, factor_idx, fs=16):
#     xn_factor = stack_xn_over_iterations(xn, factor_idx)
#     log_xn = np.log(xn_factor + 1e-8)
#     # For each state of the factor, filter across inference iterations
#     lfp = bandpass_filter(log_xn, fs=fs)
#     return lfp

# def compute_dopamine(gamma_history):
#     gamma_arr = np.array(gamma_history)
#     dn = np.gradient(gamma_arr)
#     return dn
# import matplotlib.pyplot as plt

# def plot_spm_style(xn, vn, gamma_history, factor_idx=0, fs=16):
#     lfp = compute_lfp(xn, factor_idx, fs=fs)
#     dn = compute_dopamine(gamma_history)

#     fig, axs = plt.subplots(3, 1, figsize=(12, 7))
#     axs[0].plot(np.exp(lfp))  # exp(log_xn) recovers the probability trace
#     axs[0].set_title(f"Factor {factor_idx}: Synthetic LFP (band-passed log-belief)")
#     axs[0].set_ylabel("LFP")
#     axs[1].plot(stack_vn_over_iterations(vn, factor_idx))
#     axs[1].set_title(f"Factor {factor_idx}: Prediction Error Trace")
#     axs[1].set_ylabel("Prediction error")
#     axs[2].plot(dn)
#     axs[2].set_title("Dopamine/Precision (gamma)")
#     axs[2].set_ylabel("Precision change")
#     axs[2].set_xlabel("VB Iteration")
#     plt.tight_layout()
#     plt.show()

# plot_spm_style(xn_bma_current, vn_bma_current, gamma_history, factor_idx=0, fs=16)

# import numpy as np

# def analyze_vn_structure(vn):
#     print("Type of vn:", type(vn))
#     if isinstance(vn, np.ndarray):
#         print("Shape of vn:", vn.shape)
#         print("Dtype of vn:", vn.dtype)
#     else:
#         print("vn is not a numpy array.")
#     # Optionally, for nested object arrays, print depths or example values
#     def recursive_shape(obj, level=0):
#         if isinstance(obj, np.ndarray) and obj.dtype == 'O':  # object array (possibly nested arrays)
#             print("  " * level + f"Level {level}: Array of length {len(obj)}")
#             for i, item in enumerate(obj):
#                 print("  " * level + f"  Item {i}: {type(item)}")
#                 recursive_shape(item, level+1)
#         elif isinstance(obj, np.ndarray):
#             print("  " * level + f"Level {level}: Array shape {obj.shape}, dtype {obj.dtype}")
#         else:
#             print("  " * level + f"Level {level}: Leaf value type {type(obj)}")
#     print("Nested structure analysis:")
#     recursive_shape(vn)
#     print("Sample contents:")
#     #print(vn)

# # Usage:
# analyze_vn_structure(vn)

# `vn`: Prediction errors over hidden states for each policy, for each iteration of marginal message passing.
# Nesting structure is policy, iteration, factor, so ``vn[p][itr][f]`` stores the ``num_states x infer_len``
# array of beliefs about hidden states at different time points of inference horizon.
# In our simulation, there are 5 hidden state factors ("factors"). Each of them is represented, for example, in the `D`
# matrix which is composed of submatrices of prior beliefs over hidden states, one submatrix per factor.
# The 5 hidden state factors have their own respective hidden states:
# len(D[0]): 2 states
# len(D[1]): 2 states
# len(D[2]): 2 states
# len(D[3]): 4 states
# len(D[4]): 3 states

# Define the logic for the CardAdvisor task

class CardAdvisor(object):

  def __init__(self, trustworthy_process = None, correct_card = None, safety_belief=None):
    """
    Simulates an environment for a card-advice trust task.

    The CardAdvisor class models an environment where an agent must select the color of an initially unseen card, where
    the card can only be blue or green.
    The advisor provides advice to the agent to select between two card colors ("blue" or "green") prior to revealing the true color.
    The advisor can be either trustworthy (gives correct advice) or untrustworthy (gives incorrect advice) relative to the true color of the card.
    The agent receives feedback as to whether the card was correct or not and simulated arousal responses based on outcomes.

    Args:
        trustworthy_process (str, optional): Advisor's trustworthiness for the episode.
            - "trustworthy": Advisor gives correct advice.
            - "untrustworthy": Advisor gives incorrect advice.
            - If None, randomly chosen at initialization.
        correct_card (str, optional): The correct card for the episode.
            - "blue" or "green".
            - If None, randomly chosen at initialization.

    Attributes:
        correct_card_names (list): Possible correct card colors ("blue", "green").
        trustworthy_names (list): Possible advisor types ("trustworthy", "untrustworthy").
        correct_card (str): The correct card for the current trial.
        trustworthy_process (str): Advisor's trustworthiness for the current trial.
        choice_obs (list): Possible observed card choices.
        feedback_obs (list): Possible feedback outcomes.
        #stage_obs (list): Possible task stages.
        advice_obs (list): Possible advice options.
        arousal_obs (list): Possible arousal states ("high", "low").

    Public Methods:
        step(trust_action, card_action):
            Processes the agent's trust and card actions, returning a list of observations:
            [advice, feedback, arousal, choice].
    """
    # Set environment's true advisor trustworthiness and true correct card color
    self.correct_card_names = ["blue", "green"]
    self.trustworthy_names = ["trustworthy", "untrustworthy"]

    if correct_card == None:
      self.correct_card = self.correct_card_names[utils.sample(np.array([0.5, 0.5]))] # randomly sample correct card color (blue or green)
    else:
      self.correct_card = correct_card

    if trustworthy_process == None:
      self.trustworthy_process = self.trustworthy_names[utils.sample(np.array([0.5, 0.5]))] # randomly sample advisor trustworthiness (trustworthy, untrustworthy)
    else:
      self.trustworthy_process = trustworthy_process

    self.choice_obs = ['blue', 'green','null','withdraw']
    self.feedback_obs = ['correct', 'incorrect', 'null']
    #self.stage_obs = ['null', 'advice', 'decision']
    self.advice_obs = [ 'blue', 'green', 'null']
    self.arousal_obs = ['high', 'low']

  def step(self, trust_action,  card_action, agent_for_arousal_modulation=False, monitoring=False):
    """Process agent actions and return environment observations.
    The arguments pass the lower level agent's actions to the environment, advancing the stages of the trial.

    Args:
        trust_action (str): Trust-related action from agent. One of:
            - "trust": Trust advisor (outcome of 'low' arousal with probability 0.667, else returns 'high' arousal)
            - "distrust": Distrust advisor (outcome of 'high' arousal with probability 0.667, else returns 'low' arousal)
        card_action (str): Card selection action. One of:
            - "blue"/"green": Card choice (during final stage, outcome of 'correct' if card choice is equivalent to true card color, else 'incorrect')
            - "null": No selection (null feedback/choice)

    Returns:
        list: Observation components [advice, feedback, arousal, choice]:
            - advice (str): Advisor's recommendation ("blue"/"green")
            - feedback (str): "correct"/"incorrect" if card played, else "null"
            - arousal (str): "high"/"low" emotional response
            - choice (str): Matches card_action or "null"

    Observation Logic:
        - Advice reflects advisor's trustworthiness (truthful vs. reversed)
        - Feedback shows if card matched environment's correct card
        - Arousal probabilities depend on trust_action type
        - Choice directly mirrors card_action input
    """

    # observed_advice: uncontrollable by agent; advice matches true color if trustworthy else does not match)
    if self.trustworthy_process == "trustworthy":
          if self.correct_card == "blue":
              observed_advice = "blue"            # if trustworthy, then match correct card to observed advice
          elif self.correct_card == "green":
              observed_advice = "green"
    elif self.trustworthy_process == "untrustworthy":
          if self.correct_card == "blue":
              observed_advice = "green"
          elif self.correct_card == "green":
              observed_advice = "blue"

    # observed_arousal: dependent on agent's 'trust' action
    if trust_action == "trust":
      #observed_arousal = arousal_obs[utils.sample(np.array([0.5, 0.5]))]
      observed_arousal = self.arousal_obs[utils.sample(np.array([0.3333, 0.6667]))] # if choose to trust, 2/3 probability of low arousal
    elif trust_action == "distrust":
      observed_arousal = arousal_obs[utils.sample(np.array([0.6667, 0.3333]))]    # if choose to distrust, 2/3 probability of high arousal
    elif trust_action == "null":
      observed_arousal = arousal_obs[utils.sample(np.array([0.5, 0.5]))]  # for initial observation generation at start of 'null' stage (independent of agent)

    # observed_choice and observed_feedback: dependent on agent's choice (always 'null' in at end of initial 'null' stage; one of 'blue','green','withdraw' at end of 'advice' stage')
    if card_action == "null":
      observed_choice = "null"
      observed_feedback = "null"
    if card_action == "withdraw":
      observed_choice = "withdraw"
      observed_feedback = "null"
    if card_action == "blue":
      observed_choice = "blue"   # map observed choice to card choice (observe self)
      if self.correct_card == "blue":
          observed_feedback = "correct"
      elif self.correct_card == "green":
          observed_feedback = "incorrect"
    if card_action == "green":
      observed_choice = "green"
      if self.correct_card == "blue":
          observed_feedback = "incorrect"
      elif self.correct_card == "green":
          observed_feedback = "correct"

    # Interoceptive 'arousal' observation based on beliefs (Autonomic state gives rise to cardiac outcome, tachy/bradycardia))
    if agent_for_arousal_modulation != False:
      true_trustworthiness_idx = self.trustworthy_names.index(self.trustworthy_process)   # get true trustworthy state
      true_correct_color_idx = self.correct_card_names.index(self.correct_card)           # get true correct card state
      # get agent's affect belief state
      if monitoring==True:
        print(f"Extract max_affect_idx for arousal modulation from A matrix...")
      if hasattr(agent_for_arousal_modulation, 'qs_current_bma'):
        max_affect_idx = np.argmax(agent_for_arousal_modulation.qs_current_bma[2])  #agent_for_arousal_modulation.qs_current_bma[2].index(max(agent_for_arousal_modulation.qs_current_bma[2]))
      else:
        max_affect_idx = np.argmax(agent_for_arousal_modulation.D[2])

      if monitoring==True:
        print(f"max_affect_idx = {max_affect_idx}")
      true_choice_idx = self.choice_obs.index(card_action)
      if card_action == "null":
        if trust_action == "null":
          true_stage_idx = 0
        else:
          true_stage_idx = 1
      elif card_action != "null":
        true_stage_idx = 2
      # get agent's likelihood
      arousal_probs_array = agent_for_arousal_modulation.A[2][:,true_trustworthiness_idx,true_correct_color_idx,max_affect_idx,true_choice_idx,true_stage_idx]  # Per (Adams et al, 2022)
      if monitoring==True:
        print(f"arousal_probs_array = {arousal_probs_array}")
        print(f"arousal_probs_array.shape = {arousal_probs_array.shape}")
      observed_arousal = self.arousal_obs[np.argmax(arousal_probs_array)]

    obs = [observed_advice, observed_feedback, observed_arousal, observed_choice]

    return obs, arousal_probs_array[0]


def make_env_rules_df(CardAdvisor, true_trustworthiness_list, true_color_list, choice_trust_actions, choice_card_actions):
    records = []
    for true_color in true_color_list:
        for true_trustworthiness in true_trustworthiness_list:
            env_test = CardAdvisor(trustworthy_process=true_trustworthiness, correct_card=true_color)
            for card_action in choice_card_actions:
                for trust_action in choice_trust_actions:
                    obs_label = env_test.step(trust_action=trust_action, card_action=card_action)
                    record = {
                        'true_trustworthiness': true_trustworthiness,
                        'true_color': true_color,
                        'trust_action': trust_action,
                        'card_action': card_action,
                        'observed_advice': obs_label[0],
                        'observed_feedback': obs_label[1],
                        'observed_arousal': obs_label[2],
                        'observed_choice': obs_label[3]
                    }
                    records.append(record)
    df = pd.DataFrame.from_records(records)
    return df

# Print all possible combinations of environment dynamics
#env_rules_df = make_env_rules_df(CardAdvisor, true_trustworthiness_list=['trustworthy','untrustworthy'], true_color_list=['blue','green'], choice_trust_actions=choice_trust_actions, choice_card_actions=choice_card_actions)
#display(env_rules_df.sort_values('card_action'))

# arousal_obs = ['high','low']
# arousal_probs_array = agent.A[2][:,0,0,0,0,0]
# print(arousal_obs[np.argmax(arousal_probs_array)])
print(agent_info.qs_current_bma[2])
print(np.argmax(agent_info.qs_current_bma[2]))
#max_affect_idx = agent_info.qs_current_bma[2].index(max(agent_for_arousal_modulation.qs_current_bma[2]))



env_test = CardAdvisor(trustworthy_process="untrustworthy",
                                correct_card="blue")
obs_label = env_test.step(trust_action="trust",card_action="null", agent_for_arousal_modulation=agent_info)  # Transition (step) from first 'null' step of trial to second 'advice' step of trial, passing 'null' as both the agent's action and agent's card choice
print(obs_label)

# Define the full perception-action loop, incorporating the hierarchical agent and Card Advisor task, with arguments
# for determining whether the lower lever and higher level agents should learn their respective matrices.

# NOTE!!**: 11SEPT2025 MATLAB flow and sampling observations from agent's A matrix:

def run_active_inference_loop(my_agent,                 # Lower level agent
                              my_env,                   # Environment
                              T = 3,                    # Number of timesteps per trial
                              trial_results=False,      # Indicator to record results for trials
                              learn_A=False,            # learn A at lower level
                              learn_B=False,            # learn B at lower level
                              learn_C=False,            # learn C at lower level
                              learn_D=False,            # learn D at lower level
                              learn_E=False,            # learn E at lower level       # ADDED 17AUG2025
                              learn_gamma = False,       # learn gamma at lower level   # ADDED 17AUG2025
                              my_agent_high = None,     # Supply higer level agent
                              learn_A2=False,           # learn A2 at higher level
                              learn_B2=False,           # learn B2 at higher level
                              learn_D2=False,            # learn D2 at higher level
                              monitoring=False,
                              lower_learning_timesteps='all',   # list of timesteps for lower level agent to learn matrices, e.g., [2] for only at end of the second timestep, or 'all' to match T
                              arousal_modulation_from_A=False
                              ):
  if lower_learning_timesteps == 'all':
    lower_learning_timesteps = list(range(T))
  """ Initialize the first observation """
  arousal_obs_names = ['high', 'low']
  if monitoring==True:
    print(f"t=0: Generating initial observations..")
  if arousal_modulation_from_A == False:
    obs_label = my_env.step(trust_action="null",card_action="null")  # Transition (step) from first 'null' step of trial to second 'advice' step of trial, passing 'null' as both the agent's action and agent's card choice
  else:
    obs_label, arousal_prob = my_env.step(trust_action="null",card_action="null", agent_for_arousal_modulation=my_agent)
  if monitoring==True:
    print(f"t=0: initial obs_label={obs_label}")
  obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]   # Return initial observations `obs` elicited from environment
  if monitoring == True:
    print(f"t=0 pre-trial initial observation obs_label={obs_label}")
  if trial_results == True:
    results = {}

  for t in range(T):
    if trial_results == True:
      results[t] = {}
    if monitoring==True:
      print(f"t={t} : my_agent.infer_states")
    # All timesteps: infer hidden states from observation
    qs, xn, vn = infer_states_info(my_agent, obs)    # Lower level agent infers posterior beliefs (updates beliefs) based on initial observations `obs`
    # First and second timesteps (Initial null stage and advice stage): infer policies; update policy precision; sample actions; generate new observation for next timestep
    if t != 2:
      if monitoring==True:
        print(f"t={t} : my_agent.infer_policies")
      #q_pi, neg_efe = my_agent.infer_policies()       # Lower level infers policies (computes posteriors over policies Q(pi) as well as negative EFE per policy
      q_pi, neg_efe, expected_utility, info_gain = infer_policies_info(my_agent)       # Lower level infers policies (computes posteriors over policies Q(pi) as well as negative EFE per policy
      if learn_gamma == True:
        if monitoring==True:
          print(f"t={t} : update_gamma(my_agent, num_iterations=16, step_size=2)")
        gamma, beta, gamma_history, q_pi_history = update_gamma(my_agent, num_iterations=16, step_size=2)
      else:
        beta, gamma_history, q_pi_history = None, None, None
      if monitoring==True:
        print(f"t={t} : sample_action_timestep_dependent(my_agent,t)")
      chosen_action = sample_action_timestep_dependent(my_agent, t)    # Agent samples action from two-step policy chosen (samples from the step in policy based on if t==0 or if t==1)
      trust_action = choice_trust_actions[int(chosen_action[0])]    # Extract trust action (trust/distrust) from sampled policy for current timestep. The trust_action controls both trustworthiness_states and affect_states and is equivalent in each policy (always trust and trust or distrust and distrust)
      card_action = choice_card_actions[int(chosen_action[3])]      # Extract card choice action (blue/green/null/withdraw) from sampled policy for current timestep
      if monitoring==True:
        print(f"t={t} : sampled actions: {trust_action},{card_action}")
      if arousal_modulation_from_A == False:
        obs_label = my_env.step(trust_action,card_action)             # send trust_action and card_action to environment step to generate observations for next timestep
      else:
        obs_label, arousal_prob = my_env.step(trust_action,card_action, agent_for_arousal_modulation=my_agent)             # send trust_action and card_action to environment step to generate observations for next timestep
      if monitoring==True:
        print(f"t={t} : post-action obs_label={obs_label}")
      obs = [advice_obs.index(obs_label[0]), feedback_obs.index(obs_label[1]), arousal_obs.index(obs_label[2]), choice_obs.index(obs_label[3])]   # Return new observations from the environment
    # No policy inference nor action selection during final timestep (t=2)
    elif t == 2:
      # No policy inference nor action selection following decision
      chosen_action = None
      trust_action = None
      card_action = None
      if update_gamma == True:
        beta = None
        gamma_history = None
        q_pi_history = None

    qs_bma = compute_current_qs_bma(my_agent, current_timestep_idx=None, save_history=True)   # compute Bayesian Model Average of beliefs for current timestep in agent's inference window
    qs_hist = copy.deepcopy(qs_bma)



    if t in lower_learning_timesteps:
      # After trial, learn A/B/D matrices / update higher level / learn A2/B2/D2 (where applicable)
      if learn_A == True:
        if monitoring==True:
          print(f"t={t} : update_A_MMP_distributional(my_agent, obs, distr_obs=False)")
        update_A_MMP_distributional(my_agent,obs, distr_obs=False)

      if learn_B == True:
        if monitoring==True:
          print(f"t={t} : my_agent.update_B(qs_prev_formatted)")
        # After final timestep - format current beliefs and call update_B

        #qs_current_formatted = compute_current_qs_bma(my_agent, current_timestep_idx=None, save_history=True)
        # Temporarily store the formatted current beliefs
        original_qs = my_agent.qs
        my_agent.qs = my_agent.qs_current_bma
        my_agent.update_B(my_agent.qs_prev_bma)
        my_agent.qs = original_qs  # restore original structure
        my_agent.B[1] = B[1]  # Reset to original priors over correct card states
        my_agent.B[3] = B[3]  # Reset to original priors over card choice states
        my_agent.B[4] = B[4]  # Reset to original priors over stage states
      else:
        if monitoring==True:
          print(f"t={t} : skipping my_agent.update_B(qs_prev)")

      if learn_C == True:
        if monitoring==True:
          print(f"t={t} : update_C_MMP_distributional(my_agent, obs, distr_obs=False, modalities_to_learn=[1,2])")
        update_C_MMP_distributional(my_agent, obs, lr_pC=1.0, initial_scale=1.0, distr_obs=False, modalities_to_learn=[1,2])

      if learn_D == True:
        if monitoring==True:
          print(f"t={t} : my_agent.curr_timestep = {my_agent.curr_timestep}")
          print(f"t={t} : my_agent.update_D()")
        my_agent.update_D()
        # The agent only learns the D submatrices (updates parameters for priors over initial hidden states) for trustworthiness and affect
        my_agent.D[1]=D[1]  # Reset to original priors over correct card states
        my_agent.D[3]=D[3]  # Reset to original priors over card choice states
        my_agent.D[4]=D[4]  # Reset to original priors over stage states

      if learn_E == True:
        if monitoring==True:
          print(f"t={t} : update_E(my_agent, q_pi, lr_pE=1.0, initial_scale=1.0)")
        update_E(my_agent, q_pi, lr_pE=1.0, initial_scale=1.0)


    if trial_results == True:
        if monitoring==True:
          print(f"t={t} : Storing my_agent learning results")
        results[t]['timestep'] = t
        results[t]['qs'] = my_agent.qs
        results[t]['xn'] = xn
        results[t]['vn'] = vn
        results[t]['qs_bma'] = qs_bma
        results[t]['q_pi'] = q_pi
        results[t]['neg_efe'] = neg_efe
        results[t]['expected_utility'] = expected_utility
        results[t]['info_gain'] = info_gain
        if monitoring==True:
          print(f"t={t} : Storing chosen_action={chosen_action}, trust_action={trust_action}, card_action={card_action}")
        results[t]['trust_action'] = trust_action
        results[t]['card_action'] = card_action
        results[t]['chosen_action'] = chosen_action
        results[t]['obs_label'] = obs_label
        results[t]['obs'] = obs
        results[t]['true_trustworthiness'] = my_env.trustworthy_process
        results[t]['true_color'] = my_env.correct_card
        results[t]['A'] = my_agent.A  # record results for learned A
        results[t]['B'] = my_agent.B  # record results for learned B
        results[t]['C'] = my_agent.C  # record results for learned B
        results[t]['D'] = my_agent.D  # record results for learned D
        results[t]['E'] = my_agent.E  # record results for learned E
        if learn_gamma == True:
          results[t]['gamma'] = my_agent.gamma  # record results for learned gamma
          results[t]['beta'] = my_agent.beta
          results[t]['gamma_history'] = my_agent.gamma_history
          results[t]['policy_posteriors_history'] = my_agent.policy_posteriors_history
          #gamma, beta, gamma_history, q_pi_history
        if arousal_modulation_from_A == False:
          results[t]['high_arousal_prob'] = np.nan
        else:
          results[t]['high_arousal_prob'] = arousal_prob

    # if trial_results == True:
    #   # Record trial results
    #     results[t] = {'timestep' : t, 'qs_hist' : qs_hist, 'q_pi' : q_pi, 'efe' : neg_efe*-1, 'trust_action' : trust_action, 'card_action' : card_action, 'obs_label' : obs_label, 'obs' : obs,
    #                   'D' : my_agent.D, 'true_trustworthiness' : my_env.trustworthy_process, 'true_color': my_env.correct_card, 'A':my_agent.A, 'B':my_agent.B, 'C' : my_agent.C, 'E' : my_agent.E, 'gamma' : my_agent.gamma,
    #                   'chosen_action' : chosen_action, 'qs_bma' : qs_bma, 'beta' : beta, 'q_pi_history' : q_pi_history, 'gamma_history' : gamma_history}

  if my_agent_high != None:
    if monitoring==True:
      print(f"t={t}: Storing my_agent_high 'prev' matrices")
    # Store higher level agent's previous matrices
    my_agent_high_A_prev = copy.deepcopy(my_agent_high.A)
    my_agent_high_B_prev = copy.deepcopy(my_agent_high.B)
    my_agent_high_C_prev = copy.deepcopy(my_agent_high.C)
    my_agent_high_D_prev = copy.deepcopy(my_agent_high.D)
    # if hierarchical agent already has a qs history, store currently expected obs for result storing `qo_pi_high` in results for trial's first timestep (otherwise, initialize with higher level agent's preferences my_agent_high.C i.e. C2)
    # and also use BMA for qs
    if len(my_agent_high.qs_hist) > 0:
      qo_pi_high_prev = pymdp.control.get_expected_obs(my_agent_high.latest_belief, my_agent_high.A)   # Compute expected obs at higher level, given new posteriors
      qo_pi_high_prev = [subarray / subarray.sum() for subarray in qo_pi_high_prev[0]]   # Extract expected obs at current timestep, normalize each distribution
      # qs_high_current_timestep = utils.obj_array(len(my_agent_high.policies))
      # for p_idx in range(len(my_agent_high.policies)):
      #     qs_high_current_timestep[p_idx] = my_agent_high.qs[p_idx][0]  # timestep 1
      #     print(f"for p_idx(={p_idx}) in range(len(my_agent_high.policies)): qs_high_current_timestep[p_idx={p_idx}] = my_agent_high.qs[p_idx={p_idx}][0] = {my_agent_high.qs[p_idx][0]}***")

      # qs_high_prev_formatted = inference.average_states_over_policies(qs_high_current_timestep, my_agent_high.q_pi)
      qs_high_prev_formatted = compute_current_qs_bma(my_agent_high, current_timestep_idx=None, save_history=True)
    else:
      qo_pi_high_prev = my_agent_high_C_prev
      qs_high_prev_formatted = my_agent_high_D_prev

    if monitoring==True:
        print(f"t={t} : qs_high_prev_formatted = {qs_high_prev_formatted}")

    # L2 state inference -- higher level agent updates posterior beliefs using the lower level agent's posterior belief distribution as observations for the higher level
    #current_qs_per_policy = [my_agent.qs[p_idx][0] for p_idx in range(len(my_agent.policies))]     # note: qs is qs[policy_idx][timestep_idx][factor_idx] ; this uses timestep_idx=0
    #qs_for_hierarchy = inference.average_states_over_policies(current_qs_per_policy, my_agent.q_pi)
    qs_for_hierarchy = my_agent.qs_current_bma
    if monitoring==True:
      print(f"t={t}: computed qs_for_hierarchy with type {qs_for_hierarchy.dtype} = {qs_for_hierarchy}")
      print(f"t={t}: my_agent_high.infer_states(qs_for_hierarchy, distr_obs=True)")
    #qs_high = my_agent_high.infer_states(qs_for_hierarchy, distr_obs=True)
    qs_high, xn_high, vn_high = infer_states_info(my_agent_high, qs_for_hierarchy, distr_obs=True)
    if monitoring==True:
      print(f"t={t}: qs_high = {qs_high}")

    # L2 policy inference and 'action selection'
    if monitoring==True:
      print(f"t={t}: my_agent_high.infer_policies()")
    #q_pi_high, neg_efe_high = my_agent_high.infer_policies()              # higher level agent infers its posterior beliefs about policies (a necessary step despite the higher level agent only having a single 'NULL' action)
    q_pi_high, neg_efe_high, expected_utility_high, info_gain_high = infer_policies_info(my_agent_high)           # higher level agent infers its posterior beliefs about policies (a necessary step despite the higher level agent only having a single 'NULL' action)

    action_high = my_agent_high.sample_action()                       # higher level agent samples action from its posterior beliefs about policies ()
    if monitoring==True:
      print(f"t={t}: structure of qs_high with type {type(qs_high)} and length {len(qs_high)} = {qs_high}")
    qo_pi_high = pymdp.control.get_expected_obs(my_agent_high.latest_belief, my_agent_high.A)   # compute expected obs at higher level, given new posteriors
    qo_pi_high = [subarray / subarray.sum() for subarray in qo_pi_high[0]]   # extract expected observations from higher level agent at current timestep, normalize each distribution

    my_agent.D = qo_pi_high    # lower level inherits empirical priors from higher level
    # The agent only learns the D submatrices (updates parameters for priors over initial hidden states) for trustworthiness and affect
    my_agent.D[1]=D[1]  # Reset to original priors over correct card states
    my_agent.D[3]=D[3]  # Reset to original priors over card choice states
    my_agent.D[4]=D[4]  # Reset to original priors over stage states
    if trial_results == True:
      results[T-1]['D'] = my_agent.D

  if my_agent_high != None and learn_A2 == True:
    if monitoring==True:
      print(f"t={t}: update_A_MMP_distributional(my_agent_high,qs_for_hierarchy, distr_obs=True)")
    update_A_MMP_distributional(my_agent_high, qs_for_hierarchy, distr_obs=True)    # higher level agent learns its A matrix at end of trial

  if my_agent_high != None and learn_B2 == True:
    # print("Learning B2:")
    # print(f"my_agent_high_D_prev = {my_agent_high_D_prev}")
    # print(f"qs_high = {qs_high}")
    if monitoring==True:
      print(f"t={t}: my_agent_high.update_B(qs_high_prev_formatted)")


    qs_high_current_final = utils.obj_array(len(my_agent_high.policies))
    for p_idx in range(len(my_agent_high.policies)):
        qs_high_current_final[p_idx] = my_agent_high.qs[p_idx][1]  # timestep t beliefs
    if monitoring==True:
      print(f"t={t} : qs_high_prev_formatted = {qs_high_prev_formatted}")
      print(f"t={t} : qs_high_current_formatted = inference.average_states_over_policies(qs_high_current_final, my_agent_high.q_pi)")
      print(f"--qs_high_current_final = {qs_high_current_final}")
      print(f"--my_agent_high.q_pi = {my_agent_high.q_pi}")
    qs_high_current_formatted = inference.average_states_over_policies(
        qs_high_current_final, my_agent_high.q_pi
    )
    if monitoring==True:
      print(f"--qs_high_current_formatted = {qs_high_current_formatted}")

    # Temporarily store the formatted current beliefs
    original_qs_high = my_agent_high.qs
    my_agent_high.qs = qs_high_current_formatted
    my_agent_high.update_B(qs_high_prev_formatted)
    my_agent_high.qs = original_qs_high  # restore original structure


    #my_agent_high.update_B(my_agent_high_D_prev)    # higher level agent learns its B matrix at end of trial
  if my_agent_high != None and learn_D2 == True:
    if monitoring==True:
      print(f"t={t}: my_agent_high.update_D(my_agent_high.sqs)")
    my_agent_high.update_D()    #my_agent_high.qs)        # higher level agent learns its D matrix at end of trial
  if my_agent_high != None and trial_results == True:
    for t in range(T-1):
      # Record results for higher level agent. Since the results are stored for each timestep per trial but the higher level agent's learning and updating
      # only occur at the end of the trial, the higher level agent's results will be equivalent for the first two timesteps of the trial and only change
      # for the final timestep.
      results[t]['A2'] = my_agent_high_A_prev
      results[t]['B2'] = my_agent_high_B_prev
      results[t]['D2'] = my_agent_high_D_prev
      results[t]['qs_high'] = qs_high_prev_formatted   #my_agent_high_D_prev
      results[t]['qs_high_bma'] = qs_high_prev_formatted
      results[t]['qo_pi_high'] = qo_pi_high_prev

    results[T-1]['A2'] = my_agent_high.A
    results[T-1]['B2'] = my_agent_high.B
    results[T-1]['D2'] = my_agent_high.D
    results[T-1]['qs_high'] = my_agent_high.qs
    results[T-1]['xn_high'] = xn_high
    results[T-1]['vn_high'] = vn_high
    results[T-1]['qs_high_bma'] = qs_high_current_formatted
    results[T-1]['qo_pi_high'] = qo_pi_high

  #my_agent.reset()                  # forces uniform distribution
  #my_agent_high.reset()             # forces uniform distribution
  my_agent.curr_timestep = 0
  my_agent_high.curr_timestep = 0

  if trial_results == True:
    return results

import ast

def safe_literal_eval(val):
    try:
        # Only evaluate strings that look like lists
        if isinstance(val, str) and val.startswith('[') and val.endswith(']'):
            return ast.literal_eval(val)
        else:
            # Return original value if not a valid list string
            return val
    except Exception:
        # Return original value for any parse error
        return val

# plt.rc('font', size=20)          # default text size
# plt.rc('axes', titlesize=20)     # title font size
# plt.rc('axes', labelsize=20)     # x and y label font size
# plt.rc('xtick', labelsize=20)    # x-axis tick labels
# plt.rc('ytick', labelsize=20)    # y-axis tick labels

def process_simulation_results(results):
    """ Input results dictionary to process as a pandas DataFrame"""
    def process_array(arr):
        if isinstance(arr, np.ndarray):
            if arr.dtype == object:
                # Handle arrays of arrays
                return [process_array(x) if isinstance(x, np.ndarray)
                        else x.tolist() if isinstance(x, np.ndarray)
                        else x for x in arr]
            else:
                # Handle numeric arrays
                return np.round(arr, 2).tolist()
        elif isinstance(arr, list):
            return [process_array(x) if isinstance(x, np.ndarray)
                    else x for x in arr]
        return arr

    # Initialize an empty list to store dataframes
    dataframes = []

    # Loop through trials and timesteps
    for trial, timesteps in results.items():
        for timestep, data in timesteps.items():
            # === AUDIT PRINT ONLY, PRESERVES ORIGINAL FUNCTIONALITY ===
            # Print the value of card_action directly from the results Dict
            #print(f"(Trial {trial}, Timestep {timestep}) card_action in raw results:", data.get("card_action", "<not set>"))

            # Create a copy of the data to avoid modifying the original
            processed_data = {
                'trial': trial,
                'timestep': timestep
            }

            # Process each field in the timestep data
            for key, value in data.items():
                if isinstance(value, (np.ndarray, list)):
                    processed_data[key] = process_array(value)
                else:
                    processed_data[key] = value

            # Append to list of dataframes
            dataframes.append(pd.DataFrame([processed_data]))

    # Concatenate all dataframes
    final_df = pd.concat(dataframes, ignore_index=True)
    return final_df

from google.colab import drive
drive.mount('/content/drive')

# import pandas as pd
# results_df_reloaded = pd.read_csv(f"/content/drive/MyDrive/PTSD_ActInf_GridSearch/grid_search_results_20250927225724745036.csv", low_memory=False, keep_default_na=False)

# for col in results_df_reloaded.columns:
#         #print(f"Convert {col} to list of lists...")
#         try:
#           results_df_reloaded[col] = results_df_reloaded[col].apply(safe_literal_eval)
#           print(f"Convert {col} to list of lists: results_df_reloaded.loc[0,'{col}'][0]) {results_df_reloaded.loc[0,col][0]}")
#         except:
#           continue

# print(results_df_reloaded.shape)
# print(5040 / 80)
# print(results_df_reloaded.loc[500,'A2'])
# print(results_df_reloaded['grid_idx'].max())
# print(results_df_reloaded[results_df_reloaded['grid_idx'] == 4].shape)

# results_df = pd.read_csv(f"/content/drive/MyDrive/PTSD_ActInf_GridSearch/grid_search_results_20250915040928468311.csv", low_memory=False, keep_default_na=False)

import itertools
import pandas as pd
import numpy as np
import datetime

# ---------- USER: MODIFY PARAM GRID HERE ---------

param_grid = {
  'pA_advice_trust' :            [0.9],     # A[0]: Probability that advice is correct if trustworthy (only in advice stage); P(obs_advice=color|trust,correct=blue,...,)
  'pA_feedback_belief_choice' :  [0.9],        # A[1]: Probability that feedback is correct if agent chooses the card they believe is correct (only in decision stage; and expects null if chose null); P(feedback=correct|...,correct=blue,...,choice=blue,...)
 'pA_feedback_withdraw_trust' : [0.5],            # A[1]: Probability that withdrawing (receiving 'withdraw' as the feedback observation rather than 'correct', etc.) evidences advisor trustworthiness belief (0.5 signaling that as no feedback to true correct card, agent does not know if advisor is trustworthy)
  'pA_arousal_affect' :          [0.667],  # A[2]: Probability that high/low arousal aligns with negative/positive (angry/calm) affect; P(high|..,angry,..,..,..)
  # No parameter for A[3] (proprioceptive outcome aligns with choice they made)
  'pB_trust' :                   [0.9],    # B[0]: Probability that the advisor will stay (un)trustworthy if the agent (dis)trusts them
  'pB_correctcard' :             [0.9],  # B[1]: Probability that the correct card color will remain the same for the given trial (fixity of agent's beliefs about correct card), e.g., P(blue at t+1|blue at t); uncontrollable
  'pB_affect_trust' :            [0.5,0.6667],   # B[2]: Probability that trust actions lead to positive (calm) affect, and vice-versa; P(calm at t+1|<any affect>,trust)
  'pB_choice' :                  [0.95],     # B[3]: Probability that the choice belief will be conditioned on the action, e.g., P(blue @ t+1|believe blue,choose blue)
  'pB_stage' :                   [1.0],       # B[4]: Probability that stages proceed deterministically (null->advice->decision); uncontrollable by agent
  'pC_correct' :                 [0.5,1.0],             # C[1]: Agent's preference for 'correct' feedback (where 'incorrect'= -cc-3 and 'null' hard-coded to zero, followed by softmax)
  'pC_arousal_low' :             [0.3,0.5,0.7],  # C[2]: Agent's preference for low arousal (and conversely, high arousal preference = 1 - arousal_low_preference)
  'pD_trust' :                   [0.5],    # D[0]: prior belief that advisor is trustworthy
  'pD_affect' :                  [0.5],   # D[2]: prior belief that agent is in negative affect
  'pA2_trust_safety' :           [0.5,0.667], # A2[0]: association between inferences about advisor's trustworthiness with higher-order beliefs about safety
  'pA2_affect_safety' :          [0.5,0.667], # A2[2]: association between inferences about negative/angry affect with higher-order safety beliefs
  'pB2_safety_self_transition' :  [0.9],  # B2[0]: Agent's belief that self safety persists over time, P(self=safe at t+1|self=safe at t)
  'pB2_safety_world_transition' :  [0.9],  # B2[1]: Agent's belief that world safety persists over time, P(self=safe at t+1|self=safe at t)
  'pB2_safety_other_transition' :  [0.9],  # B2[2]: Agent's belief that others' safety persists over time, P(self=safe at t+1|self=safe at t)
  'pC2_trust' :                    [0.2,0.5,0.9],   # C2[0]: Agent's preference for inferring the advisor is trustworthy (as opposed to untrustworthy)
  'pC2_angry_affect' :             [0.5],  # C2[2]: Agent's preference for inferring negative/angry affect (as opposed to positive/calm affect)
  'pD2_safety' :                  [0.25,0.75] # D2[0],D2[1],D2[2]: Agent's initial beliefs about safety
}

param_grid = {
  'pA_advice_trust' :            [0.9],     # A[0]: Probability that advice is correct if trustworthy (only in advice stage); P(obs_advice=color|trust,correct=blue,...,)
  'pA_feedback_belief_choice' :  [0.9],        # A[1]: Probability that feedback is correct if agent chooses the card they believe is correct (only in decision stage; and expects null if chose null); P(feedback=correct|...,correct=blue,...,choice=blue,...)
  'pA_feedback_withdraw_trust' : [0.5],            # A[1]: Probability that withdrawing (receiving 'withdraw' as the feedback observation rather than 'correct', etc.) evidences advisor trustworthiness belief (0.5 signaling that as no feedback to true correct card, agent does not know if advisor is trustworthy)
  'pA_arousal_affect' :          [0.667],  # A[2]: Probability that high/low arousal aligns with negative/positive (angry/calm) affect; P(high|..,angry,..,..,..)
  # No parameter for A[3] (proprioceptive outcome aligns with choice they made)
  'pB_trust' :                   [0.9],    # B[0]: Probability that the advisor will stay (un)trustworthy if the agent (dis)trusts them
  'pB_correctcard' :             [0.9],  # B[1]: Probability that the correct card color will remain the same for the given trial (fixity of agent's beliefs about correct card), e.g., P(blue at t+1|blue at t); uncontrollable
  'pB_affect_trust' :            [0.6667],   # B[2]: Probability that trust actions lead to positive (calm) affect, and vice-versa; P(calm at t+1|<any affect>,trust)
  'pB_choice' :                  [0.95],     # B[3]: Probability that the choice belief will be conditioned on the action, e.g., P(blue @ t+1|believe blue,choose blue)
  'pB_stage' :                   [1.0],       # B[4]: Probability that stages proceed deterministically (null->advice->decision); uncontrollable by agent
  'pC_correct' :                 [0.5,0.9],             # C[1]: Agent's preference for 'correct' feedback (where 'incorrect'= -cc-3 and 'null' hard-coded to zero, followed by softmax)
  'pC_arousal_low' :             [0.35,0.65],  # C[2]: Agent's preference for low arousal (and conversely, high arousal preference = 1 - arousal_low_preference)
  'pD_trust' :                   [0.5],    # D[0]: prior belief that advisor is trustworthy
  'pD_affect' :                  [0.5],   # D[2]: prior belief that agent is in negative affect
  'pA2_trust_safety' :           [0.5], # A2[0]: association between inferences about advisor's trustworthiness with higher-order beliefs about safety
  'pA2_affect_safety' :          [0.5,0.9], # A2[2]: association between inferences about negative/angry affect with higher-order safety beliefs
  'pB2_safety_self_transition' :  [0.333,0.9],  # B2[0]: Agent's belief that self safety persists over time, P(self=safe at t+1|self=safe at t)
  'pB2_safety_world_transition' :  [0.9],  # B2[1]: Agent's belief that world safety persists over time, P(self=safe at t+1|self=safe at t)
  'pB2_safety_other_transition' :  [0.9],  # B2[2]: Agent's belief that others' safety persists over time, P(self=safe at t+1|self=safe at t)
  'pC2_trust' :                    [0.5,0.9],   # C2[0]: Agent's preference for inferring the advisor is trustworthy (as opposed to untrustworthy)
  'pC2_angry_affect' :             [0.5],  # C2[2]: Agent's preference for inferring negative/angry affect (as opposed to positive/calm affect)
  'pD2_safety' :                  [0.333,0.5] # D2[0],D2[1],D2[2]: Agent's initial beliefs about safety
}

# Single config run
param_grid = {
  'pA_advice_trust' :            [0.9],     # A[0]: Probability that advice is correct if trustworthy (only in advice stage); P(obs_advice=color|trust,correct=blue,...,)
  'pA_feedback_belief_choice' :  [0.9],        # A[1]: Probability that feedback is correct if agent chooses the card they believe is correct (only in decision stage; and expects null if chose null); P(feedback=correct|...,correct=blue,...,choice=blue,...)
  'pA_feedback_withdraw_trust' : [0.5],            # A[1]: Probability that withdrawing (receiving 'withdraw' as the feedback observation rather than 'correct', etc.) evidences advisor trustworthiness belief (0.5 signaling that as no feedback to true correct card, agent does not know if advisor is trustworthy)
  'pA_arousal_affect' :          [0.667],  # A[2]: Probability that high/low arousal aligns with negative/positive (angry/calm) affect; P(high|..,angry,..,..,..)
  # No parameter for A[3] (proprioceptive outcome aligns with choice they made)
  'pB_trust' :                   [0.9],    # B[0]: Probability that the advisor will stay (un)trustworthy if the agent (dis)trusts them
  'pB_correctcard' :             [0.9],  # B[1]: Probability that the correct card color will remain the same for the given trial (fixity of agent's beliefs about correct card), e.g., P(blue at t+1|blue at t); uncontrollable
  'pB_affect_trust' :            [0.6667],   # B[2]: Probability that trust actions lead to positive (calm) affect, and vice-versa; P(calm at t+1|<any affect>,trust)
  'pB_choice' :                  [0.95],     # B[3]: Probability that the choice belief will be conditioned on the action, e.g., P(blue @ t+1|believe blue,choose blue)
  'pB_stage' :                   [1.0],       # B[4]: Probability that stages proceed deterministically (null->advice->decision); uncontrollable by agent
  'pC_correct' :                 [0.5],             # C[1]: Agent's preference for 'correct' feedback (where 'incorrect'= -cc-3 and 'null' hard-coded to zero, followed by softmax)
  'pC_arousal_low' :             [0.65],  # C[2]: Agent's preference for low arousal (and conversely, high arousal preference = 1 - arousal_low_preference)
  'pD_trust' :                   [0.5],    # D[0]: prior belief that advisor is trustworthy
  'pD_affect' :                  [0.5],   # D[2]: prior belief that agent is in negative affect
  'pA2_trust_safety' :           [0.5], # A2[0]: association between inferences about advisor's trustworthiness with higher-order beliefs about safety
  'pA2_affect_safety' :          [0.5], # A2[2]: association between inferences about negative/angry affect with higher-order safety beliefs
  'pB2_safety_self_transition' :  [0.5],  # B2[0]: Agent's belief that self safety persists over time, P(self=safe at t+1|self=safe at t)
  'pB2_safety_world_transition' :  [0.9],  # B2[1]: Agent's belief that world safety persists over time, P(self=safe at t+1|self=safe at t)
  'pB2_safety_other_transition' :  [0.9],  # B2[2]: Agent's belief that others' safety persists over time, P(self=safe at t+1|self=safe at t)
  'pC2_trust' :                    [0.5],   # C2[0]: Agent's preference for inferring the advisor is trustworthy (as opposed to untrustworthy)
  'pC2_angry_affect' :             [0.5],  # C2[2]: Agent's preference for inferring negative/angry affect (as opposed to positive/calm affect)
  'pD2_safety' :                  [0.5] # D2[0],D2[1],D2[2]: Agent's initial beliefs about safety
}



param_names = sorted(param_grid)
value_lists = [param_grid[k] for k in param_names]

import os
import datetime
import re
import itertools
import pandas as pd
import numpy as np

# Set up unique time label and output paths
raw_time = str(datetime.datetime.now())
stripped_time = re.sub(r'[^A-Za-z0-9]', '', raw_time)
BASE_PATH = "/content/drive/MyDrive/PTSD_ActInf_GridSearch"
RESULTS_FOLDER = f"{BASE_PATH}/grid_search_{stripped_time}"
COMBINATIONS_CSV = f"{BASE_PATH}/parameter_grid_{stripped_time}.csv"

os.makedirs(RESULTS_FOLDER, exist_ok=True)

# ------ STEP 1: Generate & save grid to CSV ONCE ------
def generate_param_grid_csv():
    combos = itertools.product(*value_lists)
    settings_list = [dict(zip(param_names, vals)) for vals in combos]
    df = pd.DataFrame(settings_list)
    df['status'] = 'pending'
    df['metric'] = np.nan  # Placeholder for result metric, add more as needed
    df.to_csv(COMBINATIONS_CSV, index=False)
    print(f"Wrote {len(df)} grid settings to {COMBINATIONS_CSV}")

# --- Ensure the parameter CSV exists before running the grid search ---
generate_param_grid_csv()  # Only run once per time-stamped grid

# ------ STEP 2: Run grid search, save each simulation ------
def run_grid_search():
    grid_df = pd.read_csv(COMBINATIONS_CSV, keep_default_na=False)
    print(f"Loaded grid with {len(grid_df)} parameter combinations.")

    start_time = datetime.datetime.now()

    for idx, row in grid_df.iterrows():
        if row["status"] == "done":
            continue  # Already completed

        # --- Gather settings into dict ---
        param_settings = {k: row[k] for k in param_names}

        print(f"Running {idx+1}/{len(grid_df)}: {param_settings}")

        try:
            # 1. Build agent config
            A,B,C,D,E,policies,pA,pB,pD,A2,B2,C2,D2,pA2,pB2,pD2 = construct_agent_components(
                **param_settings
            )

            # 2. Initialize agent/HL-agent
            my_agent_high = Agent(A2, B2, C2, D2, pA=pA2, pB=pB2, pD=pD2,
                                  inference_algo="MMP", save_belief_hist=True)
            my_agent = Agent(A, B, C, D, E, pA=pA, pB=pB, pD=pD, gamma=1.0,
                             policies=policies, policy_len=2, inference_horizon=3, control_fac_idx=[0,2,3],
                             inference_algo="MMP", sampling_mode='full', policy_sep_prior=True,
                             save_belief_hist=True, use_BMA=False)

            n_trials1 = 20
            n_trials2 = 60
            n_timesteps = 3
            lower_learning_timesteps = [0,1,2]
            results_5 = {}

            # 3. Simulation block 1: untrustworthy
            for trial in range(n_trials1):
                print(f"Running trial {trial}...")
                results_trial = run_active_inference_loop(
                    my_agent,
                    CardAdvisor(trustworthy_process="untrustworthy",
                                correct_card=np.random.choice(['blue','green'])),
                    T=n_timesteps,
                    trial_results=True,
                    learn_A=True, learn_B=True, learn_C=True, learn_D=True,
                    learn_E=True, learn_gamma=True, my_agent_high=my_agent_high,
                    learn_A2=True, learn_B2=True, learn_D2=True,
                    monitoring=False, lower_learning_timesteps=lower_learning_timesteps,
                    arousal_modulation_from_A=True
                )
                results_5[trial] = results_trial

            # 4. Simulation block 2: trustworthy
            for trial in range(n_trials2):
                print(f"Running trial {trial + n_trials1}...")
                results_trial = run_active_inference_loop(
                    my_agent,
                    CardAdvisor(trustworthy_process="trustworthy",
                                correct_card=np.random.choice(['blue','green'])),
                    T=n_timesteps,
                    trial_results=True,
                    learn_A=True, learn_B=True, learn_C=True, learn_D=True,
                    learn_E=True, learn_gamma=True, my_agent_high=my_agent_high,
                    learn_A2=True, learn_B2=True, learn_D2=True,
                    monitoring=False, lower_learning_timesteps=lower_learning_timesteps,
                    arousal_modulation_from_A=True
                )
                results_5[trial + n_trials1] = results_trial

            # 5. Aggregate results for this parameter set
            final_df_5 = process_simulation_results(results_5)
            final_df_5["grid_idx"] = idx
            for k, v in param_settings.items():
                final_df_5[k] = v
            metric_val = np.nan  # Put your computation here!
            grid_df.at[idx, 'metric'] = metric_val
            grid_df.at[idx, 'status'] = "done"

            # ---- Save results for this grid_idx to its own CSV ----
            save_path = os.path.join(RESULTS_FOLDER, f"grid_idx_{idx}.csv")
            final_df_5.to_csv(save_path, index=False)
            print(f"Saved grid idx {idx} results to {save_path}")

            # ---- Memory management: wipe simulation traces ----
            del results_5, final_df_5, my_agent, my_agent_high
            import gc
            gc.collect()

        except Exception as e:
            print(f"Exception on grid index {idx}: {e}")
            grid_df.at[idx, 'status'] = "error"
            grid_df.at[idx, 'metric'] = np.nan
            continue

        # Save grid progress after each run (robust checkpointing)
        grid_df.to_csv(COMBINATIONS_CSV, index=False)

    end_time = datetime.datetime.now()
    elapsed_time = end_time - start_time
    print(f"Elapsed time: {elapsed_time}")

    print(f"All results indexed and saved in {RESULTS_FOLDER}")
    return grid_df  # Returns only grid metadata now

# --- Run the grid search ---
results_df = run_grid_search()

"""## Grid Analysis"""

from google.colab import drive
drive.mount('/content/drive')

# print(list(param_grid.keys()))
# print('')

param_cols = ['pA_advice_trust', 'pA_feedback_belief_choice', 'pA_feedback_withdraw_trust', 'pA_arousal_affect', 'pB_trust', 'pB_correctcard', 'pB_affect_trust', 'pB_choice', 'pB_stage', 'pC_correct',
              'pC_arousal_low', 'pD_trust', 'pD_affect', 'pA2_trust_safety', 'pA2_affect_safety', 'pB2_safety_self_transition', 'pB2_safety_world_transition', 'pB2_safety_other_transition', 'pC2_trust',
              'pC2_angry_affect', 'pD2_safety']

# # with pd.option_context('display.max_rows',None):
# #   display(results_df.head(6).T)
# print(results_df.loc[2,'qs_bma'])
# print(results_df.loc[200,'qs_high_bma'])
# print(results_df.loc[200,'high_arousal_prob'])

# # withdraw prob total - policy inference
# print(results_df.loc[0,'q_pi'])
# for i in [0,20,40,200]:
#   print(results_df.loc[i,'E'][4] + results_df.loc[i,'E'][5])
# print(results_df[['timestep','card_action']].value_counts())

# # withdraw prob total - habits

# with pd.option_context('display.max_rows',None):
#   display(results_df.head(6).T)

# print([col for col in results_df.columns if 'prob' in col])

import pandas as pd

BASE_PATH = "/content/drive/MyDrive/PTSD_ActInf_GridSearch"
stripped_time = "20251005043852938836"
results_df = pd.read_csv(f"{BASE_PATH}/grid_search_{stripped_time}/grid_idx_0.csv", low_memory=False, keep_default_na=False)
print(results_df.shape)

import ast

def safe_literal_eval(val):
    try:
        # Only evaluate strings that look like lists
        if isinstance(val, str) and val.startswith('[') and val.endswith(']'):
            return ast.literal_eval(val)
        else:
            # Return original value if not a valid list string
            return val
    except Exception:
        # Return original value for any parse error
        return val

for col in results_df.columns:
        #print(f"Convert {col} to list of lists...")
        try:
          results_df[col] = results_df[col].apply(safe_literal_eval)
          #print(f"Convert {col} to list of lists: results_df_reloaded.loc[0,'{col}'][0]) {results_df_reloaded.loc[0,col][0]}")
        except:
          continue

def process_simulation_aggregates(results_df):
  # Delusion scores / Evaluation criteria
  import numpy as np

  results_df['was_correct'] = np.where(results_df['card_action'] == results_df['true_color'], 1, np.nan)
  results_df['was_correct'] = np.where((results_df['card_action'] != results_df['true_color']) & (results_df['timestep'] == 1) & (results_df['card_action'] != 'withdraw'), 0, results_df['was_correct'])
  results_df['was_incorrect'] = np.where( (results_df['card_action'] != results_df['true_color']) & (results_df['timestep'] == 1) & (results_df['card_action'] != 'withdraw'), 1, np.nan)
  results_df['was_incorrect'] = np.where((results_df['was_correct'] == 1) & (results_df['timestep'] == 1), 0, results_df['was_incorrect'])

  # False trustworthiness criteria
  results_df['trust_belief'] = results_df['qs_bma'].apply(lambda x: x[0][0])    # Extract trustworthiness beliefs (first element of first belief list/array in 'qs_bma')
  # False trust inference: tracked only at timestep 2 (i.e. following agent's receiving evidence of true trustworthiness from witnessing if advice color matched true color)
  results_df['inferred_trustworthiness_state'] = np.where(results_df['trust_belief'] >= 0.5, 'trustworthy', 'untrustworthy')   # categorical indicator
  results_df['inferred_trustworthiness_certainty'] = np.where(
      results_df['trust_belief'] > 0.5,
      results_df['trust_belief'] - 0.5,
      0.5 - results_df['trust_belief'])
  results_df['inferred_trustworthiness_certainty_scaled'] = results_df['inferred_trustworthiness_certainty'].apply(lambda x: x * 2 if pd.notnull(x) else np.nan)
  results_df['false_trust_inference'] = np.where(
      results_df['timestep'] == 2,
      (((results_df['true_trustworthiness'] == 'trustworthy') & (results_df['trust_belief'] < 0.5)) | ((results_df['true_trustworthiness'] == 'untrustworthy') & (results_df['trust_belief'] > 0.5))).astype(float),np.nan)

  results_df['false_trust_certainty'] = np.where(results_df['false_trust_inference'] == 1, results_df['inferred_trustworthiness_certainty'], np.nan)
  results_df['false_trust_certainty_scaled'] = np.where(results_df['false_trust_inference'] == 1, results_df['inferred_trustworthiness_certainty_scaled'], np.nan)
  # Incorrigibility: of the false trust inferences, which were followed by another false inference?
  results_df['false_trust_inference_subsequent'] = np.nan
  indices = results_df.index[results_df['timestep'] == 2].tolist()  # Get indices where timestep == 2

  for i in range(len(indices) - 1):
      current_idx = indices[i]
      next_idx = indices[i + 1]
      if results_df.loc[current_idx, 'false_trust_inference'] == 1.0:
          if results_df.loc[next_idx, 'false_trust_inference'] == 1.0:
              results_df.at[current_idx, 'false_trust_inference_subsequent'] = 1.0
          elif results_df.loc[next_idx, 'false_trust_inference'] == 0.0:
              results_df.at[current_idx, 'false_trust_inference_subsequent'] = 0.0

  ## For false untrustworthy inferences, just filter to where 'false_trust_inference' equals 1.0 and 'inferred_trustworthiness_state'=='untrustworthy' then do same aggregations
  results_df['false_untrustworthiness_inference'] = np.where( (results_df['timestep'] == 2) & (results_df['inferred_trustworthiness_state'] == 'untrustworthy')  & (results_df['false_trust_inference'] == 1.0) , 1.0, np.nan)
  results_df['false_untrustworthiness_inference'] = np.where( (results_df['timestep'] == 2) & (results_df['false_trust_inference'] == 0.0) , 0.0, results_df['false_untrustworthiness_inference'])
  results_df['false_untrustworthiness_certainty'] = np.where(results_df['false_untrustworthiness_inference'] == 1.0, results_df['false_trust_certainty'],np.nan)
  results_df['false_untrustworthiness_certainty_scaled'] = np.where(results_df['false_untrustworthiness_inference'] == 1.0, results_df['false_trust_certainty_scaled'],np.nan)
  results_df['false_untrustworthiness_inference_subsequent'] = np.where( (results_df['false_untrustworthiness_inference'] == 1) & (results_df['false_trust_inference_subsequent'] == 1.0), 1.0, np.nan)
  results_df['false_untrustworthiness_inference_subsequent'] = np.where( (results_df['false_untrustworthiness_inference'] == 1) & (results_df['false_trust_inference_subsequent'] == 0.0), 0.0, results_df['false_untrustworthiness_inference_subsequent'])

  # Negative affect criteria
  results_df['affect_belief'] = results_df['qs_bma'].apply(lambda x: x[2][0])    # Extract negative affect belief (first element of third, i.e. 2 with zero-indexing, belief list/array in 'qs_bma')
  # False trust inference: tracked only at timestep 2 (i.e. following agent's receiving evidence of true trustworthiness from witnessing if advice color matched true color)
  results_df['inferred_affect_state'] = np.where(results_df['affect_belief'] >= 0.5, 'negative', 'positive')   # categorical indicator
  results_df['inferred_affect_certainty'] = np.where(
      results_df['affect_belief'] > 0.5,
      results_df['affect_belief'] - 0.5,
      0.5 - results_df['affect_belief'])
  results_df['inferred_affect_certainty_scaled'] = results_df['inferred_affect_certainty'].apply(lambda x: x * 2 if pd.notnull(x) else np.nan)

  results_df['negative_affect_inference'] = np.where( (results_df['inferred_affect_state'] == 'negative'), 1.0, 0.0)
  results_df['inferred_negative_affect_certainty'] = np.where(results_df['negative_affect_inference'] == 1.0, results_df['inferred_affect_certainty'],np.nan)
  results_df['inferred_negative_affect_certainty_scaled'] = np.where(results_df['negative_affect_inference'] == 1.0, results_df['inferred_affect_certainty_scaled'],np.nan)

  results_df['inferred_negative_affect_subsequent'] = np.nan
  indices = results_df.index[results_df['timestep'] == 2].tolist()  # Get indices where timestep == 2

  for i in range(len(indices) - 1):
      current_idx = indices[i]
      next_idx = indices[i + 1]
      if results_df.loc[current_idx, 'inferred_affect_state'] == 'negative':
          if results_df.loc[next_idx, 'inferred_affect_state'] == 'negative':
              results_df.at[current_idx, 'inferred_negative_affect_subsequent'] = 1.0
          elif results_df.loc[next_idx, 'inferred_affect_state'] != 'negative':
              results_df.at[current_idx, 'false_trust_inference_subsequent'] = 0.0

  # Arousal criteria
  results_df['arousal_obs'] = results_df['obs_label'].apply(lambda x: x[2]) # Extract the third element of the obs_label list
  #results_df['high_arousal_prob'] = results_df['high_arousal_prob']

  results_df['high_arousal_certainty'] = np.where(
      results_df['high_arousal_prob'] >= 0.5,
      results_df['high_arousal_prob'] - 0.5,
      np.nan)
  results_df['high_arousal_certainty_scaled'] = results_df['high_arousal_certainty'].apply(lambda x: x * 2 if pd.notnull(x) else np.nan)

  results_df['high_arousal'] = results_df['arousal_obs'].apply(lambda val: 1.0 if val == 'high' else 0.0)


  results_df['high_arousal_subsequent_timestep'] = np.nan
  indices = results_df.index.tolist()
  for i in range(len(indices) - 1):
      current_idx = indices[i]
      next_idx = indices[i + 1]
      if results_df.loc[current_idx, 'high_arousal'] == 1.0:
          if results_df.loc[next_idx, 'high_arousal'] == 1.0:
              results_df.at[current_idx, 'high_arousal_subsequent_timestep'] = 1.0
          elif results_df.loc[next_idx, 'high_arousal'] == 0.0:
              results_df.at[current_idx, 'high_arousal_subsequent_timestep'] = 0.0

  results_df['high_arousal_subsequent_trial'] = np.nan
  indices = results_df.index[results_df['timestep'] == 2].tolist()
  for i in range(len(indices) - 1):
      current_idx = indices[i]
      next_idx = indices[i + 1]
      if results_df.loc[current_idx, 'high_arousal'] == 1.0:
          if results_df.loc[next_idx, 'high_arousal'] == 1.0:
              results_df.at[current_idx, 'high_arousal_subsequent_trial'] = 1.0
          elif results_df.loc[next_idx, 'high_arousal'] == 0.0:
              results_df.at[current_idx, 'high_arousal_subsequent_trial'] = 0.0


  # False safety criteria ----------------------------------------------------------------------------------------------------------------------
  results_df['safety_belief'] = results_df['qs_high_bma'].apply(lambda x: x[0][0])   # Extract safety beliefs Q(safety)
  results_df['safety_belief'] = np.where( results_df['timestep'] == 2, results_df['safety_belief'], np.nan )

  results_df['inferred_safety_state'] = np.where( (results_df['safety_belief'] >= 0.5) & (results_df['timestep'] ==2), 'safe', None)   # categorical indicator
  results_df['inferred_safety_state'] = np.where( (results_df['safety_belief'] < 0.5) & (results_df['timestep'] ==2), 'danger', results_df['inferred_safety_state'])   # categorical indicator
  results_df['inferred_safety_certainty'] = np.where(
      results_df['safety_belief'] >= 0.5,
      results_df['safety_belief'] - 0.5,
      0.5 - results_df['safety_belief']
  )
  results_df['inferred_safety_certainty'] = np.where(results_df['timestep'] != 2, np.nan, results_df['inferred_safety_certainty'])
  results_df['inferred_safety_certainty_scaled'] = results_df['inferred_safety_certainty'].apply(lambda x: x * 2 if pd.notnull(x) else np.nan)
  results_df['false_danger_inference'] = np.where( (results_df['timestep'] == 2) & (results_df['inferred_safety_state'] == 'danger'), 1.0, np.nan)
  results_df['false_danger_inference'] = np.where( (results_df['timestep'] == 2) & (results_df['inferred_safety_state'] == 'safe'), 0.0, results_df['false_danger_inference'])

  results_df['false_danger_certainty'] = np.where(results_df['false_danger_inference'] == 1.0, results_df['inferred_safety_certainty'],np.nan)
  results_df['false_danger_certainty_scaled'] = np.where(results_df['false_danger_inference'] == 1.0, results_df['inferred_safety_certainty_scaled'],np.nan)

  results_df['false_danger_inference_subsequent'] = np.nan
  indices = results_df.index[results_df['timestep'] == 2].tolist()
  for i in range(len(indices) - 1):
      current_idx = indices[i]
      next_idx = indices[i + 1]
      if results_df.loc[current_idx, 'false_danger_inference'] == 1.0:
          if results_df.loc[next_idx, 'false_danger_inference'] == 1.0:
              results_df.at[current_idx, 'false_danger_inference_subsequent'] = 1.0
          elif results_df.loc[next_idx, 'false_danger_inference'] == 0.0:
              results_df.at[current_idx, 'false_danger_inference_subsequent'] = 0.0
  # # False danger inference: task is necessarily 'safe', thus bias to 'danger' (safety < 0.5) is considered a false safety belief
  #results_df['false_safety_inference'] = np.where( (results_df['timestep'] == 2,(results_df['safety_belief'] < 0.5).astype(float),np.nan)

  results_df['withdraw'] = np.where( (results_df['card_action'] == 'withdraw') & (results_df['timestep'] == 1), 1, np.nan)
  results_df['withdraw'] = np.where( (results_df['card_action'] != 'withdraw') & (results_df['timestep'] == 1), 0, results_df['withdraw'])

  results_df['withdraw_subsequent'] = np.nan
  indices = results_df.index[results_df['timestep'] == 1].tolist()
  for i in range(len(indices) - 1):
      current_idx = indices[i]
      next_idx = indices[i + 1]
      if results_df.loc[current_idx, 'withdraw'] == 1.0:
          if results_df.loc[next_idx, 'withdraw'] == 1.0:
              results_df.at[current_idx, 'withdraw_subsequent'] = 1.0
          elif results_df.loc[next_idx, 'withdraw'] == 0.0:
              results_df.at[current_idx, 'withdraw_subsequent'] = 0.0

  results_df['delusion_score'] = results_df['false_trust_inference'] + results_df['false_trust_certainty_scaled'] + results_df['false_trust_inference_subsequent']
  results_df['interpretation_bias_score'] = results_df['false_untrustworthiness_inference'] + results_df['false_untrustworthiness_certainty_scaled'] + results_df['false_trust_inference_subsequent']
  results_df['negative_affect_score'] = results_df['negative_affect_inference'] + results_df['inferred_negative_affect_certainty_scaled'] + results_df['inferred_negative_affect_subsequent']
  results_df['high_arousal_score'] = results_df['high_arousal'] + results_df['high_arousal_certainty_scaled'] + results_df['high_arousal_subsequent_timestep']
  results_df['danger_score'] = results_df['false_danger_inference'] + results_df['false_danger_certainty_scaled'] + results_df['false_danger_inference_subsequent']

  #results_df['hypervigilance']

  analysis_cols = [#'timestep','true_trustworthiness','qs_bma',
                  #'grid_idx',
                  'was_correct',     # received 'correct' feedback (did not withdraw, chose blue/green and matched correct color of blue/green)
                  'was_incorrect',    # received 'incorrect' feedback (did not withdraw, chose blue/green but true color was green/blue)
                  'trust_belief',    # q(trustworthiness) on range [0,1]
                  #'inferred_trustworthiness_state',   # categorical q(trustworthiness)
                  'inferred_trustworthiness_certainty',    # q(trustworthiness) difference from max uncertainty (difference from 0.5)
                  'inferred_trustworthiness_certainty_scaled',    # q(trustworthiness) difference from max uncertainty, scaled to [0.0,1.0]
                  'false_trust_inference',  # false trust inference trial (inferred_trustworthiness_state != true_trustworthiness), i.e. false positive or false negative
                  'false_trust_certainty',   # inferred_trustworthiness_certainty only when false_trust_inference (other values null)
                  'false_trust_certainty_scaled',  # false_trust_certainty scaled to [0,1.0]
                  'false_trust_inference_subsequent',  # false trust inference is followed by another false trust inference trial
                  'false_untrustworthiness_inference',  # false 'untrustworthy' inference when true_trustworthiness is 'trustworthy' (i.e. subset of false trust inference, only where true_trustworthiness == 'trustworthy')
                  'false_untrustworthiness_certainty',  # certainty of false untrustworthiness inference, only when false_untrustworthiness_inference (other values null)
                  'false_untrustworthiness_certainty_scaled',  # false_untrustworthiness_certainty scaled to [0.0,1.0]
                  'false_untrustworthiness_inference_subsequent',  # false untrustworthiness inference is followed by another false untrustworthiness inference

                  'affect_belief',    # q(anger) on range [0,1]
                  #'inferred_affect_state',  # categorical q(anger)
                  'inferred_affect_certainty',   # q(anger) difference from max uncertainty

                  'negative_affect_inference',
                  'inferred_negative_affect_certainty',
                  'inferred_negative_affect_certainty_scaled',
                  'inferred_negative_affect_subsequent',

                  #'arousal_obs',  # categorical arousal observation
                  'high_arousal',
                  'high_arousal_prob',
                  'high_arousal_certainty',
                  'high_arousal_certainty_scaled',
                  'high_arousal_subsequent_timestep',
                  'high_arousal_subsequent_trial',

                  #'qs_high_bma',
                  'safety_belief',  # q(safety) belief that agent is safe
                  #'inferred_safety_state', # categorical q(safety)
                  'inferred_safety_certainty',
                  'inferred_safety_certainty_scaled',


                  'false_danger_inference',
                  'false_danger_certainty',
                  'false_danger_certainty_scaled',
                  'false_danger_inference_subsequent',
                  'withdraw',    # indicator that agent chose to withdraw
                  'withdraw_subsequent', # indicator that agent withdraw in current trial and next
                  'delusion_score', 'interpretation_bias_score', 'negative_affect_score', 'high_arousal_score', 'danger_score'
                  ]

  param_cols = [#'grid_idx',
                'pA_advice_trust', 'pA_feedback_belief_choice', 'pA_feedback_withdraw_trust', 'pA_arousal_affect', 'pB_trust', 'pB_correctcard', 'pB_affect_trust', 'pB_choice', 'pB_stage',
                'pC_correct', 'pC_arousal_low', 'pD_trust', 'pD_affect', 'pA2_trust_safety', 'pA2_affect_safety', 'pB2_safety_self_transition', 'pB2_safety_world_transition', 'pB2_safety_other_transition',
                'pC2_trust', 'pC2_angry_affect', 'pD2_safety']


  #print(results_df[analysis_cols + param_cols].info(verbose=True, show_counts=True))
  # analysis_aggregates = results_df[analysis_cols + param_cols].agg(['mean', 'std'])
  # display(analysis_aggregates)

  def make_aggregate_row(results_df, analysis_cols, param_cols):
      # Compute mean and std for analysis columns
      agg_means = results_df[analysis_cols].mean()
      agg_stds  = results_df[analysis_cols].std()
      # Suffix column names
      agg_means.index = [f"{c}_mean" for c in agg_means.index]
      agg_stds.index  = [f"{c}_std" for c in agg_stds.index]
      aggregates = pd.concat([agg_means, agg_stds])    # Combine means and stds into one Series
      params = results_df.loc[0, param_cols]      # Get param_cols from first row
      full_row = pd.concat([aggregates, params]).to_frame().T       # Concatenate all into single row DataFrame (as one row)
      col_order = list(aggregates.index) + list(param_cols)     # Ensure param_cols come after statistics if you prefer
      return full_row[col_order]

  #print(type(analysis_aggregates))
  analysis_aggregates_i = make_aggregate_row(results_df, analysis_cols, param_cols)
  return analysis_aggregates_i, results_df

analysis_aggregate_i, results_df_processed = process_simulation_aggregates(results_df)
display(analysis_aggregate_i.T)

import os
import pandas as pd
import re

def compute_all_aggregates_iterative(stripped_time, output_path):
    base_path = f"/content/drive/MyDrive/PTSD_ActInf_GridSearch/grid_search_{stripped_time}"
    aggregate_csv_path = output_path
    files = os.listdir(base_path)
    csv_files = [f for f in files if 'grid_idx' in f and f.endswith('.csv')]

    # 1. Load previously processed grid_idx values if output CSV exists
    if os.path.exists(aggregate_csv_path):
        prev_df = pd.read_csv(aggregate_csv_path)
        processed_grid_idx = set(prev_df['grid_idx'].astype(int))
    else:
        processed_grid_idx = set()

    # 2. Iteratively process only new files
    for csv_file in csv_files:
        match = re.search(r'grid_idx_(\d+)\.csv', csv_file)
        if match:
            grid_idx_value = int(match.group(1))
            if grid_idx_value in processed_grid_idx:
                continue  # Skip already processed
            print(f"Processing {csv_file}")

            file_path = os.path.join(base_path, csv_file)
            results_df = pd.read_csv(file_path, low_memory=False, keep_default_na=False)
            for col in results_df.columns:
                try:
                    results_df[col] = results_df[col].apply(safe_literal_eval)
                except Exception:
                    continue

            analysis_aggregate_i = process_simulation_aggregates(results_df)
            if isinstance(analysis_aggregate_i, tuple):
                analysis_aggregate_i = analysis_aggregate_i[0]
            if isinstance(analysis_aggregate_i, pd.Series):
                analysis_aggregate_i = analysis_aggregate_i.to_frame().T
            analysis_aggregate_i['grid_idx'] = grid_idx_value

            # Convert columns except 'grid_idx' to float
            cols_to_convert = analysis_aggregate_i.columns.difference(['grid_idx'])
            analysis_aggregate_i[cols_to_convert] = analysis_aggregate_i[cols_to_convert].apply(pd.to_numeric, errors='coerce')

            # 3. Append to file (header only if new file)
            analysis_aggregate_i.to_csv(
                aggregate_csv_path,
                mode='a',
                header=not os.path.exists(aggregate_csv_path),
                index=False
            )
            # Optionally, update processed_grid_idx in memory for current run
            processed_grid_idx.add(grid_idx_value)

# Usage:
aggregate_path = f"{BASE_PATH}/grid_search_{stripped_time}/analyses_aggregates.csv"
compute_all_aggregates_iterative(stripped_time, aggregate_path)

# To load results afterward:
final_aggregates_df = pd.read_csv(aggregate_path)
display(final_aggregates_df.T)

"""# final_aggregates_df"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np

BASE_PATH = "/content/drive/MyDrive/PTSD_ActInf_GridSearch"
stripped_time = "20251005043852938836"

aggregate_path = f"{BASE_PATH}/grid_search_{stripped_time}/analyses_aggregates.csv"
final_aggregates_df = pd.read_csv(aggregate_path)
score_cols = [col for col in final_aggregates_df.columns if '_score' in col]
print(final_aggregates_df[score_cols + ['withdraw_mean']].info())
final_aggregates_df[score_cols] = final_aggregates_df[score_cols].fillna(0.0)
final_aggregates_df['hypervigilance_score'] = final_aggregates_df['interpretation_bias_score_mean'] + final_aggregates_df['negative_affect_score_mean'] + final_aggregates_df['danger_score_mean'] + final_aggregates_df['high_arousal_score_mean']
print(final_aggregates_df[score_cols + ['withdraw_mean']].info())

# Rows with the lowest hypervigilance_score
lowest_rows = final_aggregates_df[final_aggregates_df['hypervigilance_score'] == final_aggregates_df['hypervigilance_score'].min()]

# Rows with the highest hypervigilance_score
highest_rows = final_aggregates_df[final_aggregates_df['hypervigilance_score'] == final_aggregates_df['hypervigilance_score'].max()]

# Row closest to the average hypervigilance_score
average_value = final_aggregates_df['hypervigilance_score'].mean()
closest_idx = (final_aggregates_df['hypervigilance_score'] - average_value).abs().idxmin()
closest_row = final_aggregates_df.loc[[closest_idx]]
select_df = pd.concat([lowest_rows,closest_row,highest_rows], axis=0)     # 323	391	382
with pd.option_context('display.max_rows',None):
  display(select_df[[col for col in select_df.columns if '_std' not in col]].T)
  ### Differences:
  # Lowest: pC_arousal_low = 0.7, pD2_safety = 0.75, never withdrew, always 'safe', mean 'safety' = 0.5, interpretation_bias = 0
  # Middle: pC_arousal_low = 0.5, pD2_safety = 0.75, always withdrew, always 'safe', mean 'safety' = 0.663250, interpretation_bias =
  # Highest: pC_arousal_low = 0.7, pD2_safety = 0.25, always withdrew, always 'danger', mean 'safety' = 	0.306750
select_df_focused = select_df.copy()[['grid_idx', 'hypervigilance_score','was_correct_mean','delusion_score_mean','interpretation_bias_score_mean','danger_score_mean','negative_affect_score_mean','high_arousal_score_mean','pC_arousal_low','pD2_safety','withdraw_mean','safety_belief_mean','withdraw_mean']]
select_df_focused.T.to_csv('select_df.csv',index=True)

print(final_aggregates_df.info())

# with pd.option_context('display.max_rows',None):
#   display(final_aggregates_df.head(6).T)

import pandas as pd
import numpy as np



# Select score columns and compute summary
score_cols = [c for c in final_aggregates_df.columns if '_score' in c]  # [execute_python:1]
summary = final_aggregates_df[score_cols].agg(['mean', 'std', 'min', 'max']).T  # [execute_python:1]
summary = summary.rename_axis('column').reset_index()  # [execute_python:1]
print(final_aggregates_df[score_cols].info())

print(summary)  # [execute_python:1]


#print(final_aggregates_df['withdraw_mean'].value_counts())

with pd.option_context('display.max_rows',None,'display.max_columns',None):
  display(final_aggregates_df[[col for col in final_aggregates_df.columns if '_score' in col and '_std' not in col]].T)

import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression

# Prepare data
X = final_aggregates_df[param_cols + score_cols]
y = final_aggregates_df['withdraw_mean']

# Logistic regression with balanced class weights for imbalanced binary target
logreg = LogisticRegression(class_weight='balanced', solver='liblinear')
logreg.fit(X, y)

# Extract coefficients
coefs = logreg.coef_[0]

# Compute odds ratios for:
# - a full-range increase (from 0.0 to 1.0)
# - a 0.1 increment (10% step along feature range)
odds_ratio_0_to_1 = np.exp(coefs)
odds_ratio_per_0_1_increase = np.exp(coefs * 0.1)

# Summarize results in an interpretable DataFrame
results = pd.DataFrame({
    'param_col': param_cols + score_cols,
    'coefficient': coefs,
    'odds_ratio_0_to_1': odds_ratio_0_to_1,
    'odds_ratio_per_0.1_increase': odds_ratio_per_0_1_increase
})

#print(results)
display(results)

### FIGURE FOR SLIDE SLIDES IWAI

import numpy as np
import pandas as pd
from scipy.stats import pearsonr
from itertools import combinations

metrics = [col for col in final_aggregates_df.columns if 'grid_idx' not in col and 'certainty_mean' not in col and '_std' not in col]  ##final_aggregates_df.columns]
results = []

for metric1, metric2 in combinations(metrics, 2):
    x = final_aggregates_df[metric1]
    y = final_aggregates_df[metric2]
    # Check for constant columns
    if np.all(x == x.iloc[0]) or np.all(y == y.iloc[0]):
        coeff = np.nan
        pval = np.nan
    else:
        coeff, pval = pearsonr(x, y)
    results.append({
        'metric1': metric1,
        'metric2': metric2,
        'coefficient': coeff,
        'pvalue': pval
    })

correlation_table = pd.DataFrame(results)
#correlation_table.to_csv('pairwise_correlations.csv', index=False)

withdraw_corr_filter = correlation_table[correlation_table['metric1'].str.contains('withdraw', case=False, na=False) | correlation_table['metric2'].str.contains('withdraw', case=False, na=False)]
scores_corr_filter = correlation_table[correlation_table['metric1'].str.contains('_score', case=False, na=False) | correlation_table['metric2'].str.contains('_score', case=False, na=False)]
hypervigilance_corr_filter = correlation_table[correlation_table['metric1'].str.contains('hypervigilance', case=False, na=False) | correlation_table['metric2'].str.contains('hypervigilance', case=False, na=False)]
with pd.option_context('display.max_rows',None):
  #display(correlation_table[(correlation_table['coefficient'].notnull()) & (correlation_table['pvalue'] <= 0.05)].reset_index(drop=True).sort_values('metric1').sort_values('pvalue'))
  display(withdraw_corr_filter[(withdraw_corr_filter['coefficient'].notnull()) & (withdraw_corr_filter['pvalue'] <= 0.05)].reset_index(drop=True).sort_values('metric1').sort_values('pvalue').reset_index(drop=True))
  withdraw_corr_filter[(withdraw_corr_filter['coefficient'].notnull()) & (withdraw_corr_filter['pvalue'] <= 0.05)].reset_index(drop=True).sort_values('metric1').sort_values('pvalue').reset_index(drop=True).to_csv(
      'withdraw_corr_filter.csv', index=False)

  #display(scores_corr_filter[(scores_corr_filter['coefficient'].notnull()) & (scores_corr_filter['pvalue'] <= 0.05)].reset_index(drop=True).sort_values('metric1').sort_values('pvalue').reset_index(drop=True))
  display(hypervigilance_corr_filter[(scores_corr_filter['coefficient'].notnull()) & (hypervigilance_corr_filter['pvalue'] <= 0.05)].reset_index(drop=True).sort_values('metric1').sort_values('pvalue').reset_index(drop=True))
  hypervigilance_corr_filter[(scores_corr_filter['coefficient'].notnull()) & (hypervigilance_corr_filter['pvalue'] <= 0.05)].reset_index(drop=True).sort_values('metric1').sort_values('pvalue').reset_index(drop=True).to_csv(
      'hypervigilance_corr_filter.csv', index=False)

print(final_aggregates_df['grid_idx'].max())



## IWAI SLIDE SLIDES

def scatter_with_options(
    df,
    x_col,
    y_col,
    color_col=None,
    fit_line=False,
    show_corr=False,
    ax=None,
    x_label=None,
    y_label=None,
    legend_title=None,
    x_label_size=None,
    y_label_size=None,
    legend_title_size=None,
    **scatter_kwargs
):
    import matplotlib.pyplot as plt
    from scipy.stats import pearsonr
    import numpy as np

    if ax is None:
        fig, ax = plt.subplots(figsize=(6, 4))

    # Color mapping
    if color_col is not None:
        norm = plt.Normalize(df[color_col].min(), df[color_col].max())
        cmap = plt.cm.bwr
        colors = cmap(norm(df[color_col]))
        sc = ax.scatter(df[x_col], df[y_col], c=colors, **scatter_kwargs)
        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)
        sm.set_array([])
        cbar = plt.colorbar(sm, ax=ax)
        cbar.set_label(
            legend_title if legend_title is not None else color_col,
            fontsize=legend_title_size
        )
    else:
        ax.scatter(df[x_col], df[y_col], color="royalblue", **scatter_kwargs)

    # Line of best fit using numpy.polyfit and matplotlib
    if fit_line:
        x = df[x_col].values
        y = df[y_col].values
        mask = ~np.isnan(x) & ~np.isnan(y)
        x = x[mask]
        y = y[mask]
        if x.size > 1 and y.size > 1:
            slope, intercept = np.polyfit(x, y, 1)
            x_vals = np.array([x.min(), x.max()])
            y_vals = slope * x_vals + intercept
            ax.plot(x_vals, y_vals, color="black", linewidth=2)

    # Pearson correlation coefficient and p-value annotation
    if show_corr:
        r, p = pearsonr(df[x_col], df[y_col])
        annot = f"r = {r:.2f}\np = {p:.2g}"
        ax.annotate(annot, xy=(0.02, 0.98), ha='left', va='top',
                    xycoords='axes fraction', fontsize=11,
                    bbox=dict(facecolor='white', edgecolor='none', alpha=0.7))

    # Set axis labels (after everything else)
    ax.set_xlabel(x_label if x_label is not None else x_col, fontsize=x_label_size)
    ax.set_ylabel(y_label if y_label is not None else y_col, fontsize=y_label_size)

    plt.tight_layout()
    return ax





print(final_aggregates_df['withdraw_mean'].value_counts())
print( 184 / (392+184))
print([col for col in final_aggregates_df.columns if '_score' in col])
scatter_with_options(final_aggregates_df, 'negative_affect_score_mean', 'danger_score_mean', color_col='withdraw_mean', fit_line=True, show_corr=False,
                     x_label = 'Negative Affect Score (0-3)', y_label = 'Danger Score (0-3)', legend_title='Withdrawal Rate (%)')
scatter_with_options(final_aggregates_df, 'high_arousal_score_mean', 'danger_score_mean', color_col='withdraw_mean', fit_line=False, show_corr=False,
                     x_label = 'High Arousal Score (0-3)', y_label = 'Danger Score (0-3)', legend_title='Withdrawal Rate (%)')
scatter_with_options(final_aggregates_df, 'delusion_score_mean', 'danger_score_mean', color_col='withdraw_mean', fit_line=True, show_corr=False,
                     x_label = 'Delusion Score (0-3)', y_label = 'Danger Score (0-3)', legend_title='Withdrawal Rate (%)')
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'negative_affect_score_mean', color_col='withdraw_mean', fit_line=True, show_corr=False,
                     x_label = 'Hypervigilance Score (0-12)', y_label = 'Negative Affect Score (0-3)', legend_title='Withdrawal Rate (%)', x_label_size=12, y_label_size=12, legend_title_size=12)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'negative_affect_inference_mean', color_col='withdraw_mean', fit_line=True, show_corr=True)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'inferred_negative_affect_certainty_scaled_mean', color_col='withdraw_mean', fit_line=True, show_corr=True)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'inferred_negative_affect_subsequent_mean', color_col='withdraw_mean', fit_line=True, show_corr=True)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'delusion_score_mean', color_col='withdraw_mean', fit_line=True, show_corr=True)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'danger_score_mean', color_col='withdraw_mean', fit_line=True, show_corr=False)

scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'interpretation_bias_score_mean', color_col='withdraw_mean', fit_line=True, show_corr=True)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'pB2_safety_self_transition', color_col='withdraw_mean', fit_line=True, show_corr=True)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'pB_affect_trust', color_col='withdraw_mean', fit_line=True, show_corr=True)
scatter_with_options(final_aggregates_df, 'hypervigilance_score', 'pD2_safety', color_col='withdraw_mean', fit_line=True, show_corr=True)


#display(final_aggregates_df[final_aggregates_df['negative_affect_score_mean'].isnull()][['negative_affect_score_mean','hypervigilance_score']])
#print(final_aggregates_df['negative_affect_score_mean'].min())
  # results_df['delusion_score'] = results_df['false_trust_inference'] + results_df['false_trust_certainty_scaled'] + results_df['false_trust_inference_subsequent']
  # results_df['interpretation_bias_score'] = results_df['false_untrustworthiness_inference'] + results_df['false_untrustworthiness_certainty_scaled'] + results_df['false_trust_inference_subsequent']
  # results_df['negative_affect_score'] = results_df['negative_affect_inference'] + results_df['inferred_negative_affect_certainty_scaled'] + results_df['inferred_negative_affect_subsequent']
  # results_df['high_arousal_score'] = results_df['high_arousal'] + results_df['high_arousal_certainty_scaled'] + results_df['high_arousal_subsequent_timestep']
  # results_df['danger_score'] = results_df['false_danger_inference'] + results_df['false_danger_certainty_scaled'] + results_df['false_danger_inference_subsequent']

# import numpy as np
# import pandas as pd

# def most_extreme_parameter_combo(final_aggregates_df, param_cols_filtered, group_col='withdraw_mean'):
#     # Separate the two groups
#     group1 = final_aggregates_df[final_aggregates_df[group_col] == 1.0][param_cols_filtered]
#     group0 = final_aggregates_df[final_aggregates_df[group_col] == 0.0][param_cols_filtered]
#     # Compute distances for each group1 row to all group0 rows (treat as numeric, can modify to categorical if needed)
#     distances = []
#     for idx1, row1 in group1.iterrows():
#         # Compute all distances (L1 norm here; can use Euclidean or Hamming for categorical)
#         dists = np.abs(group0 - row1).sum(axis=1)
#         min_dist = dists.min()
#         distances.append((idx1, min_dist))
#     # Get the idx with the largest minimum distance
#     most_extreme_idx, max_min_dist = max(distances, key=lambda x: x[1])
#     result_row = final_aggregates_df.loc[most_extreme_idx]
#     return result_row, max_min_dist

# # Example usage:
# result_row, distance = most_extreme_parameter_combo(final_aggregates_df, param_cols_filtered)
# display(result_row)
# print("Greatest minimal distance to other group:", distance)

# import pandas as pd
# import numpy as np

# def most_different_categorical_combo(final_aggregates_df, param_cols_filtered, group_col='withdraw_mean'):
#     # Select param sets for each group
#     group1 = final_aggregates_df[final_aggregates_df[group_col] == 1.0][param_cols_filtered]
#     group0 = final_aggregates_df[final_aggregates_df[group_col] == 0.0][param_cols_filtered]

#     # For each group1 row, find its minimal Hamming distance to any group0 row
#     best_idx = None
#     best_dist = -1
#     for idx1, row1 in group1.iterrows():
#         # Hamming: count mismatches per column, for each group0 row
#         hamming = (group0 != row1).sum(axis=1)
#         min_dist = hamming.min()
#         # Track max among mins (most different from its closest group0 row)
#         if min_dist > best_dist:
#             best_dist = min_dist
#             best_idx = idx1

#     # Output the most different group1 row with its minimal distance
#     result = final_aggregates_df.loc[best_idx]
#     return result, best_dist

# # Usage example:
# combo, distance = most_different_categorical_combo(final_aggregates_df, param_cols) #_filtered)
# print("Most categorically different group1 combo:\n", combo[param_cols]) #_filtered])
# print("Distance to nearest group0 combo:", distance)
# #print(param_cols)

# import pandas as pd
# from itertools import combinations

# def unique_combinations_by_group(final_aggregates_df, param_cols_filtered, group_col='withdraw_mean', X=2):
#     result_rows = []
#     for cols in combinations(param_cols_filtered, X):
#         # Find all unique value combinations and their group membership
#         combos = (
#             final_aggregates_df
#             .groupby(list(cols) + [group_col])
#             .size()
#             .reset_index()
#         )
#         # Remove the count column (not needed for uniqueness logic)
#         combos = combos.drop(columns=0)
#         # For each combo of values, see if it only appears in one group
#         combo_counts = combos.groupby(list(cols))[group_col].nunique()
#         unique_in_one_group = combo_counts[combo_counts == 1].index.values
#         # Record which group and which value combo
#         for combo_vals in unique_in_one_group:
#             # Find which group this combo is present in
#             group_present = combos[
#                 (combos[cols[0]] == combo_vals[0]) & (combos[cols[1]] == combo_vals[1])
#             ][group_col].iloc[0]
#             result_rows.append({
#                 'columns': cols,
#                 'values': combo_vals,
#                 'unique_to_group': int(group_present)
#             })
#     return pd.DataFrame(result_rows)

# # Example usage for X=2:
# unique_df = unique_combinations_by_group(final_aggregates_df, param_cols_filtered, X=4)
# display(unique_df)

# import pandas as pd

# def param_value_group_counts(final_aggregates_df, param_cols_filtered, group_col='withdraw_mean'):
#     total_group0 = (final_aggregates_df[group_col] == 0.0).sum()
#     total_group1 = (final_aggregates_df[group_col] == 1.0).sum()
#     output_rows = []
#     for param in param_cols_filtered:
#         unique_vals = sorted(final_aggregates_df[param].unique())
#         for val in unique_vals:
#             count_0 = ((final_aggregates_df[group_col] == 0.0) & (final_aggregates_df[param] == val)).sum()
#             count_1 = ((final_aggregates_df[group_col] == 1.0) & (final_aggregates_df[param] == val)).sum()
#             prop_0 = count_0 / total_group0 if total_group0 > 0 else 0
#             prop_1 = count_1 / total_group1 if total_group1 > 0 else 0
#             output_rows.append({
#                 "parameter": param,
#                 "value": val,
#                 "group0_counts": count_0,
#                 "group1_counts": count_1,
#                 "group0_prop": prop_0,
#                 "group1_prop": prop_1
#             })
#     return pd.DataFrame(output_rows)

# # Example usage:
# df_counts = param_value_group_counts(final_aggregates_df, param_cols_filtered)
# df_counts['prop_sum'] = df_counts['group0_prop'] + df_counts['group1_prop']
# display(df_counts)

display(final_aggregates_df.shape)

"""Here is an accurate interpretation of your logistic regression results table:

### Key Points for Each Column
- **coefficient**: Indicates the direction and strength of each parameter's effect across its full  range. Positive means higher values make 'withdraw' more likely, negative means less likely.
- **odds_ratio_0_to_1**: Multiplier for the odds of withdrawal if the parameter moves from its minimum (0.0) to maximum (1.0). Greater than 1 increases odds, less than 1 decreases odds.
- **odds_ratio_per_0.1_increase**: Multiplier for withdrawal odds for a small .1 (10%) increase in the parameter's value. Easier to interpret for incremental changes.

### Strongest Predictors
- **Increasing odds of withdrawal** (odds_ratio_0_to_1 distinctly >1):
    - `pA2_trust_safety`: Odds ratio 1.74; moving from 0 to 1 increases odds of withdrawal by 74%. A 0.1 increase boosts odds by 5.7%.
    - `pC_correct`: Odds ratio 1.67; moving from 0 to 1 increases odds by 67%. A 0.1 increase boosts odds by 5.3%.
    - `pB_affect_trust`: Odds ratio 1.32; moving parameter from 0 to 1 increases odds by 32%. A 0.1 increase boosts odds by 2.8%.
    - `pC_arousal_low` and `pA2_affect_safety` have moderate positive effects (odds ratios ~1.2 and ~1.08, respectively).

- **Decreasing odds of withdrawal** (odds_ratio_0_to_1 distinctly <1):
    - `pC2_trust`: Odds ratio 0.77; moving from 0 to 1 decreases odds by 23%. A 0.1 increase reduces odds by about 2.6%.
    - `pD2_safety`: Odds ratio 0.84; moving from 0 to 1 decreases odds by 16%. A 0.1 increase reduces odds by about 1.7%.
    - Parameters like `pA_advice_trust`, `pB_trust`, `pB_correctcard`, `pB_choice`, `pB_stage`, `pB2_safety_world_transition`, and others all have odds ratios of ~0.920.91, meaning a full-range increase reduces withdrawal odds by about 89%, and a 0.1 increase reduces odds by ~0.8%.

- **Minimal effects**:
    - `pA_feedback_withdraw_trust`, `pA_arousal_affect`, `pD_trust`, `pD_affect`, `pC2_angry_affect`odds ratios very close to 1 for both full-range and 0.1 steps (indicating a weak association).

### How to Read the Table for Decision-Making
- **If a parameters odds_ratio_0_to_1 is much greater than 1**: Large changes in that parameter drive strong increases in withdrawal likelihood.
- **If a parameters odds_ratio_0_to_1 is much less than 1**: Large changes in that parameter drive strong decreases in withdrawal likelihood.
- **odds_ratio_per_0.1_increase**: Use for practical incrementsmost parameter increases of 0.1 have only 16% effects on odds for your data.

### Example Interpretations:
- If `pA2_trust_safety` is raised from 0 to 1, withdrawal odds are multiplied by 1.74 (i.e., 74% higher odds), and each 0.1 increase multiplies odds by 1.057 (about 5.7% higher odds).
- If `pC2_trust` is raised from 0 to 1, withdrawal odds are multiplied by 0.77 (i.e., 23% lower odds), and each 0.1 increase multiplies odds by 0.9743 (about 2.6% lower odds).

This provides a clear, quantitative ranking of which parameters most strongly predict 'withdraw' in your model, and how their individual changes affect the likelihood for both full-scale and steptepwise changes.[1][2][3]

[1](https://www.bookdown.org/rwnahhas/RMPH/blr-interp.html)
[2](https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faq-how-do-i-interpret-odds-ratios-in-logistic-regression/)
[3](https://support.minitab.com/en-us/minitab/help-and-how-to/statistical-modeling/regression/how-to/fit-binary-logistic-model/interpret-the-results/all-statistics-and-graphs/odds-ratios/)
"""

import pandas as pd
import numpy as np

# Custom mode function for Series
def safe_mode(x):
    m = x.mode()
    return m.iloc[0] if not m.empty else np.nan

# List of aggregation functions
agg_funcs = ['mean', 'std']   # 'median', safe_mode, 'min', 'max']

# Group by 'withdraw_mean' and aggregate using list of functions
summary = final_aggregates_df.groupby('withdraw_mean')[param_cols].agg(agg_funcs)

# Rename columns for clarity
summary.columns = [
    f"{col}_{stat}" if not stat == '<lambda>' else f"{col}_mode"
    for col, stat in summary.columns
]

#display(summary)
summary_t = summary.T.reset_index()
summary_t.columns = ['parameter','withdraw_no','withdraw_yes']
summary_t['withdraw_diff'] = summary_t['withdraw_yes'] - summary_t['withdraw_no']
display(summary_t[summary_t['withdraw_no'] != summary_t['withdraw_yes']].sort_values('withdraw_diff'))



from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_text
clf = DecisionTreeClassifier(random_state=0, max_depth=None)
# Remove static columns from param_cols
param_cols_filtered = [col for col in param_cols if final_aggregates_df[col].nunique() > 1]
clf.fit(final_aggregates_df[param_cols_filtered], final_aggregates_df['withdraw_mean'])
#print(export_text(clf, feature_names=param_cols_filtered))
tree_fi = pd.DataFrame({'feature': param_cols_filtered, 'importance': clf.feature_importances_})
display(tree_fi.sort_values('importance', ascending=False))
print("Feature importances:", clf.feature_importances_)


# Visualize splits
plt.figure(figsize=(12,8))
plot_tree(clf, feature_names=param_cols_filtered, class_names=['Never withdraw', 'Always withdraw'], filled=True)
plt.show()

for feature1 in param_cols:
  print(feature1)
  display(final_aggregates_df[['withdraw_mean',feature1]].value_counts(dropna=False).reset_index())

final_aggregates_df['withdraw_mean'].value_counts(dropna=False)

param_cols

# correlation_df = final_aggregates_df.corr()
# display(correlation_df)
#print(final_aggregates_df.info())

final_aggregates_df = pd.read_csv(f"{BASE_PATH}/grid_search_{stripped_time}/analyses_aggregates.csv")
print(final_aggregates_df_reloaded.info())
display(final_aggregates_df_reloaded)

print(len(final_aggregates_df['grid_idx'].unique()))

import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import pearsonr

# Scatter data
x = final_aggregates_df['trust_belief_mean']
y = final_aggregates_df['withdraw_mean']

# Pearson's r and p-value
r, p = pearsonr(x, y)

fig, ax = plt.subplots(figsize=(8, 6))

# Scatter points
ax.scatter(x, y, alpha=0.7, edgecolor='w', label='Data points')

# Regression (best-fit) line
sns.regplot(x=x, y=y, scatter=False, ax=ax, color='red', line_kws={'label': 'Best fit'})

# Annotate correlation
annot_text = f"Pearson r = {r:.2f}\np-value = {p:.3g}"
ax.annotate(annot_text, xy=(0.02, 0.98), xycoords='axes fraction',
            ha='left', va='top', fontsize=12)

ax.set_xlabel('trust_belief_mean')
ax.set_ylabel('withdraw_mean')
ax.set_title('Scatterplot: trust_belief_mean vs. withdraw_mean')
ax.legend()
plt.tight_layout()
plt.show()


print(final_aggregates_df['withdraw_mean'].unique())

# rd_action'] != 'withdraw

"""# Timestep and individual model visualization (reload lowest/middle/highest hypervigilance score agents)"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

BASE_PATH = "/content/drive/MyDrive/PTSD_ActInf_GridSearch"
stripped_time = "20251005043852938836"
results_df = pd.read_csv(f"{BASE_PATH}/grid_search_{stripped_time}/grid_idx_323.csv", low_memory=False, keep_default_na=False)
results_df_mid = pd.read_csv(f"{BASE_PATH}/grid_search_{stripped_time}/grid_idx_391.csv", low_memory=False, keep_default_na=False)
results_df_high = pd.read_csv(f"{BASE_PATH}/grid_search_{stripped_time}/grid_idx_382.csv", low_memory=False, keep_default_na=False)
print(results_df.shape)

import ast

def safe_literal_eval(val):
    try:
        # Only evaluate strings that look like lists
        if isinstance(val, str) and val.startswith('[') and val.endswith(']'):
            return ast.literal_eval(val)
        else:
            # Return original value if not a valid list string
            return val
    except Exception:
        # Return original value for any parse error
        return val

for col in results_df.columns:
        #print(f"Convert {col} to list of lists...")
        try:
          results_df[col] = results_df[col].apply(safe_literal_eval)
          #print(f"Convert {col} to list of lists: results_df_reloaded.loc[0,'{col}'][0]) {results_df_reloaded.loc[0,col][0]}")
        except:
          continue

for col in results_df.columns:
        #print(f"Convert {col} to list of lists...")
        try:
          results_df_mid[col] = results_df_mid[col].apply(safe_literal_eval)
          #print(f"Convert {col} to list of lists: results_df_reloaded.loc[0,'{col}'][0]) {results_df_reloaded.loc[0,col][0]}")
        except:
          continue

for col in results_df.columns:
        #print(f"Convert {col} to list of lists...")
        try:
          results_df_high[col] = results_df_high[col].apply(safe_literal_eval)
          #print(f"Convert {col} to list of lists: results_df_reloaded.loc[0,'{col}'][0]) {results_df_reloaded.loc[0,col][0]}")
        except:
          continue

print(results_df.columns)
display(results_df.loc[0][['pA2_affect_safety', 'pA2_trust_safety', 'pA_advice_trust',
       'pA_arousal_affect', 'pA_feedback_belief_choice',
       'pA_feedback_withdraw_trust', 'pB2_safety_other_transition',
       'pB2_safety_self_transition', 'pB2_safety_world_transition',
       'pB_affect_trust', 'pB_choice', 'pB_correctcard', 'pB_stage',
       'pB_trust', 'pC2_angry_affect', 'pC2_trust', 'pC_arousal_low',
       'pC_correct', 'pD2_safety', 'pD_affect', 'pD_trust']])

display(results_df.loc[0][['pA2_affect_safety', 'pA2_trust_safety', 'pA_advice_trust',
       'pA_arousal_affect', 'pA_feedback_belief_choice',
       'pA_feedback_withdraw_trust', 'pB2_safety_other_transition',
       'pB2_safety_self_transition', 'pB2_safety_world_transition',
       'pB_affect_trust', 'pB_choice', 'pB_correctcard', 'pB_stage',
       'pB_trust', 'pC2_angry_affect', 'pC2_trust', 'pC_arousal_low',
       'pC_correct', 'pD2_safety', 'pD_affect', 'pD_trust']])

display(results_df.loc[0][['grid_idx','pA2_affect_safety', 'pA2_trust_safety', 'pA_advice_trust',
       'pA_arousal_affect', 'pA_feedback_belief_choice',
       'pA_feedback_withdraw_trust', 'pB2_safety_other_transition',
       'pB2_safety_self_transition', 'pB2_safety_world_transition',
       'pB_affect_trust', 'pB_choice', 'pB_correctcard', 'pB_stage',
       'pB_trust', 'pC2_angry_affect', 'pC2_trust', 'pC_arousal_low',
       'pC_correct', 'pD2_safety', 'pD_affect', 'pD_trust']])

print(type(results_df.loc[0, 'qs_bma']))
print(len(results_df.loc[0, 'qs_bma']))
for i in range(len(results_df.loc[0, 'qs_bma'])):
  print(results_df.loc[0, 'qs_bma'][i])
print(len(results_df.loc[0, 'gamma_history']))

df = results_df.copy()

print(results_df.loc[0,'B'][4])
for i in range(len(results_df.loc[0,'B'][4])):
  print(f"B[4][{i}] = {results_df.loc[0,'B'][4][i]}")

for row in range(len(results_df[220:240])):
  print(f"Row {row} --------------------------------")
  for i in range(len(results_df.loc[row,'B'][4])):
    print(f"B[4][{i}] = {results_df.loc[row,'B'][4][i]}")
print(type(results_df.loc[0,'B']))

import matplotlib.pyplot as plt
import numpy as np

def plot_model(df):
    from matplotlib.gridspec import GridSpec

    hidden_state_labels = [
        ['trust', 'distrust'],
        ['blue', 'green'],
        ['angry', 'calm'],
        ['blue', 'green', 'null', 'withdraw'],
        ['null', 'advice', 'decision']
    ]
    qs_high_labels = [
        ['safe', 'danger'],
        ['safe', 'danger'],
        ['safe', 'danger']
    ]
    policy_labels = [
        'trust+green', 'trust+blue', 'distrust+blue', 'distrust+green', 'trust+withdraw', 'distrust+withdraw'
    ]
    obs_modality_labels = [
        ['blue', 'green', 'null'],       # advice_obs
        ['correct', 'incorrect', 'null'],# feedback_obs
        ['high', 'low'],                 # arousal_obs
        ['blue', 'green', 'null', 'withdraw'] # choice_obs
    ]
    modality_names = ['advice_obs', 'feedback_obs', 'arousal_obs', 'choice_obs']
    modality_colors = {
        'blue':'#1f77b4', 'green':'#2ca02c', 'null':'#7f7f7f', 'correct':'#FFD700', 'incorrect':'#C41236',
        'high':'#e377c2', 'low':'#8c564b', 'withdraw':'#9467bd'
    }

    action_labels = {
        'trust_action': ['trust', 'distrust'],
        'card_action': ['green', 'blue', 'null', 'withdraw']
    }
    action_colors = {
        'trust': '#1f77b4', 'distrust': '#C41236',
        'green': '#2ca02c', 'blue': '#1f77b4',
        'null': '#7f7f7f', 'withdraw': '#9467bd'
    }
    action_names = ['trust_action', 'card_action']

    N = len(df)

    def expand_hidden_states(series, labels, prefix):
        expanded = []
        ytick_labels = []
        for i, state_labels in enumerate(labels):
            for j, label in enumerate(state_labels):
                arr = [row[i][j] for row in series]
                expanded.append(arr)
                ytick_labels.append(f'{prefix}{i+1}_{label}')
        return np.array(expanded), ytick_labels

    qo_heatmap, qo_yticks = expand_hidden_states(df['qo_pi_high'], hidden_state_labels, 's')
    qsh_heatmap, qsh_yticks = expand_hidden_states(df['qs_high_bma'], qs_high_labels, 's')
    qs_heatmap, qs_yticks = expand_hidden_states(df['qs_bma'], hidden_state_labels, 's')

    q_pi = np.array([row for row in df['q_pi']])
    q_pi = q_pi.T

    obs_array = np.array([row for row in df['obs']]).T

    gamma = df['gamma'].values
    gamma_history_full = np.zeros(N*16)
    for i,row in enumerate(df['gamma_history']):
        if len(row) == 16:
            gamma_history_full[i*16:(i+1)*16] = row

    heights = [2,2,4,3,2,3,4]
    width_ratios = [18,2]  # Widen right col a bit for wide legends if needed
    gs = GridSpec(7, 2, width_ratios=width_ratios, height_ratios=heights, wspace=0.04, hspace=0.45, figure=plt.gcf() if plt.get_fignums() else plt.figure(figsize=(16, 25)))
    fig = plt.gcf() if plt.get_fignums() else plt.figure(figsize=(16, 25))
    plt.clf()
    cmap = 'binary_r'

    axes = []
    cbaxes = []
    for row in range(7):
        ax = fig.add_subplot(gs[row,0])
        cax = fig.add_subplot(gs[row,1])
        cbaxes.append(cax)
        axes.append(ax)

    # --- 1. qo_pi_high heatmap ---
    im1 = axes[0].imshow(qo_heatmap, aspect='auto', cmap=cmap, interpolation='nearest', vmin=0.0, vmax=1.0)
    axes[0].set_ylabel('Expected L1 states')
    axes[0].set_yticks(range(len(qo_yticks)))
    axes[0].set_yticklabels(qo_yticks)
    axes[0].set_xlabel('Timestep')
    axes[0].set_title('Expected L1 States (qo_pi_high)')
    plt.colorbar(im1, cax=cbaxes[0])
    cbaxes[0].set_ylabel('Prob.')

    # --- 2. qs_high_bma heatmap ---
    im2 = axes[1].imshow(qsh_heatmap, aspect='auto', cmap=cmap, interpolation='nearest', vmin=0.0, vmax=1.0)
    axes[1].set_ylabel('Higher level states')
    axes[1].set_yticks(range(len(qsh_yticks)))
    axes[1].set_yticklabels(qsh_yticks)
    axes[1].set_xlabel('Timestep')
    axes[1].set_title('Higher Level State Beliefs (qs_high_bma)')
    plt.colorbar(im2, cax=cbaxes[1])
    cbaxes[1].set_ylabel('Prob.')

    # --- 3. qs_bma heatmap ---
    im3 = axes[2].imshow(qs_heatmap, aspect='auto', cmap=cmap, interpolation='nearest', vmin=0.0, vmax=1.0)
    axes[2].set_ylabel('Lower level states')
    axes[2].set_yticks(range(len(qs_yticks)))
    axes[2].set_yticklabels(qs_yticks)
    axes[2].set_xlabel('Timestep')
    axes[2].set_title('Lower Level State Beliefs (qs_bma)')
    plt.colorbar(im3, cax=cbaxes[2])
    cbaxes[2].set_ylabel('Prob.')

    # --- 4. Observation scatterplot (one row per modality) ---
    for mi, modality in enumerate(modality_names):
        row = obs_array[mi]
        for j,label in enumerate(obs_modality_labels[mi]):
            indices = np.where(row == j)[0]
            color = modality_colors.get(label, None)
            axes[3].scatter(indices,
                            np.full_like(indices, mi),
                            color=color, label=f'{modality}:{label}', s=30)
    axes[3].set_yticks(range(len(modality_names)))
    axes[3].set_yticklabels(modality_names)
    axes[3].set_title('Observations over Time')
    axes[3].set_ylabel('Modality')
    axes[3].set_xlabel('Timestep')
    obs_handles, obs_labels = axes[3].get_legend_handles_labels()
    obs_by_label = dict(zip(obs_labels, obs_handles))
    # Place legend in external column only, not on the plot
    cbaxes[3].axis('off')
    cbaxes[3].legend(obs_by_label.values(), obs_by_label.keys(), loc='center left', bbox_to_anchor=(0, 0.5), fontsize='small', frameon=False)

    # --- 5. Actions over Time scatterplot (NEW) ---
    for ai, action in enumerate(action_names):
        series = df[action].values
        for label in action_labels[action]:
            indices = np.where((series == label) & (df['timestep'].values != 2))[0]
            color = action_colors.get(label, None)
            axes[4].scatter(indices, np.full_like(indices, ai), color=color, label=f'{action}:{label}', s=40)
    axes[4].set_yticks(range(len(action_names)))
    axes[4].set_yticklabels(action_names)
    axes[4].set_title('Actions over Time')
    axes[4].set_ylabel('Action')
    axes[4].set_xlabel('Timestep')
    action_handles, action_labels_ = axes[4].get_legend_handles_labels()
    action_by_label = dict(zip(action_labels_, action_handles))
    cbaxes[4].axis('off')
    cbaxes[4].legend(action_by_label.values(), action_by_label.keys(), loc='center left', bbox_to_anchor=(0, 0.5), fontsize='small', frameon=False)

    # --- 6. Policy heatmap + chosen policies ---
    im4 = axes[5].imshow(q_pi, aspect='auto', cmap=cmap, interpolation='nearest', vmin=0.0, vmax=1.0)
    axes[5].set_yticks(range(6))
    axes[5].set_yticklabels(policy_labels)
    axes[5].set_title('Policy Posterior Probabilities (q_pi)')
    axes[5].set_ylabel('Policy')
    axes[5].set_xlabel('Timestep')
    plt.colorbar(im4, cax=cbaxes[5])
    cbaxes[5].set_ylabel('Prob.')
    for i in range(N):
        if df['timestep'].iloc[i] != 2:
            chosen = np.argmax(df['q_pi'].iloc[i])
            axes[5].scatter(i, chosen, color='red', s=24)

    # --- 7. Gamma term line plot ---
    x_gamma = np.arange(N)  # trial-level steps
    x_gamma_history = np.linspace(0, N, N * 16, endpoint=False)
    line1, = axes[6].plot(x_gamma_history, gamma_history_full, color='purple', label='gamma_history', linewidth=1)
    line2, = axes[6].plot(x_gamma, gamma, color='green', label='gamma', marker='o', markersize=7, linewidth=2)
    axes[6].set_title('Policy Precision Terms')
    axes[6].set_xlabel('Trial (gamma) / Sub-timestep (gamma_history)')
    axes[6].set_ylabel('Value')
    for t in x_gamma:
        axes[6].axvline(t, color='gray', linestyle='--', alpha=0.2)
    cbaxes[6].axis('off')
    cbaxes[6].legend([line1, line2], ['gamma_history', 'gamma'], loc='center left', bbox_to_anchor=(0, 0.5), fontsize='small', frameon=False)

    #plt.tight_layout()
    plt.show()

# Usage example (replace with your real DataFrame slice)
plot_model(results_df_mid) #[0:30])

display(results_df['obs_label'].head(10))
for i in range(10):
  print(results_df_mid.loc[i, 'qs_bma'][4])

# Data dictionary
print(results_df.loc[0, 'qs_bma'][0])

with pd.option_context('display.max_rows',None):
  display(results_df_mid.head(10).T)


"""
'trial'        # the ID of the current trial (int)
'timestep'     # the current timestep in the trial (int with values in {0,1,2})
'qs_bma'       # Bayesian model averaged LOWER LEVEL (L1) agent's/layer's beliefs at the current timestep  (list of 5 lists i.e. 1 list per hidden state factors in N=5 hidden state factors (each list representing a categorical probability distribution over x states where number of states can
               # differ between each hidden state factor, where all list elements contain float probabilities), where list i represents Q(s_n) in N hidden state factors)
# The labels for each hidden state factor in the LOWER LEVEL (L1) qs_bma, in order from index 0 to 4, are as follows:
# trustworthiness_states = ['trust','distrust'] # example value of qs_bma[0]: `[0.5, 0.5]`
# correct_card_states = ['blue','green'] # example value of qs_bma[1]: `[0.5, 0.5]`
# affect_states = ['angry','calm']     # example value of qs_bma[2]: [0.67, 0.33]
# choice_states = ['blue','green','null','withdraw']  # example value of qs_bma[3]: [0.0, 0.0, 1.0, 0.0]
# stage_states = ['null','advice','decision']   # example value of qs_bma[4]: [0.0, 1.0, 0.0]
# num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)]

'q_pi'         # Current posteriors over policies (list of 6 float probabilities in range [0,1], representing Q(pi), indexed to each policy)
# The labels for each policy in the q_pi, in order from index 0 to 5, are as follows:
# 0: 'trust+green'
# 1: 'trust+blue'
# 2: 'distrust+blue'
# 3: 'distrust+green'
# 4: 'trust+withdraw'
# 5: 'distrust+withdraw'


'neg_efe'      # Negative expected free energy computed at the current timestep (list of 6 float values indexed to each policy; G_pi)
'expected_utility'  # Pragmatic value terms contributing to neg_efe per policy (list of 6 float values indexed to each policy)
'info_gain'         # Epistemic value terms contributing to neg_efe per policy (list of 6 float values indexed to each policy)
'trust_action'      # Trust-related action agent chose at current timestep (string value in {'trust','distrust'}) *Always empty at timestep 2*
'card_action'       # Card-related action agent chose at current timestep (string value in {'green','blue','null','withdraw'}) *Always empty at timestep 2*
'obs_label'         # Observations the agent received at current timestep (list of 4 strings indexed to each modality; modalities per index are [advice,feedback,arousal,choice])
'obs'               # Observations the agent received at current timestep (list of 4 integers indexed to discrete observations in modalities)
'true_trustworthiness' # True trustworthiness of advisor in current timestep; an uncontrollable environmental state (string in {'trustworthy','untrustworthy'})
'true_color'           # True color of the card; an uncontrollable environmental state (string in {'blue','green'})
'high_arousal_prob'    # Probability of high arousal at current timestep (float in [0,1])
'gamma'                # Expected policy precision term (float)
'gamma_history'        # History of expected policy precision terms from update during current timestep (list of 16 float values) *Always empty at timestep 2*

'qs_high_bma'       # Bayesian model averaged HIGHER LEVEL (L2) agent's/layer's beliefs at the current timestep  (list of 3 lists i.e. 1 list per hidden state factor in N=3 hidden state factors (each list representing a categorical probability distribution over x states where number of states can
               # differ between each hidden state factor, where all list elements contain float probabilities), where list i represents Q(s_n) in N hidden state factors)
# The labels for each hidden state factor in the HIGHER LEVEL (L2) qs_high_bma, in order from index 0 to 2, are as follows:
# safety_self_states = ['safe','danger'] # example value of qs_high_bma[0]: `[0.5, 0.5]`
# safety_self_states = ['safe','danger'] # example value of qs_high_bma[0]: `[0.5, 0.5]`
# safety_self_states = ['safe','danger'] # example value of qs_high_bma[0]: `[0.5, 0.5]`
'qo_pi_high'      # HIGHER LEVEL (L2) expected observations (list of 5 lists i.e. 1 list per hidden state factor in N=5 hidden state factors; this list of lists is STRUCTURED and LABELLED exactly like the LOWERL LEVEL agent posteriors over states `qs_bma`)

"""

print(f"Lowest hypervigilance: {results_df.shape}")
print(f"Middle hypervigilance: {results_df_mid.shape}")
print(f"Highest hypervigilance: {results_df_high.shape}")

print(results_df.info(verbose=True))

import numpy as np
import re

def parse_vn_string(vn_str):
    # Replace "array" with "np.array([" and add ending brackets
    vn_str = re.sub(r'array', 'np.array([', vn_str)
    # Replace dtypeobject, dtype=float, or similar artifacts if present
    vn_str = re.sub(r'dtype[^\s,]*', '', vn_str)
    # Convert trailing numbers to end brackets and commas, heuristic
    vn_str = re.sub(r'(\d)\s*,?\s*$', r'\1])', vn_str)
    # Add ending brackets if missing, make array splits
    vn_str = vn_str.replace('])', ']),')
    # Fix numbers separated by just commas (inside arrays)
    vn_str = re.sub(r'\s,', ',', vn_str)
    # Remove redundant commas
    vn_str = re.sub(r',+', ',', vn_str)
    # Add ending bracket to the full structure
    vn_str = vn_str.rstrip(',') + ']'
    # Final structure should start with "np.array(["
    if not vn_str.startswith('np.array(['):
        vn_str = 'np.array([' + vn_str
    # Evaluate string to get structure
    results_df_vn0 = eval(vn_str)
    return results_df_vn0

# Usage
vn_string = results_df.loc[0, 'vn']
results_df_vn0 = parse_vn_string(vn_string)

print(type(results_df.loc[0,'C']))
print(results_df.loc[0,'C'])

print(results_df.loc[0,'xn'])
print(type(results_df.loc[0,'xn']))
print(results_df.loc[0,'xn'].shape)

with pd.option_context('display.max_rows',None):
  display(results_df.head(5).T)

import matplotlib.pyplot as plt
import numpy as np

def plot_C_modalities(results_df, y_prob_limits=False, modalities_to_plot=[0,1,2,3], plot_title=''):
    modality_labels = ['Advice', 'Feedback', 'Arousal', 'Choice']
    value_names = [
        ['blue', 'green', 'null'],
        ['Correct', 'Incorrect', 'No Feedback'],
        ['High', 'Low'],
        ['blue', 'green', 'null', 'withdraw']
    ]

    n_modalities = len(modalities_to_plot)
    fig, axes = plt.subplots(
        n_modalities, 1, figsize=(8, 2 * n_modalities), sharex=True,
        gridspec_kw={'hspace': 0}
    )
    if n_modalities == 1:
        axes = [axes]

    timesteps = range(len(results_df))

    for plot_idx, mod_idx in enumerate(modalities_to_plot):
        mod_label = modality_labels[mod_idx]
        val_names = value_names[mod_idx]
        n_values = len(val_names)
        avg_probs = np.zeros((len(results_df), n_values))

        for i in timesteps:
            row_C = results_df.iloc[i]['C']
            if len(row_C) > mod_idx and isinstance(row_C[mod_idx], list):
                avg = np.array(row_C[mod_idx])
            else:
                avg = np.zeros(n_values)
            avg_probs[i] = avg

        for val_idx, name in enumerate(val_names):
            axes[plot_idx].plot(timesteps, avg_probs[:, val_idx], label=name)

        axes[plot_idx].set_ylabel(mod_label)
        if y_prob_limits:
            axes[plot_idx].set_ylim(0.0, 1.0)
        axes[plot_idx].legend(loc='upper right', fontsize=10)
        axes[plot_idx].grid(True, alpha=0.3)

    axes[-1].set_xlabel('Timestep')
    if plot_title:
        fig.suptitle(plot_title)
    plt.tight_layout(h_pad=0)
    plt.subplots_adjust(hspace=0, top=0.92 if plot_title else 0.97)
    plt.show()

# display(results_df_mid[['C']].head(30))

plot_C_modalities(results_df, modalities_to_plot = [1,2], plot_title='C Matrix: Feedback and Arousal Modalities')
plot_C_modalities(results_df_mid, modalities_to_plot = [1,2], plot_title='C Matrix: Feedback and Arousal Modalities')
plot_C_modalities(results_df_high, modalities_to_plot = [1,2], plot_title='C Matrix: Feedback and Arousal Modalities')

import matplotlib.pyplot as plt
import numpy as np

def plot_factors(results_df, col='D', y_prob_limits=False, factors_to_plot=[0,1,2,3,4], plot_title=''):
    factor_labels = [
        'Trustworthiness', 'Correct Card', 'Affect', 'Choice', 'Stage'
    ]
    value_names = [
        ['Trustworthy', 'Untrustworthy'],
        ['blue', 'green'],
        ['Negative', 'Positive'],
        ['blue', 'green', 'null', 'withdraw'],
        ['null', 'advice', 'decision']
    ]

    # Select which factors to plot
    plot_labels = [factor_labels[i] for i in factors_to_plot]
    plot_values = [value_names[i] for i in factors_to_plot]
    n_factors = len(plot_labels)

    fig, axes = plt.subplots(
        n_factors, 1, figsize=(8, 2 * n_factors), sharex=True,
        gridspec_kw={'hspace': 0}
    )
    if plot_title:
        fig.suptitle(plot_title)
    if n_factors == 1:
        axes = [axes]

    timesteps = range(len(results_df))

    for ax_idx, (mod_idx, factor_label, val_names) in enumerate(zip(factors_to_plot, plot_labels, plot_values)):
        n_values = len(val_names)
        avg_probs = np.zeros((len(results_df), n_values))

        for i in timesteps:
            row_D = results_df.iloc[i][col]
            if len(row_D) > mod_idx and isinstance(row_D[mod_idx], list):
                avg = np.array(row_D[mod_idx])
            else:
                avg = np.zeros(n_values)
            avg_probs[i] = avg

        for val_idx, name in enumerate(val_names):
            axes[ax_idx].plot(timesteps, avg_probs[:, val_idx], label=name)

        axes[ax_idx].set_ylabel(factor_label)
        if y_prob_limits:
            axes[ax_idx].set_ylim(0.0, 1.0)
        axes[ax_idx].legend(loc='upper right', fontsize=10)
        axes[ax_idx].grid(True, alpha=0.3)

    axes[-1].set_xlabel('Timestep')
    plt.tight_layout(h_pad=0)
    plt.subplots_adjust(hspace=0)
    plt.show()

plot_factors(results_df, col='D', y_prob_limits=True, factors_to_plot=[0,2], plot_title='L1 D matrix')
plot_factors(results_df, col='qs_bma', y_prob_limits=True, factors_to_plot=[0,2], plot_title='L1 Q(s) (Bayesian Model Average)')
plot_factors(results_df, col='qo_pi_high', y_prob_limits=True, plot_title='L2 Q(o)')

plot_factors(results_df_mid, col='D', y_prob_limits=True, factors_to_plot=[0,2], plot_title='L1 D matrix')
plot_factors(results_df_mid, col='qs_bma', y_prob_limits=True, factors_to_plot=[0,2], plot_title='L1 Q(s) (Bayesian Model Average)')
plot_factors(results_df_mid, col='qo_pi_high', y_prob_limits=True, plot_title='L2 Q(o)')

plot_factors(results_df_mid, col='qs_bma', y_prob_limits=True, factors_to_plot=[0], plot_title='L1 Q(s) (Bayesian Model Average)')
plot_factors(results_df_mid, col='qs_bma', y_prob_limits=True, factors_to_plot=[2], plot_title='L1 Q(s) (Bayesian Model Average)')

#display(results_df_mid.loc[0,'D'])

print('')
print(results_df.loc[0,'B'])
print('')

for i in range(len(results_df.loc[0,'B'])):
  print(f"B[{i}] len={len(results_df.loc[0,'B'][i])}")

print(results_df.loc[0,'B'][0][0][0])

import matplotlib.pyplot as plt
import numpy as np

def plot_transitions(results_df, col='B', plot_title='', states=None, actions=None, factor_idx=0):
    if states is None or actions is None:
        raise ValueError("states and actions arguments must be provided.")

    n_states = len(states)
    n_actions = len(actions)
    n_transitions = n_states * n_states
    n_timesteps = len(results_df)

    transition_probs = np.zeros((n_transitions, n_actions, n_timesteps))
    transition_labels = []
    for t in range(n_timesteps):
        b_mat = results_df.iloc[t][col][factor_idx]
        tr_idx = 0
        for from_state in range(n_states):
            for to_state in range(n_states):
                for action in range(n_actions):
                    transition_probs[tr_idx, action, t] = b_mat[to_state][from_state][action]
                if t == 0:
                    transition_labels.append(f"P({states[to_state]} | {states[from_state]}, action)")
                tr_idx += 1

    fig, axes = plt.subplots(
        n_transitions, 1, figsize=(8, 2 * n_transitions),
        sharex=True, gridspec_kw={'hspace': 0}
    )
    if plot_title:
        fig.suptitle(plot_title)
    if n_transitions == 1:
        axes = [axes]

    timesteps = range(n_timesteps)

    for ax_idx in range(n_transitions):
        for action_idx, action_name in enumerate(actions):
            axes[ax_idx].plot(
                timesteps, transition_probs[ax_idx, action_idx, :],
                label=f"action={action_name}"
            )
        axes[ax_idx].set_ylabel(transition_labels[ax_idx])
        axes[ax_idx].legend(loc='upper right', fontsize=10)
        axes[ax_idx].grid(True, alpha=0.3)

    axes[-1].set_xlabel('Timestep')
    plt.tight_layout(h_pad=0)
    plt.subplots_adjust(hspace=0)
    plt.show()

print(results_df.loc[0,'B'][0][0][0])
print(results_df.loc[30,'B'][0][0][0])
print(results_df.loc[200,'B'][0][0][0])
#print(results_df.loc[0,'B'][0][0][0])

plot_transitions(results_df, col = 'B', plot_title='L1 Trustworthiness Transitions', states=['trust', 'distrust'], actions=['trust', 'distrust'], factor_idx=0)


plot_transitions(results_df, col = 'B', plot_title='L1 Affect Transitions', states=['angry', 'calm'], actions=['trust', 'distrust'], factor_idx=2)

plot_transitions(results_df, col = 'B2', plot_title='L2 Safety (Self) Transitions', states=['safe', 'danger'], actions=['null'], factor_idx=0)

import matplotlib.pyplot as plt
import numpy as np

def plot_action_transitions(results_df, plot_title='', states=None, actions=None, factor_idx=0):
    if states is None or actions is None:
        raise ValueError("states and actions arguments must be provided.")

    n_states = len(states)
    n_actions = len(actions)
    n_transitions = n_states * n_states
    n_timesteps = len(results_df)

    # shape: (n_actions, n_transitions, n_timesteps)
    transition_probs = np.zeros((n_actions, n_transitions, n_timesteps))
    transition_labels = []
    for t in range(n_timesteps):
        b_mat = results_df.iloc[t]['B'][factor_idx]
        for a in range(n_actions):
            tr_idx = 0
            for from_state in range(n_states):
                for to_state in range(n_states):
                    transition_probs[a, tr_idx, t] = b_mat[to_state][from_state][a]
                    if t == 0 and a == 0:
                        transition_labels.append(f"{states[from_state]} to {states[to_state]}")
                    tr_idx += 1

    fig, axes = plt.subplots(
        n_actions, 1, figsize=(8, 2 * n_actions),
        sharex=True, gridspec_kw={'hspace': 0}
    )
    if plot_title:
        fig.suptitle(plot_title)
    if n_actions == 1:
        axes = [axes]

    timesteps = range(n_timesteps)

    for action_idx, action_name in enumerate(actions):
        for tr_idx, label in enumerate(transition_labels):
            axes[action_idx].plot(
                timesteps, transition_probs[action_idx, tr_idx, :],
                label=label
            )
        axes[action_idx].set_ylabel(f"action={action_name}")
        axes[action_idx].legend(loc='upper right', fontsize=10)
        axes[action_idx].grid(True, alpha=0.3)

    axes[-1].set_xlabel('Timestep')
    plt.tight_layout(h_pad=0)
    plt.subplots_adjust(hspace=0)
    plt.show()

plot_action_transitions(
    results_df,
    plot_title='',
    states=['trust', 'distrust'],
    actions=['trust', 'distrust'],
    factor_idx=0
)

plot_action_transitions(
    results_df,
    plot_title='',
    states=['angry', 'calm'],
    actions=['trust', 'distrust'],
    factor_idx=2
)

print(results_df.loc[0,'B'][0][0])

print(results_df.loc[0,'qs_bma'])
print(results_df.loc[0,'qs_high_bma'])

# Define hidden state factors with respective hidden states
trustworthiness_states = ['trust','distrust'] # Hidden state factor denoting beliefs about advisor's trustworthiness, used in B[0] and D[0] and A matrix
correct_card_states = ['blue','green'] # Hidden state factor denoting beliefs about the correct color of the card, used in B[1] and D[1] and A matrix
affect_states = ['angry','calm']     # Hidden state factor denoting beliefs about affective state, used in B[2] and D[2] and A matrix
choice_states = ['blue','green','null','withdraw']  # Hidden state factor denoting belief about the action the agent took (if they chose the blue card, green card, or took no action), used in B[3] and D[3] and A matrix
stage_states = ['null','advice','decision']   # Hidden state factor denoting beliefs about the current stage of the trial, used in B[4] and D[4] and A matrix
num_states = [len(trustworthiness_states), len(correct_card_states), len(affect_states), len(choice_states), len(stage_states)]
num_factors = len(num_states)

# Define observation modalities with respective discrete observations
advice_obs = ['blue','green','null']                # Advice modality, used in A[0] and C[0]. Agent receives 'blue' or 'green' advice from the advisor during the advice stage, or 'null' if in a different behavioral stage.
feedback_obs = ['correct','incorrect','null']       # Feedback modality, used in A[1] and C[1]. Agent receives 'correct' or 'incorrect' following the decision stage, or 'null' if in a prior behavioral task stage.
arousal_obs = ['high','low']                        # Arousal modality, used in A[2] and C[2]. Agent receives 'high' or 'low' arousal observations at each behavioral stage.
choice_obs = ['blue','green','null','withdraw']                # Choice modality, used in A[3] and C[3]. Agent receives 'blue' or 'green' based on the choice they made after the decision stage, or 'null' if in a different behavioral task change.
num_obs = [len(advice_obs), len(feedback_obs), len(arousal_obs), len(choice_obs)]
num_modalities = len(num_obs)

# Define control factors with respective actions
choice_trust_actions = ['trust','distrust']    # Control factor with actions to trust or distrust, used in B[0] and B[2], i.e. choosing to trust or distrust the advisor is assumed to influence trustworthiness and affect.
choice_card_actions = ['blue','green','null','withdraw']  # Control factor with actions for choosing the card color during the decision stage, or 'null' in timesteps where choosing is not applicable, used in B[3]
null_actions = ['NULL']                         # Control factor with null action for the uncontrollable hidden state factors correct card and stage, used B[1] and B[4], i.e. the agent does not assume it can act to influence the true correct card state nor behavioral task stage state.

# Hidden state factors at level 2
safety_self_states_2 = ['safe','danger']
safety_world_states_2 = ['safe','danger']
safety_other_states_2 = ['safe','danger']
num_states_2 = [len(safety_self_states_2), len(safety_world_states_2), len(safety_other_states_2)]
num_factors_2 = len(num_states_2)
# The level 2 model does not take actions, thus we simply supply a singular NULL action to allow the computations to play out coherently.
null_action_2 = ['NULL']

# for advice in advice_obs:
#   for trust in trustworthiness_states:
#     for correct in correct_card_states:
#       for affect in affect_states:
#         for choice in choice_states:
#           for stage in stage_states:
#             print(f"A[0][{advice},{trust},{correct},{affect},{choice},{stage}] = A[0][{advice_obs.index(advice)},{trustworthiness_states.index(trust)},{correct_card_states.index(correct)},{affect_states.index(affect)},{choice_states.index(choice)},{stage_states.index(stage)}]")

print(results_df.loc[0,'C'])

# print(results_df.loc[0,'A'][0][0])

with pd.option_context('display.max_rows',None):
  display(results_df.head(6).T)

import pandas as pd
import numpy as np
import ast

# --- Convert 'E' column to list of floats ---
def convert_E(e_str):
    if isinstance(e_str, str):
        return list(map(float, ast.literal_eval(e_str)))
    return e_str

results_df['E_list'] = results_df['E'].apply(convert_E)
num_E = len(results_df['E_list'].iloc[0])  # Correct: get length from first row

# Split E_list into separate columns
for i in range(num_E):
    results_df[f'E_{i+1}'] = results_df['E_list'].apply(lambda l: l[i])

# --- Parse column 'C' into nested lists ---
def convert_C(c_str):
    if isinstance(c_str, str):
        return ast.literal_eval(c_str)
    return c_str

results_df['C_list'] = results_df['C'].apply(convert_C)

# Get C's shape
C_example = results_df['C_list'].iloc[0]  # <-- FIXED HERE!
num_C_outer = len(C_example)
num_C_inners = [len(sublist) for sublist in C_example]

# Flatten C into separate columns: C_i_j
for i in range(num_C_outer):
    for j in range(num_C_inners[i]):
        results_df[f'C_{i+1}_{j+1}'] = results_df['C_list'].apply(lambda l: l[i][j])

# --- AGGREGATION ---
agg_df = results_df.groupby('grid_idx')['gamma'].agg(['mean', 'std']).rename(
    columns={'mean': 'gamma_mean', 'std': 'gamma_std'}
)

# For each E_i, add mean, std, first, last columns
for i in range(num_E):
    colname = f'E_{i+1}'
    mean_std = results_df.groupby('grid_idx')[colname].agg(['mean', 'std'])
    mean_std.columns = [f"{colname}_mean", f"{colname}_std"]
    agg_df = pd.concat([agg_df, mean_std], axis=1)

    first_vals, last_vals = [], []
    for grid_val in agg_df.index:
        group = results_df[results_df['grid_idx'] == grid_val]
        min_trial = group['trial'].min()
        sub_group = group[group['trial'] == min_trial]
        min_timestep = sub_group['timestep'].min()
        first_row = sub_group[sub_group['timestep'] == min_timestep].iloc[0]
        first_vals.append(first_row[colname])
        max_trial = group['trial'].max()
        sub_group = group[group['trial'] == max_trial]
        max_timestep = sub_group['timestep'].max()
        last_row = sub_group[sub_group['timestep'] == max_timestep].iloc[0]
        last_vals.append(last_row[colname])
    agg_df[f"{colname}_first"] = first_vals
    agg_df[f"{colname}_last"] = last_vals

# For each C_i_j, add mean, std, first, last columns
for i in range(num_C_outer):
    for j in range(num_C_inners[i]):
        colname = f'C_{i+1}_{j+1}'
        mean_std = results_df.groupby('grid_idx')[colname].agg(['mean', 'std'])
        mean_std.columns = [f"{colname}_mean", f"{colname}_std"]
        agg_df = pd.concat([agg_df, mean_std], axis=1)
        first_vals, last_vals = [], []
        for grid_val in agg_df.index:
            group = results_df[results_df['grid_idx'] == grid_val]
            min_trial = group['trial'].min()
            sub_group = group[group['trial'] == min_trial]
            min_timestep = sub_group['timestep'].min()
            first_row = sub_group[sub_group['timestep'] == min_timestep].iloc[0]
            first_vals.append(first_row[colname])
            max_trial = group['trial'].max()
            sub_group = group[group['trial'] == max_trial]
            max_timestep = sub_group['timestep'].max()
            last_row = sub_group[sub_group['timestep'] == max_timestep].iloc[0]
            last_vals.append(last_row[colname])
        agg_df[f"{colname}_first"] = first_vals
        agg_df[f"{colname}_last"] = last_vals

# --- ORDER COLUMNS ---
ordered_cols = ['gamma_mean', 'gamma_std']
for i in range(num_E):
    base = f'E_{i+1}'
    ordered_cols += [f'{base}_mean', f'{base}_std', f'{base}_first', f'{base}_last']
for i in range(num_C_outer):
    for j in range(num_C_inners[i]):
        base = f'C_{i+1}_{j+1}'
        ordered_cols += [f'{base}_mean', f'{base}_std', f'{base}_first', f'{base}_last']

agg_df = agg_df[ordered_cols].reset_index()

with pd.option_context('display.max_rows',None,'display.max_columns',None):
  display(agg_df.head().T)

# Optional: save
# agg_df.to_csv("/content/drive/MyDrive/PTSD_ActInf_GridSearch/grid_search_stats_by_grididx_full.csv", index=False)

# display(pd.DataFrame(results_df[['grid_idx','timestep','trust_action','card_action']].value_counts(dropna=False)).sort_values(['grid_idx','timestep']))

break

def analyze_card_action_patterns(df):
    """Analyzes card_action patterns across trials with 3 timesteps each."""
    # Group by trial and collect actions in timestep order
    pattern_df = df.groupby('trial')['card_action'].apply(list).reset_index()

    # Calculate pattern frequencies with explicit column naming
    pattern_counts = (
        pattern_df['card_action']
        .value_counts()
        .reset_index(name='count')  # Name the count column explicitly
        .rename(columns={'index': 'pattern'})
        .sort_values('count', ascending=False)
    )

    return pattern_counts

def analyze_trust_action_patterns(df):
    """Analyzes trust_action patterns across trials with 3 timesteps each."""
    # Group by trial and collect actions in timestep order
    pattern_df = df.groupby('trial')['trust_action'].apply(list).reset_index()

    # Calculate pattern frequencies with explicit column naming
    pattern_counts = (
        pattern_df['trust_action']
        .value_counts()
        .reset_index(name='count')  # Name the count column explicitly
        .rename(columns={'index': 'pattern'})
        .sort_values('count', ascending=False)
    )

    return pattern_counts

pattern_counts = analyze_card_action_patterns(final_df_5)
display(pattern_counts)
card_pattern_counts = analyze_card_action_patterns(final_df_5)
trust_pattern_counts = analyze_trust_action_patterns(final_df_5)
pattern_counts = card_pattern_counts.merge(trust_pattern_counts, left_index=True, right_index=True,
                                            how='inner')
display(pattern_counts)
print(pattern_counts)
print(trust_pattern_counts)

#print(final_df_4['trust_action'].head(5))

# Group by trial and collect actions in timestep order
trust_pattern_df = final_df_5.groupby('trial')['trust_action'].apply(list).reset_index()
card_pattern_df = final_df_5.groupby('trial')['card_action'].apply(list).reset_index()
pattern_df = card_pattern_df.merge(trust_pattern_df, on = 'trial', #left_index=True, right_index=True,
                                            how='inner')
#pattern_vc = pattern_df[['card_action','trust_action']].value_counts().reset_index()
display(pattern_df)
#display(pattern_vc)

def analyze_action_patterns(df):
    """
    Analyzes card_action and trust_action patterns across trials
    with 3 timesteps each.
    Returns frequency counts of (card_action_pattern, trust_action_pattern) pairs.
    """
    # Group by trial and collect actions for both columns in timestep order
    pattern_df = (
        df.groupby('trial')
          .agg({
              'card_action': list,
              'trust_action': list
          })
          .reset_index()
    )

    # Build tuples of (card_action_pattern, trust_action_pattern)
    pattern_df['pattern_pair'] = list(zip(pattern_df['card_action'], pattern_df['trust_action']))

    # Count frequencies of unique pattern pairs
    pattern_counts = (
        pattern_df['pattern_pair']
        .value_counts()
        .reset_index(name='count')
        .rename(columns={'index': 'pattern_pair'})
        .sort_values('count', ascending=False)
    )

    return pattern_counts

action_patterns = analyze_action_patterns(final_df_5)
display(action_patterns)

plot_agent_data_wide(final_df_5, hierarchical=True)

def plot_agent_data_focused(final_df_input, hierarchical=False):
    import matplotlib.pyplot as plt
    from matplotlib.lines import Line2D
    import numpy as np
    import ast

    fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 10))
    plt.subplots_adjust(hspace=0.4, wspace=0.3)

    final_df = final_df_input.copy()
    timesteps = range(len(final_df))

    cols_to_plot = ['qs_bma', 'obs_label', 'q_pi']
    if hierarchical:
        cols_to_plot += ['qo_pi_high', 'qs_high_bma']

    for col in cols_to_plot:
        final_df[col] = final_df[col].apply(safe_literal_eval)

    def setup_probability_plot(ax, title):
        ax.set_ylim(-0.1, 1.1)
        ax.set_yticks(np.linspace(0, 1, 11))
        ax.set_ylabel('Probability')
        ax.grid(True, alpha=0.3)
        ax.set_title(title)

    # --- TOP LEFT ---
    ax1 = axes[0, 0]
    setup_probability_plot(ax1, 'Card Probability & Advice Observations')
    blue_probs = [qs[1][0] for qs in final_df['qs_bma']]
    ax1.plot(timesteps, blue_probs, label='Posterior Blue Probability', color='blue')
    obs_advice = [row[0] for row in final_df['obs_label']]
    advice_y = [1.0 if advice=='blue' else 0.0 if advice=='green' else 0.5 for advice in obs_advice]
    advice_colors = ['blue' if advice=='blue' else 'green' if advice=='green' else 'grey' for advice in obs_advice]
    ax1.scatter(timesteps, advice_y, c=advice_colors, s=30, alpha=0.7, edgecolor='black')
    legend_elements1 = [
        Line2D([0], [0], marker='o', color='w', label='Blue Advice', markerfacecolor='blue', markersize=8),
        Line2D([0], [0], marker='o', color='w', label='Green Advice', markerfacecolor='green', markersize=8),
        Line2D([0], [0], color='blue', label='Posterior Blue Probability')
    ]
    ax1.legend(handles=legend_elements1)  # Back inside

    # --- TOP RIGHT ---
    ax2 = axes[0, 1]
    setup_probability_plot(ax2, 'Anger State Probabilities with Arousal Markers')
    prior_angry = [d[2][0] for d in final_df['D']]
    angry_probs = [qs[2][0] for qs in final_df['qs_bma']]
    ax2.plot(timesteps, prior_angry, label='Prior Anger', color='black', linestyle='--', linewidth=2.5)
    ax2.plot(timesteps, angry_probs, label='Posterior Anger', color='#cc0000', linewidth=2.5, alpha=0.5)
    arousal_states = [row[2] for row in final_df['obs_label']]
    arousal_y = [1.0 if arousal=='high' else 0.0 for arousal in arousal_states]
    arousal_colors = ['#ff7f0e' if arousal=='high' else '#2ca02c' for arousal in arousal_states]
    ax2.scatter(timesteps, arousal_y, c=arousal_colors, marker='o', s=30, alpha=0.7, edgecolor='black')
    legend_elements2 = [
        Line2D([0], [0], color='black', linestyle='--', label='Prior Anger'),
        Line2D([0], [0], color='#cc0000', label='Posterior Anger', alpha=0.5),
        Line2D([0], [0], marker='o', color='w', label='High Arousal', markerfacecolor='#ff7f0e', markersize=8),
        Line2D([0], [0], marker='o', color='w', label='Low Arousal', markerfacecolor='#2ca02c', markersize=8)
    ]
    ax2.legend(handles=legend_elements2)  # Back inside

    # --- BOTTOM LEFT ---
    ax3 = axes[1, 0]
    setup_probability_plot(ax3, 'Policy Selection Probabilities')
    policy_labels = ['Trust + Green', 'Trust + Blue', 'Distrust + Blue', 'Distrust + Green', 'Trust + Withdraw', 'Distrust + Withdraw']
    colors = ['green', 'blue', 'red', 'orange', 'purple', 'grey']
    for i in range(6):
        policy_probs = [q[i] for q in final_df['q_pi']]
        ax3.plot(timesteps, policy_probs, label=policy_labels[i], color=colors[i], linewidth=2.5)
    ax3.legend()  # Back inside, no trust/distrust dots or legend

    # --- BOTTOM RIGHT ---
    ax4 = axes[1, 1]
    if hierarchical:
        setup_probability_plot(ax4, 'Hierarchical Posterior Observation Beliefs Q(o), Real-Time Trust Beliefs, True Trustworthiness')
        trustworthiness_qo = [qo[0][0] for qo in final_df['qo_pi_high']]
        affect_qo = [qo[2][0] for qo in final_df['qo_pi_high']]
        correct_qo = [qo[1][0] for qo in final_df['qo_pi_high']]
        ax4.plot(timesteps, trustworthiness_qo, label='Trust Expectation', color='grey', linewidth=2.5)
        ax4.plot(timesteps, affect_qo, label='Calm Expectation', color='orange', linewidth=2.5)
        ax4.plot(timesteps, correct_qo, label='Blue Correct Expectation', color='purple', linewidth=2.5)
        prior_trust = [d[0][0] for d in final_df['D']]
        posterior_trust = [qs[0][0] for qs in final_df['qs_bma']]
        ax4.plot(timesteps, prior_trust, label='Prior Probability of Trust', color='blue', linestyle='--', linewidth=2)
        ax4.plot(timesteps, posterior_trust, label='Posterior Probability of Trust', color='blue', linewidth=2)
        true_trust = final_df['true_trustworthiness']
        true_y = [1.0 if t == 'trustworthy' else 0.0 for t in true_trust]
        true_colors = ['blue' if t == 'trustworthy' else 'red' for t in true_trust]
        ax4.scatter(timesteps, true_y, c=true_colors, marker='o', s=30, alpha=0.7, edgecolor='black', label='True Trustworthiness')
        legend_elements4 = [
            Line2D([0], [0], color='grey', label='Trust Expectation'),
            Line2D([0], [0], color='orange', label='Calm Expectation'),
            Line2D([0], [0], color='purple', label='Blue Correct Expectation'),
            Line2D([0], [0], color='blue', linestyle='--', label='Prior Probability of Trust'),
            Line2D([0], [0], color='blue', label='Posterior Probability of Trust'),
            Line2D([0], [0], marker='o', color='w', label='True Trustworthiness (Trustworthy)', markerfacecolor='blue', markersize=8),
            Line2D([0], [0], marker='o', color='w', label='True Trustworthiness (Untrustworthy)', markerfacecolor='red', markersize=8)
        ]
        ax4.legend(handles=legend_elements4, loc='lower right', bbox_to_anchor=(1.35, -0.05))  # Remains outside
    else:
        setup_probability_plot(ax4, 'Hierarchical Observations (Disabled)')
        ax4.text(0.5, 0.5, 'Enable with hierarchical=True', ha='center', va='center', fontsize=12, transform=ax4.transAxes)

    # Remove x-axis numbers from all plots but keep ticks and label
    for ax in axes.flatten():
        ax.set_xlabel('Timestep', fontsize=10)
        ax.set_xticks(np.arange(0, len(timesteps), 2))
        ax.set_xticklabels([])

    return fig, axes

# with pd.option_context('display.max_rows',None):
#   display(results_df.head(6).T)

results_df.loc[90,'qs_bma']

#for i in range(results_df['grid_idx'].max() + 1):
#  print(f"Grid idx {i} ==================================================================================================================================================================")
#  plot_agent_data_focused(results_df[results_df['grid_idx'] == i], hierarchical=True)
plot_agent_data_focused(results_df, hierarchical=True)

